{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialization\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax\n",
    "\n",
    "#test tools\n",
    "import test_utils\n",
    "\n",
    "#setting plot default parameters\n",
    "plt.rcParams['figure.figsize'] = (10.0, 9.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show an example:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x900 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 0 \n",
      "\n",
      "Train data shape:  (9000, 256)\n",
      "Train labels shape:  (9000,)\n",
      "Validation data shape:  (1000, 256)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 256)\n",
      "Test labels shape:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "\n",
    "X_train = jnp.load('X_train.npy')\n",
    "y_train = jnp.load('y_train.npy')\n",
    "X_val = jnp.load('X_val.npy')\n",
    "y_val = jnp.load('y_val.npy')\n",
    "X_test = jnp.load('X_test.npy')\n",
    "y_test = jnp.load('y_test.npy')\n",
    "\n",
    "print('Show an example:')\n",
    "a0 = X_train[100,:].reshape(16,16)\n",
    "plt.imshow(a0)\n",
    "plt.show()\n",
    "print('label:', y_train[100], '\\n')\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy array before mutation:\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "Numpy array after mutation:\n",
      "[10 10 10 10 10  5  6  7  8  9]\n"
     ]
    }
   ],
   "source": [
    "#Example of mutation in numpy\n",
    "\n",
    "import numpy as np\n",
    "x = np.arange(10)\n",
    "print('Numpy array before mutation:\\n{}'.format(x))\n",
    "x[0:5] = 10\n",
    "print('Numpy array after mutation:\\n{}'.format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CATCH AN TYPE ERROR!\n",
      "'<class 'jaxlib.xla_extension.DeviceArray'>' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html\n"
     ]
    }
   ],
   "source": [
    "# jax do not support!\n",
    "try:\n",
    "    # THIS WILL CAUSE AN ERROR !!!\n",
    "    x = jnp.arange(10)\n",
    "    x[0] = 10\n",
    "except TypeError as err:\n",
    "    print('CATCH AN TYPE ERROR!')\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX array before mutation:\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "JAX array after mutation:\n",
      "[10 10 10 10 10  5  6  7  8  9]\n"
     ]
    }
   ],
   "source": [
    "x = jnp.arange(10)\n",
    "print('JAX array before mutation:\\n{}'.format(x))\n",
    "x = x.at[0:5].set(10)\n",
    "print('JAX array after mutation:\\n{}'.format(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRONG: Using same key will have same random samples!\n",
      "Random array 0 : [ 1.8160863  -0.48262316  0.33988908]\n",
      "Random array 1 : [ 1.8160863  -0.48262316  0.33988908]\n",
      "Random array 2 : [ 1.8160863  -0.48262316  0.33988908]\n",
      "\n",
      "------------\n",
      "\n",
      "Correct: Using new subkeys\n",
      "Random array 0 : [ 1.1378784  -1.2209548  -0.59153634]\n",
      "Random array 1 : [-0.06607257  0.16676573  1.1780034 ]\n",
      "Random array 2 : [-2.2051446  -0.90678614 -0.51053447]\n"
     ]
    }
   ],
   "source": [
    "#Example for generate random numbers in JAX\n",
    "from jax import random\n",
    "\n",
    "#WRONG Example!\n",
    "key = random.PRNGKey(0)\n",
    "print('WRONG: Using same key will have same random samples!')\n",
    "for i in range(3):\n",
    "    normal_sample = random.normal(key, shape = [3])\n",
    "    print('Random array {} : {}'.format(i, normal_sample))\n",
    "\n",
    "print('\\n------------\\n')\n",
    "\n",
    "#Good Example\n",
    "key = random.PRNGKey(0)\n",
    "print('Correct: Using new subkeys')\n",
    "for i in range(3):\n",
    "    key, subkey = random.split(key)\n",
    "    normal_sample = random.normal(subkey, shape = [3])\n",
    "    print('Random array {} : {}'.format(i, normal_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 2.302585\n"
     ]
    }
   ],
   "source": [
    "#Test run for task 1\n",
    "\n",
    "import Network\n",
    "\n",
    "hidden_size = 256\n",
    "\n",
    "params = Network.init(hidden_size, random_seed = 0)\n",
    "loss, predict, states = Network.forward_pass(params, X_train, y_train)\n",
    "print('Loss = %f' % loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BackProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2 relative error: 1.851859e-05\n",
      "b2 relative error: 9.046243e-06\n",
      "W1 relative error: 4.959189e-02\n",
      "b1 relative error: 3.965820e-02\n"
     ]
    }
   ],
   "source": [
    "#Test run for task2+3 \n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "wd_decay = 0.05\n",
    "delta = 1e-2\n",
    "\n",
    "params = Network.init(input_size = input_size, hidden_size = hidden_size, output_size = num_classes, random_seed = 0, std=1e-1)\n",
    "X, y = test_utils.init_toy_data(num_inputs, input_size, num_classes)\n",
    "\n",
    "loss, predict, states = Network.forward_pass(params, X, y, wd_decay = wd_decay)\n",
    "grads = Network.back_prop(params, states, X, y, wd_decay = wd_decay)\n",
    "num_grads = Network.numerical_gradient(params, X, y, wd_decay = wd_decay, delta = delta)\n",
    "\n",
    "# Errors should all be less than 1e-2 or so\n",
    "\n",
    "for param_name in grads:\n",
    "    print('%s relative error: %e' % (param_name, test_utils.rel_error(num_grads[param_name], grads[param_name])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: JIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481 µs ± 24.9 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "476 µs ± 43 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "#Example for JIT\n",
    "\n",
    "import timeit\n",
    "\n",
    "def norm(x):\n",
    "    x = x - x.mean(0)\n",
    "    return x / x.std(0)\n",
    "\n",
    "@jax.jit\n",
    "def norm_jit(x):\n",
    "    x = x - x.mean(0)\n",
    "    return x / x.std(0)\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "x = jax.random.normal(key, shape = [10000, 10])\n",
    "\n",
    "%timeit norm(x).block_until_ready()\n",
    "%timeit norm_jit(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 200: training loss 2.302585 val loss: 2.302718\n",
      "iteration 10 / 200: training loss 2.335719 val loss: 2.311285\n",
      "iteration 20 / 200: training loss 2.185594 val loss: 2.169381\n",
      "iteration 30 / 200: training loss 1.703610 val loss: 1.721180\n",
      "iteration 40 / 200: training loss 1.543830 val loss: 1.579623\n",
      "iteration 50 / 200: training loss 1.773627 val loss: 1.615199\n",
      "iteration 60 / 200: training loss 1.675154 val loss: 1.599752\n",
      "iteration 70 / 200: training loss 1.454804 val loss: 1.482940\n",
      "iteration 80 / 200: training loss 1.325440 val loss: 1.363041\n",
      "iteration 90 / 200: training loss 1.494965 val loss: 1.323216\n",
      "iteration 100 / 200: training loss 1.274299 val loss: 1.299463\n",
      "iteration 110 / 200: training loss 1.221558 val loss: 1.215462\n",
      "iteration 120 / 200: training loss 1.264409 val loss: 1.277208\n",
      "iteration 130 / 200: training loss 1.500118 val loss: 1.289396\n",
      "iteration 140 / 200: training loss 1.162604 val loss: 1.155676\n",
      "iteration 150 / 200: training loss 1.178339 val loss: 1.132165\n",
      "iteration 160 / 200: training loss 1.003765 val loss: 1.081548\n",
      "iteration 170 / 200: training loss 1.141416 val loss: 1.107288\n",
      "iteration 180 / 200: training loss 1.316593 val loss: 1.456133\n",
      "iteration 190 / 200: training loss 1.253402 val loss: 1.586523\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x900 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x900 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy on test set: 0.80500007\n"
     ]
    }
   ],
   "source": [
    "# Test run for task 4\n",
    "\n",
    "hidden_size = 10\n",
    "random_seed = 0\n",
    "\n",
    "params = Network.init(random_seed = random_seed, hidden_size = hidden_size)\n",
    "\n",
    "stats, best_params = Network.train(params, X_train, y_train, X_val, y_val, random_seed = random_seed,\n",
    "            learning_rate=0.5, momentum=0.9, wd_decay=0.02, \n",
    "            num_iters=200, batch_size=100,\n",
    "            do_early_stopping=False,\n",
    "            print_every=10, verbose=True)\n",
    "\n",
    "test_utils.plot_loss_history(stats['loss_history'], stats['val_loss_history'])\n",
    "test_utils.plot_acc_history(stats['acc_history'],stats['val_acc_history'])\n",
    "print('Final accuracy on test set:', Network.get_acc(best_params, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Training Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 2000: training loss 2.302592 val loss: 2.302724\n",
      "iteration 50 / 2000: training loss 1.392031 val loss: 1.221982\n",
      "iteration 100 / 2000: training loss 1.065954 val loss: 1.079843\n",
      "iteration 150 / 2000: training loss 1.212496 val loss: 1.130052\n",
      "iteration 200 / 2000: training loss 1.004328 val loss: 1.043041\n",
      "iteration 250 / 2000: training loss 1.223512 val loss: 1.306633\n",
      "iteration 300 / 2000: training loss 1.080015 val loss: 1.176414\n",
      "iteration 350 / 2000: training loss 1.071704 val loss: 1.028394\n",
      "iteration 400 / 2000: training loss 1.097029 val loss: 1.070246\n",
      "iteration 450 / 2000: training loss 1.022327 val loss: 1.030736\n",
      "iteration 500 / 2000: training loss 1.274472 val loss: 1.131308\n",
      "iteration 550 / 2000: training loss 1.399697 val loss: 1.156562\n",
      "iteration 600 / 2000: training loss 1.067595 val loss: 1.028296\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x900 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x900 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy on test set: 0.749\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# TODO : training process with early stop and learning rate decay\n",
    "#\n",
    "# Hint : you may need additional hyperparameters\n",
    "\n",
    "hidden_size = 256\n",
    "random_seed = 0\n",
    "\n",
    "params = Network.init(random_seed = random_seed, hidden_size = hidden_size)\n",
    "\n",
    "stats, best_params = Network.train(params, X_train, y_train, X_val, y_val, random_seed = random_seed,\n",
    "            learning_rate=0.5, momentum=0.9, wd_decay=0.02, \n",
    "            num_iters=2000, batch_size=100,\n",
    "            do_early_stopping=True,\n",
    "            print_every=50, verbose=True)\n",
    "\n",
    "test_utils.plot_loss_history(stats['loss_history'], stats['val_loss_history'])\n",
    "test_utils.plot_acc_history(stats['acc_history'], stats['val_acc_history'])\n",
    "print('Final accuracy on test set:', Network.get_acc(best_params, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 2000: training loss 2.302586 val loss: 2.302591\n",
      "iteration 50 / 2000: training loss 2.017111 val loss: 1.986414\n",
      "iteration 100 / 2000: training loss 0.742690 val loss: 0.766918\n",
      "iteration 150 / 2000: training loss 0.470382 val loss: 0.583284\n",
      "iteration 200 / 2000: training loss 0.495258 val loss: 0.570314\n",
      "iteration 250 / 2000: training loss 0.472855 val loss: 0.554269\n",
      "iteration 300 / 2000: training loss 0.496794 val loss: 0.564425\n",
      "iteration 350 / 2000: training loss 0.529934 val loss: 0.562060\n",
      "iteration 400 / 2000: training loss 0.501773 val loss: 0.540658\n",
      "iteration 450 / 2000: training loss 0.535236 val loss: 0.553958\n",
      "iteration 500 / 2000: training loss 0.530827 val loss: 0.526868\n",
      "iteration 550 / 2000: training loss 0.521663 val loss: 0.527677\n",
      "iteration 600 / 2000: training loss 0.446153 val loss: 0.533121\n",
      "iteration 650 / 2000: training loss 0.424965 val loss: 0.521872\n",
      "iteration 700 / 2000: training loss 0.552490 val loss: 0.527498\n",
      "iteration 750 / 2000: training loss 0.513468 val loss: 0.538624\n",
      "iteration 800 / 2000: training loss 0.508874 val loss: 0.542888\n",
      "iteration 850 / 2000: training loss 0.510612 val loss: 0.546562\n",
      "iteration 900 / 2000: training loss 0.486183 val loss: 0.532500\n",
      "iteration 950 / 2000: training loss 0.494298 val loss: 0.521047\n",
      "iteration 1000 / 2000: training loss 0.487870 val loss: 0.529995\n",
      "iteration 1050 / 2000: training loss 0.494594 val loss: 0.540581\n",
      "iteration 1100 / 2000: training loss 0.549270 val loss: 0.541851\n",
      "iteration 1150 / 2000: training loss 0.457180 val loss: 0.530638\n",
      "iteration 1200 / 2000: training loss 0.533465 val loss: 0.518760\n",
      "iteration 1250 / 2000: training loss 0.578807 val loss: 0.534256\n",
      "hidden_size: 75 lr: 1.000000e-01 lr_decay: 9.800000e-01 reg: 1.000000e-02 train accuracy: 0.960000 val accuracy: 0.929000\n",
      "iteration 0 / 2000: training loss 2.302586 val loss: 2.302591\n",
      "iteration 50 / 2000: training loss 2.108141 val loss: 2.091963\n",
      "iteration 100 / 2000: training loss 0.699057 val loss: 0.747920\n",
      "iteration 150 / 2000: training loss 0.457227 val loss: 0.577563\n",
      "iteration 200 / 2000: training loss 0.499841 val loss: 0.568582\n",
      "iteration 250 / 2000: training loss 0.465902 val loss: 0.547704\n",
      "iteration 300 / 2000: training loss 0.485437 val loss: 0.565714\n",
      "iteration 350 / 2000: training loss 0.529011 val loss: 0.537013\n",
      "iteration 400 / 2000: training loss 0.513807 val loss: 0.527809\n",
      "iteration 450 / 2000: training loss 0.528872 val loss: 0.537937\n",
      "iteration 500 / 2000: training loss 0.492695 val loss: 0.524174\n",
      "iteration 550 / 2000: training loss 0.526898 val loss: 0.520344\n",
      "iteration 600 / 2000: training loss 0.435656 val loss: 0.525640\n",
      "hidden_size: 75 lr: 1.000000e-01 lr_decay: 9.000000e-01 reg: 1.000000e-02 train accuracy: 0.970000 val accuracy: 0.930000\n",
      "iteration 0 / 2000: training loss 2.302586 val loss: 2.302591\n",
      "iteration 50 / 2000: training loss 2.164166 val loss: 2.147377\n",
      "iteration 100 / 2000: training loss 0.693434 val loss: 0.738317\n",
      "iteration 150 / 2000: training loss 0.462301 val loss: 0.583263\n",
      "iteration 200 / 2000: training loss 0.502741 val loss: 0.561184\n",
      "iteration 250 / 2000: training loss 0.470914 val loss: 0.549387\n",
      "iteration 300 / 2000: training loss 0.473459 val loss: 0.560202\n",
      "iteration 350 / 2000: training loss 0.541395 val loss: 0.535104\n",
      "iteration 400 / 2000: training loss 0.528813 val loss: 0.534389\n",
      "iteration 450 / 2000: training loss 0.535059 val loss: 0.532522\n",
      "iteration 500 / 2000: training loss 0.490401 val loss: 0.527265\n",
      "iteration 550 / 2000: training loss 0.524287 val loss: 0.529874\n",
      "iteration 600 / 2000: training loss 0.431172 val loss: 0.520656\n",
      "iteration 650 / 2000: training loss 0.430603 val loss: 0.521998\n",
      "iteration 700 / 2000: training loss 0.521071 val loss: 0.519534\n",
      "iteration 750 / 2000: training loss 0.507150 val loss: 0.516000\n",
      "iteration 800 / 2000: training loss 0.541400 val loss: 0.527935\n",
      "iteration 850 / 2000: training loss 0.513167 val loss: 0.518261\n",
      "iteration 900 / 2000: training loss 0.477680 val loss: 0.517569\n",
      "iteration 950 / 2000: training loss 0.453007 val loss: 0.515161\n",
      "iteration 1000 / 2000: training loss 0.480461 val loss: 0.514189\n",
      "iteration 1050 / 2000: training loss 0.485843 val loss: 0.512647\n",
      "iteration 1100 / 2000: training loss 0.503107 val loss: 0.512411\n",
      "iteration 1150 / 2000: training loss 0.463895 val loss: 0.513256\n",
      "iteration 1200 / 2000: training loss 0.505595 val loss: 0.511971\n",
      "iteration 1250 / 2000: training loss 0.537342 val loss: 0.513812\n",
      "iteration 1300 / 2000: training loss 0.496277 val loss: 0.512703\n",
      "iteration 1350 / 2000: training loss 0.515095 val loss: 0.513495\n",
      "iteration 1400 / 2000: training loss 0.448490 val loss: 0.512591\n",
      "iteration 1450 / 2000: training loss 0.520019 val loss: 0.514577\n",
      "iteration 1500 / 2000: training loss 0.474846 val loss: 0.509621\n",
      "iteration 1550 / 2000: training loss 0.542761 val loss: 0.512477\n",
      "iteration 1600 / 2000: training loss 0.514651 val loss: 0.510110\n",
      "iteration 1650 / 2000: training loss 0.490543 val loss: 0.508502\n",
      "iteration 1700 / 2000: training loss 0.472069 val loss: 0.509644\n",
      "hidden_size: 75 lr: 1.000000e-01 lr_decay: 8.500000e-01 reg: 1.000000e-02 train accuracy: 0.940000 val accuracy: 0.941000\n",
      "iteration 0 / 2000: training loss 2.302586 val loss: 2.302591\n",
      "iteration 50 / 2000: training loss 2.213403 val loss: 2.196928\n",
      "iteration 100 / 2000: training loss 0.715969 val loss: 0.754131\n",
      "iteration 150 / 2000: training loss 0.464971 val loss: 0.590480\n",
      "iteration 200 / 2000: training loss 0.504900 val loss: 0.562394\n",
      "iteration 250 / 2000: training loss 0.471389 val loss: 0.554090\n",
      "iteration 300 / 2000: training loss 0.463008 val loss: 0.553528\n",
      "iteration 350 / 2000: training loss 0.551975 val loss: 0.538033\n",
      "iteration 400 / 2000: training loss 0.532337 val loss: 0.538218\n",
      "iteration 450 / 2000: training loss 0.533207 val loss: 0.529189\n",
      "iteration 500 / 2000: training loss 0.491574 val loss: 0.523455\n",
      "iteration 550 / 2000: training loss 0.514237 val loss: 0.530032\n",
      "iteration 600 / 2000: training loss 0.433769 val loss: 0.518194\n",
      "iteration 650 / 2000: training loss 0.427841 val loss: 0.522671\n",
      "iteration 700 / 2000: training loss 0.525552 val loss: 0.525579\n",
      "iteration 750 / 2000: training loss 0.505216 val loss: 0.515682\n",
      "iteration 800 / 2000: training loss 0.544432 val loss: 0.526320\n",
      "iteration 850 / 2000: training loss 0.508931 val loss: 0.517918\n",
      "iteration 900 / 2000: training loss 0.481011 val loss: 0.516471\n",
      "iteration 950 / 2000: training loss 0.447667 val loss: 0.517191\n",
      "iteration 1000 / 2000: training loss 0.480809 val loss: 0.515633\n",
      "iteration 1050 / 2000: training loss 0.491098 val loss: 0.514812\n",
      "iteration 1100 / 2000: training loss 0.505596 val loss: 0.515492\n",
      "iteration 1150 / 2000: training loss 0.471798 val loss: 0.513807\n",
      "iteration 1200 / 2000: training loss 0.500806 val loss: 0.514551\n",
      "iteration 1250 / 2000: training loss 0.536184 val loss: 0.514200\n",
      "iteration 1300 / 2000: training loss 0.504135 val loss: 0.513544\n",
      "iteration 1350 / 2000: training loss 0.515684 val loss: 0.514368\n",
      "iteration 1400 / 2000: training loss 0.454128 val loss: 0.513209\n",
      "iteration 1450 / 2000: training loss 0.521711 val loss: 0.513755\n",
      "iteration 1500 / 2000: training loss 0.476972 val loss: 0.512837\n",
      "iteration 1550 / 2000: training loss 0.543145 val loss: 0.513614\n",
      "iteration 1600 / 2000: training loss 0.513447 val loss: 0.513547\n",
      "iteration 1650 / 2000: training loss 0.491159 val loss: 0.512268\n",
      "iteration 1700 / 2000: training loss 0.471473 val loss: 0.512544\n",
      "iteration 1750 / 2000: training loss 0.428169 val loss: 0.511853\n",
      "iteration 1800 / 2000: training loss 0.543120 val loss: 0.513015\n",
      "iteration 1850 / 2000: training loss 0.489790 val loss: 0.512117\n",
      "iteration 1900 / 2000: training loss 0.418959 val loss: 0.512505\n",
      "iteration 1950 / 2000: training loss 0.497053 val loss: 0.512091\n",
      "hidden_size: 75 lr: 1.000000e-01 lr_decay: 8.000000e-01 reg: 1.000000e-02 train accuracy: 0.930000 val accuracy: 0.943000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302592\n",
      "iteration 50 / 2000: training loss 2.053769 val loss: 2.032219\n",
      "iteration 100 / 2000: training loss 0.942488 val loss: 0.948284\n",
      "iteration 150 / 2000: training loss 0.708271 val loss: 0.766175\n",
      "iteration 200 / 2000: training loss 0.718054 val loss: 0.763310\n",
      "iteration 250 / 2000: training loss 0.696249 val loss: 0.776991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 300 / 2000: training loss 0.693252 val loss: 0.763814\n",
      "iteration 350 / 2000: training loss 0.752465 val loss: 0.786803\n",
      "iteration 400 / 2000: training loss 0.723845 val loss: 0.736592\n",
      "iteration 450 / 2000: training loss 0.809816 val loss: 0.776149\n",
      "hidden_size: 75 lr: 1.000000e-01 lr_decay: 9.800000e-01 reg: 2.000000e-02 train accuracy: 0.900000 val accuracy: 0.918000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302592\n",
      "iteration 50 / 2000: training loss 2.136669 val loss: 2.122902\n",
      "iteration 100 / 2000: training loss 0.898323 val loss: 0.931055\n",
      "iteration 150 / 2000: training loss 0.693570 val loss: 0.778020\n",
      "iteration 200 / 2000: training loss 0.717632 val loss: 0.768853\n",
      "iteration 250 / 2000: training loss 0.675499 val loss: 0.755436\n",
      "iteration 300 / 2000: training loss 0.700137 val loss: 0.771678\n",
      "iteration 350 / 2000: training loss 0.747171 val loss: 0.751413\n",
      "iteration 400 / 2000: training loss 0.731534 val loss: 0.740857\n",
      "iteration 450 / 2000: training loss 0.752279 val loss: 0.743880\n",
      "iteration 500 / 2000: training loss 0.718987 val loss: 0.727754\n",
      "iteration 550 / 2000: training loss 0.721544 val loss: 0.723577\n",
      "iteration 600 / 2000: training loss 0.649870 val loss: 0.730846\n",
      "iteration 650 / 2000: training loss 0.639966 val loss: 0.725790\n",
      "iteration 700 / 2000: training loss 0.743958 val loss: 0.737768\n",
      "iteration 750 / 2000: training loss 0.724819 val loss: 0.726408\n",
      "iteration 800 / 2000: training loss 0.719585 val loss: 0.739226\n",
      "iteration 850 / 2000: training loss 0.739092 val loss: 0.722435\n",
      "iteration 900 / 2000: training loss 0.694918 val loss: 0.726249\n",
      "iteration 950 / 2000: training loss 0.678452 val loss: 0.717773\n",
      "iteration 1000 / 2000: training loss 0.701571 val loss: 0.717375\n",
      "iteration 1050 / 2000: training loss 0.701231 val loss: 0.717637\n",
      "iteration 1100 / 2000: training loss 0.712120 val loss: 0.722584\n",
      "iteration 1150 / 2000: training loss 0.675518 val loss: 0.721429\n",
      "iteration 1200 / 2000: training loss 0.744587 val loss: 0.723940\n",
      "iteration 1250 / 2000: training loss 0.765643 val loss: 0.725690\n",
      "iteration 1300 / 2000: training loss 0.721225 val loss: 0.719208\n",
      "iteration 1350 / 2000: training loss 0.764305 val loss: 0.725222\n",
      "iteration 1400 / 2000: training loss 0.682995 val loss: 0.715370\n",
      "iteration 1450 / 2000: training loss 0.760639 val loss: 0.721452\n",
      "iteration 1500 / 2000: training loss 0.693388 val loss: 0.716990\n",
      "iteration 1550 / 2000: training loss 0.748405 val loss: 0.718458\n",
      "iteration 1600 / 2000: training loss 0.736706 val loss: 0.714161\n",
      "iteration 1650 / 2000: training loss 0.716381 val loss: 0.710979\n",
      "iteration 1700 / 2000: training loss 0.668145 val loss: 0.711790\n",
      "iteration 1750 / 2000: training loss 0.637288 val loss: 0.712173\n",
      "iteration 1800 / 2000: training loss 0.761047 val loss: 0.712188\n",
      "iteration 1850 / 2000: training loss 0.713273 val loss: 0.711649\n",
      "iteration 1900 / 2000: training loss 0.623019 val loss: 0.711141\n",
      "iteration 1950 / 2000: training loss 0.715484 val loss: 0.715887\n",
      "hidden_size: 75 lr: 1.000000e-01 lr_decay: 9.000000e-01 reg: 2.000000e-02 train accuracy: 0.930000 val accuracy: 0.933000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302592\n",
      "iteration 50 / 2000: training loss 2.187595 val loss: 2.172171\n",
      "iteration 100 / 2000: training loss 0.900917 val loss: 0.930564\n",
      "iteration 150 / 2000: training loss 0.687428 val loss: 0.785444\n",
      "iteration 200 / 2000: training loss 0.712640 val loss: 0.769999\n",
      "iteration 250 / 2000: training loss 0.675661 val loss: 0.749578\n",
      "iteration 300 / 2000: training loss 0.687290 val loss: 0.767407\n",
      "iteration 350 / 2000: training loss 0.742830 val loss: 0.741764\n",
      "iteration 400 / 2000: training loss 0.747307 val loss: 0.737635\n",
      "iteration 450 / 2000: training loss 0.756390 val loss: 0.740861\n",
      "iteration 500 / 2000: training loss 0.705832 val loss: 0.732264\n",
      "iteration 550 / 2000: training loss 0.730019 val loss: 0.730245\n",
      "iteration 600 / 2000: training loss 0.650750 val loss: 0.729364\n",
      "iteration 650 / 2000: training loss 0.644084 val loss: 0.727248\n",
      "iteration 700 / 2000: training loss 0.739262 val loss: 0.727741\n",
      "iteration 750 / 2000: training loss 0.727782 val loss: 0.721585\n",
      "iteration 800 / 2000: training loss 0.737814 val loss: 0.734170\n",
      "iteration 850 / 2000: training loss 0.746016 val loss: 0.723229\n",
      "iteration 900 / 2000: training loss 0.700057 val loss: 0.725138\n",
      "iteration 950 / 2000: training loss 0.673137 val loss: 0.720003\n",
      "iteration 1000 / 2000: training loss 0.704440 val loss: 0.718738\n",
      "iteration 1050 / 2000: training loss 0.699374 val loss: 0.718950\n",
      "iteration 1100 / 2000: training loss 0.704310 val loss: 0.718698\n",
      "iteration 1150 / 2000: training loss 0.674514 val loss: 0.717919\n",
      "iteration 1200 / 2000: training loss 0.734721 val loss: 0.716875\n",
      "iteration 1250 / 2000: training loss 0.750489 val loss: 0.719489\n",
      "iteration 1300 / 2000: training loss 0.720196 val loss: 0.718318\n",
      "iteration 1350 / 2000: training loss 0.741987 val loss: 0.718579\n",
      "hidden_size: 75 lr: 1.000000e-01 lr_decay: 8.500000e-01 reg: 2.000000e-02 train accuracy: 0.940000 val accuracy: 0.933000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302592\n",
      "iteration 50 / 2000: training loss 2.230262 val loss: 2.215544\n",
      "iteration 100 / 2000: training loss 0.924469 val loss: 0.948031\n",
      "iteration 150 / 2000: training loss 0.689308 val loss: 0.794427\n",
      "iteration 200 / 2000: training loss 0.711694 val loss: 0.764037\n",
      "iteration 250 / 2000: training loss 0.678908 val loss: 0.752296\n",
      "iteration 300 / 2000: training loss 0.678235 val loss: 0.761219\n",
      "iteration 350 / 2000: training loss 0.753271 val loss: 0.740131\n",
      "iteration 400 / 2000: training loss 0.764661 val loss: 0.743011\n",
      "iteration 450 / 2000: training loss 0.759533 val loss: 0.735592\n",
      "iteration 500 / 2000: training loss 0.707377 val loss: 0.728933\n",
      "iteration 550 / 2000: training loss 0.722417 val loss: 0.735129\n",
      "iteration 600 / 2000: training loss 0.647744 val loss: 0.723169\n",
      "iteration 650 / 2000: training loss 0.643319 val loss: 0.728242\n",
      "iteration 700 / 2000: training loss 0.746220 val loss: 0.730508\n",
      "iteration 750 / 2000: training loss 0.727245 val loss: 0.722285\n",
      "iteration 800 / 2000: training loss 0.749741 val loss: 0.730948\n",
      "iteration 850 / 2000: training loss 0.736995 val loss: 0.723142\n",
      "iteration 900 / 2000: training loss 0.700235 val loss: 0.722703\n",
      "iteration 950 / 2000: training loss 0.665102 val loss: 0.722234\n",
      "iteration 1000 / 2000: training loss 0.702573 val loss: 0.721349\n",
      "iteration 1050 / 2000: training loss 0.703473 val loss: 0.720779\n",
      "iteration 1100 / 2000: training loss 0.707437 val loss: 0.721466\n",
      "iteration 1150 / 2000: training loss 0.682992 val loss: 0.719683\n",
      "iteration 1200 / 2000: training loss 0.728789 val loss: 0.719912\n",
      "iteration 1250 / 2000: training loss 0.747646 val loss: 0.720187\n",
      "iteration 1300 / 2000: training loss 0.728621 val loss: 0.719531\n",
      "iteration 1350 / 2000: training loss 0.738568 val loss: 0.719667\n",
      "iteration 1400 / 2000: training loss 0.686172 val loss: 0.719136\n",
      "iteration 1450 / 2000: training loss 0.742236 val loss: 0.719292\n",
      "iteration 1500 / 2000: training loss 0.691734 val loss: 0.718559\n",
      "iteration 1550 / 2000: training loss 0.744830 val loss: 0.719618\n",
      "iteration 1600 / 2000: training loss 0.736392 val loss: 0.719176\n",
      "iteration 1650 / 2000: training loss 0.709111 val loss: 0.717687\n",
      "iteration 1700 / 2000: training loss 0.672628 val loss: 0.718480\n",
      "iteration 1750 / 2000: training loss 0.636576 val loss: 0.717517\n",
      "iteration 1800 / 2000: training loss 0.760295 val loss: 0.718331\n",
      "iteration 1850 / 2000: training loss 0.710097 val loss: 0.717539\n",
      "hidden_size: 75 lr: 1.000000e-01 lr_decay: 8.000000e-01 reg: 2.000000e-02 train accuracy: 0.920000 val accuracy: 0.932000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302594\n",
      "iteration 50 / 2000: training loss 2.115975 val loss: 2.106633\n",
      "iteration 100 / 2000: training loss 1.212474 val loss: 1.206481\n",
      "iteration 150 / 2000: training loss 1.047169 val loss: 1.075519\n",
      "iteration 200 / 2000: training loss 1.034671 val loss: 1.093004\n",
      "iteration 250 / 2000: training loss 1.035447 val loss: 1.090239\n",
      "iteration 300 / 2000: training loss 1.005227 val loss: 1.074456\n",
      "iteration 350 / 2000: training loss 1.081843 val loss: 1.102800\n",
      "iteration 400 / 2000: training loss 1.096851 val loss: 1.049284\n",
      "iteration 450 / 2000: training loss 1.136599 val loss: 1.090742\n",
      "hidden_size: 75 lr: 1.000000e-01 lr_decay: 9.800000e-01 reg: 4.000000e-02 train accuracy: 0.890000 val accuracy: 0.890000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 50 / 2000: training loss 2.186199 val loss: 2.174309\n",
      "iteration 100 / 2000: training loss 1.204019 val loss: 1.217178\n",
      "iteration 150 / 2000: training loss 1.029598 val loss: 1.082570\n",
      "iteration 200 / 2000: training loss 1.037274 val loss: 1.073431\n",
      "iteration 250 / 2000: training loss 0.990832 val loss: 1.067846\n",
      "iteration 300 / 2000: training loss 1.004517 val loss: 1.080094\n",
      "iteration 350 / 2000: training loss 1.076236 val loss: 1.068372\n",
      "iteration 400 / 2000: training loss 1.059458 val loss: 1.052629\n",
      "iteration 450 / 2000: training loss 1.092332 val loss: 1.051515\n",
      "iteration 500 / 2000: training loss 1.037103 val loss: 1.042071\n",
      "iteration 550 / 2000: training loss 1.015744 val loss: 1.036358\n",
      "iteration 600 / 2000: training loss 0.964495 val loss: 1.043563\n",
      "iteration 650 / 2000: training loss 0.951637 val loss: 1.035318\n",
      "iteration 700 / 2000: training loss 1.048555 val loss: 1.047056\n",
      "iteration 750 / 2000: training loss 1.038137 val loss: 1.037339\n",
      "iteration 800 / 2000: training loss 1.024504 val loss: 1.055032\n",
      "iteration 850 / 2000: training loss 1.068578 val loss: 1.030439\n",
      "iteration 900 / 2000: training loss 1.003331 val loss: 1.036343\n",
      "iteration 950 / 2000: training loss 0.989056 val loss: 1.028441\n",
      "iteration 1000 / 2000: training loss 1.018232 val loss: 1.028811\n",
      "iteration 1050 / 2000: training loss 1.016313 val loss: 1.023969\n",
      "iteration 1100 / 2000: training loss 1.013228 val loss: 1.029791\n",
      "iteration 1150 / 2000: training loss 0.985764 val loss: 1.032846\n",
      "iteration 1200 / 2000: training loss 1.068813 val loss: 1.036246\n",
      "iteration 1250 / 2000: training loss 1.077751 val loss: 1.031061\n",
      "iteration 1300 / 2000: training loss 1.035917 val loss: 1.030048\n",
      "iteration 1350 / 2000: training loss 1.112868 val loss: 1.043338\n",
      "hidden_size: 75 lr: 1.000000e-01 lr_decay: 9.000000e-01 reg: 4.000000e-02 train accuracy: 0.870000 val accuracy: 0.909000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302594\n",
      "iteration 50 / 2000: training loss 2.225950 val loss: 2.212919\n",
      "iteration 100 / 2000: training loss 1.217026 val loss: 1.224972\n",
      "iteration 150 / 2000: training loss 1.018665 val loss: 1.088852\n",
      "iteration 200 / 2000: training loss 1.040617 val loss: 1.075996\n",
      "iteration 250 / 2000: training loss 0.985907 val loss: 1.058849\n",
      "iteration 300 / 2000: training loss 0.996563 val loss: 1.077317\n",
      "iteration 350 / 2000: training loss 1.068937 val loss: 1.060478\n",
      "iteration 400 / 2000: training loss 1.072710 val loss: 1.050055\n",
      "iteration 450 / 2000: training loss 1.082665 val loss: 1.049355\n",
      "iteration 500 / 2000: training loss 1.020643 val loss: 1.041066\n",
      "iteration 550 / 2000: training loss 1.029752 val loss: 1.032370\n",
      "iteration 600 / 2000: training loss 0.968233 val loss: 1.038191\n",
      "iteration 650 / 2000: training loss 0.959143 val loss: 1.036969\n",
      "iteration 700 / 2000: training loss 1.046543 val loss: 1.039213\n",
      "iteration 750 / 2000: training loss 1.042322 val loss: 1.030531\n",
      "iteration 800 / 2000: training loss 1.035864 val loss: 1.044547\n",
      "iteration 850 / 2000: training loss 1.074579 val loss: 1.031966\n",
      "iteration 900 / 2000: training loss 1.011552 val loss: 1.035363\n",
      "iteration 950 / 2000: training loss 0.987302 val loss: 1.027387\n",
      "iteration 1000 / 2000: training loss 1.025097 val loss: 1.025629\n",
      "iteration 1050 / 2000: training loss 1.018414 val loss: 1.027612\n",
      "iteration 1100 / 2000: training loss 1.003801 val loss: 1.026594\n",
      "iteration 1150 / 2000: training loss 0.981554 val loss: 1.024662\n",
      "iteration 1200 / 2000: training loss 1.054381 val loss: 1.024451\n",
      "iteration 1250 / 2000: training loss 1.062061 val loss: 1.027524\n",
      "iteration 1300 / 2000: training loss 1.034482 val loss: 1.026945\n",
      "iteration 1350 / 2000: training loss 1.070164 val loss: 1.025821\n",
      "iteration 1400 / 2000: training loss 1.020882 val loss: 1.025862\n",
      "iteration 1450 / 2000: training loss 1.061809 val loss: 1.029099\n",
      "iteration 1500 / 2000: training loss 1.006152 val loss: 1.022637\n",
      "iteration 1550 / 2000: training loss 1.026853 val loss: 1.023721\n",
      "iteration 1600 / 2000: training loss 1.057102 val loss: 1.021497\n",
      "iteration 1650 / 2000: training loss 1.024021 val loss: 1.019910\n",
      "iteration 1700 / 2000: training loss 0.980186 val loss: 1.022426\n",
      "iteration 1750 / 2000: training loss 0.934151 val loss: 1.020848\n",
      "iteration 1800 / 2000: training loss 1.071347 val loss: 1.019310\n",
      "iteration 1850 / 2000: training loss 1.026039 val loss: 1.020235\n",
      "iteration 1900 / 2000: training loss 0.943513 val loss: 1.020013\n",
      "iteration 1950 / 2000: training loss 1.027802 val loss: 1.019600\n",
      "hidden_size: 75 lr: 1.000000e-01 lr_decay: 8.500000e-01 reg: 4.000000e-02 train accuracy: 0.910000 val accuracy: 0.922000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302594\n",
      "iteration 50 / 2000: training loss 2.256587 val loss: 2.244920\n",
      "iteration 100 / 2000: training loss 1.265862 val loss: 1.257383\n",
      "iteration 150 / 2000: training loss 1.017709 val loss: 1.094828\n",
      "iteration 200 / 2000: training loss 1.031094 val loss: 1.075778\n",
      "iteration 250 / 2000: training loss 0.990659 val loss: 1.058309\n",
      "iteration 300 / 2000: training loss 0.990058 val loss: 1.073079\n",
      "iteration 350 / 2000: training loss 1.063335 val loss: 1.052270\n",
      "iteration 400 / 2000: training loss 1.095981 val loss: 1.053722\n",
      "iteration 450 / 2000: training loss 1.086588 val loss: 1.044030\n",
      "iteration 500 / 2000: training loss 1.018513 val loss: 1.037275\n",
      "iteration 550 / 2000: training loss 1.030933 val loss: 1.040098\n",
      "iteration 600 / 2000: training loss 0.962648 val loss: 1.030636\n",
      "iteration 650 / 2000: training loss 0.961178 val loss: 1.035141\n",
      "iteration 700 / 2000: training loss 1.056628 val loss: 1.036183\n",
      "iteration 750 / 2000: training loss 1.043866 val loss: 1.029721\n",
      "iteration 800 / 2000: training loss 1.048656 val loss: 1.035884\n",
      "iteration 850 / 2000: training loss 1.061504 val loss: 1.030432\n",
      "iteration 900 / 2000: training loss 1.009982 val loss: 1.030002\n",
      "hidden_size: 75 lr: 1.000000e-01 lr_decay: 8.000000e-01 reg: 4.000000e-02 train accuracy: 0.880000 val accuracy: 0.918000\n",
      "iteration 0 / 2000: training loss 2.302591 val loss: 2.302596\n",
      "iteration 50 / 2000: training loss 2.167135 val loss: 2.161334\n",
      "iteration 100 / 2000: training loss 1.448594 val loss: 1.429676\n",
      "iteration 150 / 2000: training loss 1.280850 val loss: 1.316911\n",
      "iteration 200 / 2000: training loss 1.268890 val loss: 1.335195\n",
      "iteration 250 / 2000: training loss 1.275290 val loss: 1.327124\n",
      "iteration 300 / 2000: training loss 1.241267 val loss: 1.313444\n",
      "iteration 350 / 2000: training loss 1.338503 val loss: 1.342765\n",
      "iteration 400 / 2000: training loss 1.363775 val loss: 1.291320\n",
      "iteration 450 / 2000: training loss 1.385926 val loss: 1.331575\n",
      "hidden_size: 75 lr: 1.000000e-01 lr_decay: 9.800000e-01 reg: 6.000000e-02 train accuracy: 0.890000 val accuracy: 0.862000\n",
      "iteration 0 / 2000: training loss 2.302591 val loss: 2.302596\n",
      "iteration 50 / 2000: training loss 2.225348 val loss: 2.214545\n",
      "iteration 100 / 2000: training loss 1.447580 val loss: 1.440473\n",
      "iteration 150 / 2000: training loss 1.272066 val loss: 1.328579\n",
      "iteration 200 / 2000: training loss 1.274859 val loss: 1.319850\n",
      "iteration 250 / 2000: training loss 1.239181 val loss: 1.303110\n",
      "iteration 300 / 2000: training loss 1.245991 val loss: 1.321527\n",
      "iteration 350 / 2000: training loss 1.323699 val loss: 1.310549\n",
      "iteration 400 / 2000: training loss 1.303818 val loss: 1.285266\n",
      "iteration 450 / 2000: training loss 1.347888 val loss: 1.291529\n",
      "hidden_size: 75 lr: 1.000000e-01 lr_decay: 9.000000e-01 reg: 6.000000e-02 train accuracy: 0.870000 val accuracy: 0.884000\n",
      "iteration 0 / 2000: training loss 2.302591 val loss: 2.302596\n",
      "iteration 50 / 2000: training loss 2.254208 val loss: 2.243543\n",
      "iteration 100 / 2000: training loss 1.486873 val loss: 1.467627\n",
      "iteration 150 / 2000: training loss 1.263143 val loss: 1.327750\n",
      "iteration 200 / 2000: training loss 1.281871 val loss: 1.313519\n",
      "iteration 250 / 2000: training loss 1.230760 val loss: 1.298679\n",
      "iteration 300 / 2000: training loss 1.229869 val loss: 1.314774\n",
      "iteration 350 / 2000: training loss 1.324750 val loss: 1.302851\n",
      "iteration 400 / 2000: training loss 1.317228 val loss: 1.294114\n",
      "iteration 450 / 2000: training loss 1.327403 val loss: 1.285736\n",
      "iteration 500 / 2000: training loss 1.260532 val loss: 1.279306\n",
      "iteration 550 / 2000: training loss 1.261501 val loss: 1.268237\n",
      "iteration 600 / 2000: training loss 1.209425 val loss: 1.275591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 650 / 2000: training loss 1.199573 val loss: 1.274390\n",
      "iteration 700 / 2000: training loss 1.276671 val loss: 1.277475\n",
      "iteration 750 / 2000: training loss 1.279906 val loss: 1.267102\n",
      "iteration 800 / 2000: training loss 1.265907 val loss: 1.284750\n",
      "iteration 850 / 2000: training loss 1.318994 val loss: 1.270027\n",
      "iteration 900 / 2000: training loss 1.241341 val loss: 1.273688\n",
      "iteration 950 / 2000: training loss 1.225226 val loss: 1.263917\n",
      "iteration 1000 / 2000: training loss 1.263746 val loss: 1.262035\n",
      "iteration 1050 / 2000: training loss 1.263734 val loss: 1.264891\n",
      "iteration 1100 / 2000: training loss 1.238328 val loss: 1.262775\n",
      "iteration 1150 / 2000: training loss 1.213018 val loss: 1.261539\n",
      "hidden_size: 75 lr: 1.000000e-01 lr_decay: 8.500000e-01 reg: 6.000000e-02 train accuracy: 0.890000 val accuracy: 0.906000\n",
      "iteration 0 / 2000: training loss 2.302591 val loss: 2.302596\n",
      "iteration 50 / 2000: training loss 2.275006 val loss: 2.265725\n",
      "iteration 100 / 2000: training loss 1.547296 val loss: 1.522306\n",
      "iteration 150 / 2000: training loss 1.261453 val loss: 1.330036\n",
      "iteration 200 / 2000: training loss 1.280766 val loss: 1.315912\n",
      "iteration 250 / 2000: training loss 1.231739 val loss: 1.298425\n",
      "iteration 300 / 2000: training loss 1.224916 val loss: 1.311266\n",
      "iteration 350 / 2000: training loss 1.313044 val loss: 1.293563\n",
      "hidden_size: 75 lr: 1.000000e-01 lr_decay: 8.000000e-01 reg: 6.000000e-02 train accuracy: 0.890000 val accuracy: 0.881000\n",
      "iteration 0 / 2000: training loss 2.302586 val loss: 2.302634\n",
      "iteration 50 / 2000: training loss 0.841130 val loss: 0.835065\n",
      "iteration 100 / 2000: training loss 0.605990 val loss: 0.690744\n",
      "iteration 150 / 2000: training loss 0.824064 val loss: 0.677507\n",
      "iteration 200 / 2000: training loss 0.607320 val loss: 0.613185\n",
      "iteration 250 / 2000: training loss 0.659510 val loss: 0.666755\n",
      "iteration 300 / 2000: training loss 0.627834 val loss: 0.643737\n",
      "iteration 350 / 2000: training loss 0.591864 val loss: 0.672447\n",
      "iteration 400 / 2000: training loss 0.574786 val loss: 0.573924\n",
      "iteration 450 / 2000: training loss 0.609308 val loss: 0.596434\n",
      "iteration 500 / 2000: training loss 0.618614 val loss: 0.637754\n",
      "iteration 550 / 2000: training loss 0.647761 val loss: 0.669284\n",
      "iteration 600 / 2000: training loss 0.512333 val loss: 0.624070\n",
      "iteration 650 / 2000: training loss 0.476725 val loss: 0.598202\n",
      "iteration 700 / 2000: training loss 0.682848 val loss: 0.628073\n",
      "iteration 750 / 2000: training loss 0.571851 val loss: 0.599100\n",
      "iteration 800 / 2000: training loss 0.600176 val loss: 0.614312\n",
      "iteration 850 / 2000: training loss 0.611688 val loss: 0.577850\n",
      "iteration 900 / 2000: training loss 0.571724 val loss: 0.576303\n",
      "iteration 950 / 2000: training loss 0.540080 val loss: 0.607442\n",
      "iteration 1000 / 2000: training loss 0.552342 val loss: 0.591555\n",
      "iteration 1050 / 2000: training loss 0.539782 val loss: 0.593827\n",
      "hidden_size: 75 lr: 3.000000e-01 lr_decay: 9.800000e-01 reg: 1.000000e-02 train accuracy: 0.970000 val accuracy: 0.919000\n",
      "iteration 0 / 2000: training loss 2.302586 val loss: 2.302634\n",
      "iteration 50 / 2000: training loss 0.777986 val loss: 0.900051\n",
      "iteration 100 / 2000: training loss 0.573728 val loss: 0.603461\n",
      "iteration 150 / 2000: training loss 0.674119 val loss: 0.654459\n",
      "iteration 200 / 2000: training loss 0.588757 val loss: 0.641763\n",
      "iteration 250 / 2000: training loss 0.552671 val loss: 0.632439\n",
      "iteration 300 / 2000: training loss 0.575210 val loss: 0.627575\n",
      "iteration 350 / 2000: training loss 0.564035 val loss: 0.581194\n",
      "iteration 400 / 2000: training loss 0.553153 val loss: 0.544734\n",
      "iteration 450 / 2000: training loss 0.596206 val loss: 0.591136\n",
      "iteration 500 / 2000: training loss 0.564347 val loss: 0.564808\n",
      "iteration 550 / 2000: training loss 0.515598 val loss: 0.527650\n",
      "iteration 600 / 2000: training loss 0.448297 val loss: 0.553266\n",
      "iteration 650 / 2000: training loss 0.457403 val loss: 0.553022\n",
      "iteration 700 / 2000: training loss 0.532586 val loss: 0.517077\n",
      "iteration 750 / 2000: training loss 0.496262 val loss: 0.524765\n",
      "iteration 800 / 2000: training loss 0.547261 val loss: 0.539945\n",
      "iteration 850 / 2000: training loss 0.513225 val loss: 0.556142\n",
      "iteration 900 / 2000: training loss 0.508234 val loss: 0.534285\n",
      "iteration 950 / 2000: training loss 0.496346 val loss: 0.517593\n",
      "iteration 1000 / 2000: training loss 0.485355 val loss: 0.519829\n",
      "iteration 1050 / 2000: training loss 0.485603 val loss: 0.534523\n",
      "hidden_size: 75 lr: 3.000000e-01 lr_decay: 9.000000e-01 reg: 1.000000e-02 train accuracy: 0.920000 val accuracy: 0.936000\n",
      "iteration 0 / 2000: training loss 2.302586 val loss: 2.302634\n",
      "iteration 50 / 2000: training loss 0.790785 val loss: 0.967663\n",
      "iteration 100 / 2000: training loss 0.595114 val loss: 0.595915\n",
      "iteration 150 / 2000: training loss 0.595418 val loss: 0.621241\n",
      "iteration 200 / 2000: training loss 0.548487 val loss: 0.633949\n",
      "iteration 250 / 2000: training loss 0.535607 val loss: 0.607786\n",
      "iteration 300 / 2000: training loss 0.508150 val loss: 0.569416\n",
      "iteration 350 / 2000: training loss 0.580034 val loss: 0.589857\n",
      "iteration 400 / 2000: training loss 0.507565 val loss: 0.528618\n",
      "iteration 450 / 2000: training loss 0.579270 val loss: 0.577009\n",
      "hidden_size: 75 lr: 3.000000e-01 lr_decay: 8.500000e-01 reg: 1.000000e-02 train accuracy: 0.920000 val accuracy: 0.922000\n",
      "iteration 0 / 2000: training loss 2.302586 val loss: 2.302634\n",
      "iteration 50 / 2000: training loss 0.843285 val loss: 0.916673\n",
      "iteration 100 / 2000: training loss 0.641131 val loss: 0.605991\n",
      "iteration 150 / 2000: training loss 0.581714 val loss: 0.602948\n",
      "iteration 200 / 2000: training loss 0.511709 val loss: 0.606611\n",
      "iteration 250 / 2000: training loss 0.531812 val loss: 0.581329\n",
      "iteration 300 / 2000: training loss 0.500052 val loss: 0.555924\n",
      "iteration 350 / 2000: training loss 0.547585 val loss: 0.577064\n",
      "iteration 400 / 2000: training loss 0.497543 val loss: 0.534195\n",
      "iteration 450 / 2000: training loss 0.516223 val loss: 0.550100\n",
      "iteration 500 / 2000: training loss 0.505615 val loss: 0.510048\n",
      "iteration 550 / 2000: training loss 0.517764 val loss: 0.507358\n",
      "iteration 600 / 2000: training loss 0.435514 val loss: 0.516568\n",
      "iteration 650 / 2000: training loss 0.425929 val loss: 0.510528\n",
      "iteration 700 / 2000: training loss 0.519606 val loss: 0.515670\n",
      "iteration 750 / 2000: training loss 0.482414 val loss: 0.506962\n",
      "iteration 800 / 2000: training loss 0.520698 val loss: 0.518450\n",
      "iteration 850 / 2000: training loss 0.514895 val loss: 0.507776\n",
      "iteration 900 / 2000: training loss 0.475346 val loss: 0.512262\n",
      "iteration 950 / 2000: training loss 0.453071 val loss: 0.504368\n",
      "iteration 1000 / 2000: training loss 0.478449 val loss: 0.503161\n",
      "iteration 1050 / 2000: training loss 0.477705 val loss: 0.501566\n",
      "iteration 1100 / 2000: training loss 0.494780 val loss: 0.500163\n",
      "iteration 1150 / 2000: training loss 0.450580 val loss: 0.502997\n",
      "iteration 1200 / 2000: training loss 0.499003 val loss: 0.501005\n",
      "iteration 1250 / 2000: training loss 0.532967 val loss: 0.503771\n",
      "hidden_size: 75 lr: 3.000000e-01 lr_decay: 8.000000e-01 reg: 1.000000e-02 train accuracy: 0.970000 val accuracy: 0.943000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302635\n",
      "iteration 50 / 2000: training loss 1.008458 val loss: 1.115441\n",
      "iteration 100 / 2000: training loss 0.861949 val loss: 0.901081\n",
      "iteration 150 / 2000: training loss 1.101249 val loss: 0.955604\n",
      "iteration 200 / 2000: training loss 0.801333 val loss: 0.834548\n",
      "iteration 250 / 2000: training loss 0.788903 val loss: 0.897758\n",
      "iteration 300 / 2000: training loss 0.823965 val loss: 0.977005\n",
      "iteration 350 / 2000: training loss 0.888601 val loss: 0.859405\n",
      "iteration 400 / 2000: training loss 0.855620 val loss: 0.854591\n",
      "iteration 450 / 2000: training loss 0.882398 val loss: 0.865735\n",
      "iteration 500 / 2000: training loss 0.767392 val loss: 0.813519\n",
      "hidden_size: 75 lr: 3.000000e-01 lr_decay: 9.800000e-01 reg: 2.000000e-02 train accuracy: 0.920000 val accuracy: 0.862000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302635\n",
      "iteration 50 / 2000: training loss 1.011380 val loss: 1.132981\n",
      "iteration 100 / 2000: training loss 0.847660 val loss: 0.839595\n",
      "iteration 150 / 2000: training loss 0.979117 val loss: 0.872731\n",
      "iteration 200 / 2000: training loss 0.783967 val loss: 0.829940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 250 / 2000: training loss 0.828609 val loss: 0.815273\n",
      "hidden_size: 75 lr: 3.000000e-01 lr_decay: 9.000000e-01 reg: 2.000000e-02 train accuracy: 0.870000 val accuracy: 0.880000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302635\n",
      "iteration 50 / 2000: training loss 1.048275 val loss: 1.128260\n",
      "iteration 100 / 2000: training loss 0.831229 val loss: 0.817672\n",
      "iteration 150 / 2000: training loss 0.930096 val loss: 0.864649\n",
      "iteration 200 / 2000: training loss 0.777636 val loss: 0.843361\n",
      "iteration 250 / 2000: training loss 0.772281 val loss: 0.826591\n",
      "iteration 300 / 2000: training loss 0.749993 val loss: 0.797476\n",
      "iteration 350 / 2000: training loss 0.753728 val loss: 0.802960\n",
      "iteration 400 / 2000: training loss 0.785848 val loss: 0.754123\n",
      "iteration 450 / 2000: training loss 0.829275 val loss: 0.795407\n",
      "iteration 500 / 2000: training loss 0.777093 val loss: 0.763295\n",
      "iteration 550 / 2000: training loss 0.730986 val loss: 0.741960\n",
      "iteration 600 / 2000: training loss 0.655867 val loss: 0.744447\n",
      "iteration 650 / 2000: training loss 0.650634 val loss: 0.750191\n",
      "iteration 700 / 2000: training loss 0.755902 val loss: 0.727150\n",
      "iteration 750 / 2000: training loss 0.721717 val loss: 0.736608\n",
      "iteration 800 / 2000: training loss 0.719879 val loss: 0.741886\n",
      "iteration 850 / 2000: training loss 0.725373 val loss: 0.734701\n",
      "iteration 900 / 2000: training loss 0.702201 val loss: 0.735188\n",
      "iteration 950 / 2000: training loss 0.681960 val loss: 0.723640\n",
      "iteration 1000 / 2000: training loss 0.699126 val loss: 0.720045\n",
      "iteration 1050 / 2000: training loss 0.696807 val loss: 0.718711\n",
      "iteration 1100 / 2000: training loss 0.719277 val loss: 0.720783\n",
      "iteration 1150 / 2000: training loss 0.668730 val loss: 0.723348\n",
      "iteration 1200 / 2000: training loss 0.746677 val loss: 0.726258\n",
      "iteration 1250 / 2000: training loss 0.766702 val loss: 0.720928\n",
      "iteration 1300 / 2000: training loss 0.719158 val loss: 0.718676\n",
      "iteration 1350 / 2000: training loss 0.774210 val loss: 0.727472\n",
      "iteration 1400 / 2000: training loss 0.678831 val loss: 0.712172\n",
      "iteration 1450 / 2000: training loss 0.756630 val loss: 0.717334\n",
      "iteration 1500 / 2000: training loss 0.690898 val loss: 0.713170\n",
      "iteration 1550 / 2000: training loss 0.748080 val loss: 0.715181\n",
      "iteration 1600 / 2000: training loss 0.731492 val loss: 0.710203\n",
      "iteration 1650 / 2000: training loss 0.716496 val loss: 0.707492\n",
      "iteration 1700 / 2000: training loss 0.665250 val loss: 0.707573\n",
      "iteration 1750 / 2000: training loss 0.636952 val loss: 0.708982\n",
      "iteration 1800 / 2000: training loss 0.751876 val loss: 0.708377\n",
      "iteration 1850 / 2000: training loss 0.710522 val loss: 0.707631\n",
      "iteration 1900 / 2000: training loss 0.620059 val loss: 0.707647\n",
      "iteration 1950 / 2000: training loss 0.707954 val loss: 0.711318\n",
      "hidden_size: 75 lr: 3.000000e-01 lr_decay: 8.500000e-01 reg: 2.000000e-02 train accuracy: 0.930000 val accuracy: 0.933000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302635\n",
      "iteration 50 / 2000: training loss 1.111426 val loss: 1.129852\n",
      "iteration 100 / 2000: training loss 0.803734 val loss: 0.801904\n",
      "iteration 150 / 2000: training loss 0.849577 val loss: 0.860698\n",
      "iteration 200 / 2000: training loss 0.765045 val loss: 0.824192\n",
      "iteration 250 / 2000: training loss 0.742347 val loss: 0.822579\n",
      "iteration 300 / 2000: training loss 0.698895 val loss: 0.760442\n",
      "iteration 350 / 2000: training loss 0.773356 val loss: 0.799751\n",
      "iteration 400 / 2000: training loss 0.724117 val loss: 0.727067\n",
      "iteration 450 / 2000: training loss 0.803250 val loss: 0.776563\n",
      "hidden_size: 75 lr: 3.000000e-01 lr_decay: 8.000000e-01 reg: 2.000000e-02 train accuracy: 0.900000 val accuracy: 0.917000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302637\n",
      "iteration 50 / 2000: training loss 1.248736 val loss: 1.366140\n",
      "iteration 100 / 2000: training loss 1.202285 val loss: 1.289622\n",
      "iteration 150 / 2000: training loss 1.410368 val loss: 1.380898\n",
      "iteration 200 / 2000: training loss 1.147108 val loss: 1.146087\n",
      "iteration 250 / 2000: training loss 1.246959 val loss: 1.180703\n",
      "iteration 300 / 2000: training loss 1.137399 val loss: 1.329130\n",
      "iteration 350 / 2000: training loss 1.266762 val loss: 1.201865\n",
      "iteration 400 / 2000: training loss 1.249322 val loss: 1.139297\n",
      "iteration 450 / 2000: training loss 1.221648 val loss: 1.178732\n",
      "iteration 500 / 2000: training loss 1.119879 val loss: 1.192436\n",
      "iteration 550 / 2000: training loss 1.098297 val loss: 1.134609\n",
      "iteration 600 / 2000: training loss 1.096092 val loss: 1.174947\n",
      "iteration 650 / 2000: training loss 1.149781 val loss: 1.144387\n",
      "iteration 700 / 2000: training loss 1.288714 val loss: 1.225528\n",
      "iteration 750 / 2000: training loss 1.150065 val loss: 1.152044\n",
      "iteration 800 / 2000: training loss 1.096096 val loss: 1.144560\n",
      "iteration 850 / 2000: training loss 1.181664 val loss: 1.154466\n",
      "iteration 900 / 2000: training loss 1.117387 val loss: 1.162977\n",
      "iteration 950 / 2000: training loss 1.065810 val loss: 1.207398\n",
      "iteration 1000 / 2000: training loss 1.192201 val loss: 1.330701\n",
      "iteration 1050 / 2000: training loss 1.133144 val loss: 1.150035\n",
      "iteration 1100 / 2000: training loss 1.059917 val loss: 1.103962\n",
      "iteration 1150 / 2000: training loss 1.128837 val loss: 1.169370\n",
      "iteration 1200 / 2000: training loss 1.173560 val loss: 1.126184\n",
      "iteration 1250 / 2000: training loss 1.177015 val loss: 1.096206\n",
      "iteration 1300 / 2000: training loss 1.228970 val loss: 1.100938\n",
      "iteration 1350 / 2000: training loss 1.193079 val loss: 1.191758\n",
      "iteration 1400 / 2000: training loss 1.117590 val loss: 1.175977\n",
      "iteration 1450 / 2000: training loss 1.252661 val loss: 1.152317\n",
      "iteration 1500 / 2000: training loss 1.115543 val loss: 1.161987\n",
      "iteration 1550 / 2000: training loss 1.244906 val loss: 1.105879\n",
      "iteration 1600 / 2000: training loss 1.192463 val loss: 1.158965\n",
      "iteration 1650 / 2000: training loss 1.104889 val loss: 1.112140\n",
      "iteration 1700 / 2000: training loss 1.020070 val loss: 1.117298\n",
      "iteration 1750 / 2000: training loss 1.116444 val loss: 1.142382\n",
      "iteration 1800 / 2000: training loss 1.144356 val loss: 1.108339\n",
      "iteration 1850 / 2000: training loss 1.113571 val loss: 1.119297\n",
      "hidden_size: 75 lr: 3.000000e-01 lr_decay: 9.800000e-01 reg: 4.000000e-02 train accuracy: 0.920000 val accuracy: 0.875000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302637\n",
      "iteration 50 / 2000: training loss 1.352164 val loss: 1.341332\n",
      "iteration 100 / 2000: training loss 1.195353 val loss: 1.206284\n",
      "iteration 150 / 2000: training loss 1.287291 val loss: 1.283063\n",
      "iteration 200 / 2000: training loss 1.103872 val loss: 1.112679\n",
      "iteration 250 / 2000: training loss 1.171972 val loss: 1.142476\n",
      "iteration 300 / 2000: training loss 1.053426 val loss: 1.171521\n",
      "iteration 350 / 2000: training loss 1.151788 val loss: 1.121880\n",
      "iteration 400 / 2000: training loss 1.154483 val loss: 1.109710\n",
      "iteration 450 / 2000: training loss 1.143103 val loss: 1.120243\n",
      "iteration 500 / 2000: training loss 1.076310 val loss: 1.102793\n",
      "hidden_size: 75 lr: 3.000000e-01 lr_decay: 9.000000e-01 reg: 4.000000e-02 train accuracy: 0.890000 val accuracy: 0.840000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302637\n",
      "iteration 50 / 2000: training loss 1.416887 val loss: 1.399558\n",
      "iteration 100 / 2000: training loss 1.172041 val loss: 1.171699\n",
      "iteration 150 / 2000: training loss 1.207288 val loss: 1.225337\n",
      "iteration 200 / 2000: training loss 1.102128 val loss: 1.144980\n",
      "iteration 250 / 2000: training loss 1.146118 val loss: 1.100414\n",
      "iteration 300 / 2000: training loss 1.048941 val loss: 1.096849\n",
      "iteration 350 / 2000: training loss 1.112957 val loss: 1.175071\n",
      "iteration 400 / 2000: training loss 1.121349 val loss: 1.083098\n",
      "iteration 450 / 2000: training loss 1.158886 val loss: 1.100560\n",
      "iteration 500 / 2000: training loss 1.055649 val loss: 1.062994\n",
      "hidden_size: 75 lr: 3.000000e-01 lr_decay: 8.500000e-01 reg: 4.000000e-02 train accuracy: 0.930000 val accuracy: 0.853000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302637\n",
      "iteration 50 / 2000: training loss 1.467886 val loss: 1.448201\n",
      "iteration 100 / 2000: training loss 1.150568 val loss: 1.137900\n",
      "iteration 150 / 2000: training loss 1.168075 val loss: 1.181147\n",
      "iteration 200 / 2000: training loss 1.081677 val loss: 1.145506\n",
      "iteration 250 / 2000: training loss 1.092488 val loss: 1.107413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 300 / 2000: training loss 1.034156 val loss: 1.081728\n",
      "iteration 350 / 2000: training loss 1.054461 val loss: 1.108093\n",
      "iteration 400 / 2000: training loss 1.112380 val loss: 1.053786\n",
      "iteration 450 / 2000: training loss 1.146167 val loss: 1.095439\n",
      "iteration 500 / 2000: training loss 1.042468 val loss: 1.048539\n",
      "iteration 550 / 2000: training loss 1.015929 val loss: 1.055568\n",
      "iteration 600 / 2000: training loss 0.973009 val loss: 1.051190\n",
      "iteration 650 / 2000: training loss 0.949361 val loss: 1.039326\n",
      "iteration 700 / 2000: training loss 1.046281 val loss: 1.037315\n",
      "iteration 750 / 2000: training loss 1.033387 val loss: 1.034342\n",
      "iteration 800 / 2000: training loss 1.019682 val loss: 1.053179\n",
      "iteration 850 / 2000: training loss 1.066054 val loss: 1.026067\n",
      "iteration 900 / 2000: training loss 0.998533 val loss: 1.032013\n",
      "iteration 950 / 2000: training loss 0.985071 val loss: 1.021541\n",
      "iteration 1000 / 2000: training loss 1.013760 val loss: 1.019328\n",
      "iteration 1050 / 2000: training loss 1.013132 val loss: 1.020306\n",
      "iteration 1100 / 2000: training loss 1.004357 val loss: 1.024718\n",
      "iteration 1150 / 2000: training loss 0.974470 val loss: 1.019549\n",
      "iteration 1200 / 2000: training loss 1.051502 val loss: 1.020895\n",
      "iteration 1250 / 2000: training loss 1.063297 val loss: 1.024241\n",
      "iteration 1300 / 2000: training loss 1.025685 val loss: 1.022373\n",
      "iteration 1350 / 2000: training loss 1.070219 val loss: 1.021822\n",
      "iteration 1400 / 2000: training loss 1.015651 val loss: 1.020242\n",
      "iteration 1450 / 2000: training loss 1.058470 val loss: 1.024287\n",
      "iteration 1500 / 2000: training loss 1.001196 val loss: 1.017378\n",
      "iteration 1550 / 2000: training loss 1.019722 val loss: 1.018410\n",
      "iteration 1600 / 2000: training loss 1.051338 val loss: 1.016141\n",
      "iteration 1650 / 2000: training loss 1.019873 val loss: 1.014288\n",
      "iteration 1700 / 2000: training loss 0.976372 val loss: 1.016624\n",
      "iteration 1750 / 2000: training loss 0.931960 val loss: 1.014921\n",
      "iteration 1800 / 2000: training loss 1.063809 val loss: 1.013956\n",
      "iteration 1850 / 2000: training loss 1.020900 val loss: 1.014846\n",
      "iteration 1900 / 2000: training loss 0.939640 val loss: 1.014715\n",
      "iteration 1950 / 2000: training loss 1.019120 val loss: 1.014235\n",
      "hidden_size: 75 lr: 3.000000e-01 lr_decay: 8.000000e-01 reg: 4.000000e-02 train accuracy: 0.910000 val accuracy: 0.924000\n",
      "iteration 0 / 2000: training loss 2.302591 val loss: 2.302639\n",
      "iteration 50 / 2000: training loss 1.501911 val loss: 1.494936\n",
      "iteration 100 / 2000: training loss 1.509674 val loss: 1.569523\n",
      "iteration 150 / 2000: training loss 1.599612 val loss: 1.541733\n",
      "iteration 200 / 2000: training loss 1.417194 val loss: 1.399267\n",
      "iteration 250 / 2000: training loss 1.518923 val loss: 1.397408\n",
      "hidden_size: 75 lr: 3.000000e-01 lr_decay: 9.800000e-01 reg: 6.000000e-02 train accuracy: 0.740000 val accuracy: 0.763000\n",
      "iteration 0 / 2000: training loss 2.302591 val loss: 2.302639\n",
      "iteration 50 / 2000: training loss 1.611787 val loss: 1.573163\n",
      "iteration 100 / 2000: training loss 1.405510 val loss: 1.475943\n",
      "iteration 150 / 2000: training loss 1.505434 val loss: 1.492762\n",
      "iteration 200 / 2000: training loss 1.338491 val loss: 1.347872\n",
      "iteration 250 / 2000: training loss 1.436096 val loss: 1.381968\n",
      "iteration 300 / 2000: training loss 1.356183 val loss: 1.451479\n",
      "iteration 350 / 2000: training loss 1.436919 val loss: 1.449719\n",
      "iteration 400 / 2000: training loss 1.438396 val loss: 1.361804\n",
      "iteration 450 / 2000: training loss 1.419905 val loss: 1.366680\n",
      "iteration 500 / 2000: training loss 1.300954 val loss: 1.340462\n",
      "iteration 550 / 2000: training loss 1.285469 val loss: 1.349639\n",
      "iteration 600 / 2000: training loss 1.320730 val loss: 1.370075\n",
      "iteration 650 / 2000: training loss 1.221381 val loss: 1.290209\n",
      "iteration 700 / 2000: training loss 1.337933 val loss: 1.363006\n",
      "iteration 750 / 2000: training loss 1.367593 val loss: 1.309833\n",
      "iteration 800 / 2000: training loss 1.304242 val loss: 1.306593\n",
      "iteration 850 / 2000: training loss 1.360678 val loss: 1.317015\n",
      "iteration 900 / 2000: training loss 1.349741 val loss: 1.323836\n",
      "iteration 950 / 2000: training loss 1.270112 val loss: 1.317099\n",
      "iteration 1000 / 2000: training loss 1.304013 val loss: 1.349599\n",
      "iteration 1050 / 2000: training loss 1.282144 val loss: 1.293881\n",
      "hidden_size: 75 lr: 3.000000e-01 lr_decay: 9.000000e-01 reg: 6.000000e-02 train accuracy: 0.850000 val accuracy: 0.868000\n",
      "iteration 0 / 2000: training loss 2.302591 val loss: 2.302639\n",
      "iteration 50 / 2000: training loss 1.622438 val loss: 1.622041\n",
      "iteration 100 / 2000: training loss 1.409938 val loss: 1.417459\n",
      "iteration 150 / 2000: training loss 1.462164 val loss: 1.481711\n",
      "iteration 200 / 2000: training loss 1.358833 val loss: 1.393143\n",
      "iteration 250 / 2000: training loss 1.434054 val loss: 1.359922\n",
      "iteration 300 / 2000: training loss 1.283847 val loss: 1.360698\n",
      "iteration 350 / 2000: training loss 1.383663 val loss: 1.411424\n",
      "iteration 400 / 2000: training loss 1.357731 val loss: 1.328737\n",
      "iteration 450 / 2000: training loss 1.406196 val loss: 1.334707\n",
      "iteration 500 / 2000: training loss 1.270777 val loss: 1.313484\n",
      "iteration 550 / 2000: training loss 1.249066 val loss: 1.311261\n",
      "iteration 600 / 2000: training loss 1.258392 val loss: 1.320428\n",
      "iteration 650 / 2000: training loss 1.231686 val loss: 1.298503\n",
      "iteration 700 / 2000: training loss 1.289676 val loss: 1.305948\n",
      "iteration 750 / 2000: training loss 1.291619 val loss: 1.273039\n",
      "iteration 800 / 2000: training loss 1.312107 val loss: 1.291236\n",
      "iteration 850 / 2000: training loss 1.301325 val loss: 1.290789\n",
      "iteration 900 / 2000: training loss 1.262721 val loss: 1.284953\n",
      "iteration 950 / 2000: training loss 1.252839 val loss: 1.282839\n",
      "iteration 1000 / 2000: training loss 1.263019 val loss: 1.280540\n",
      "iteration 1050 / 2000: training loss 1.265997 val loss: 1.280332\n",
      "hidden_size: 75 lr: 3.000000e-01 lr_decay: 8.500000e-01 reg: 6.000000e-02 train accuracy: 0.850000 val accuracy: 0.885000\n",
      "iteration 0 / 2000: training loss 2.302591 val loss: 2.302639\n",
      "iteration 50 / 2000: training loss 1.648383 val loss: 1.640227\n",
      "iteration 100 / 2000: training loss 1.411736 val loss: 1.397646\n",
      "iteration 150 / 2000: training loss 1.406058 val loss: 1.443283\n",
      "iteration 200 / 2000: training loss 1.331759 val loss: 1.394704\n",
      "iteration 250 / 2000: training loss 1.330540 val loss: 1.317539\n",
      "iteration 300 / 2000: training loss 1.276517 val loss: 1.335553\n",
      "iteration 350 / 2000: training loss 1.292769 val loss: 1.362490\n",
      "iteration 400 / 2000: training loss 1.380360 val loss: 1.289353\n",
      "iteration 450 / 2000: training loss 1.387368 val loss: 1.331208\n",
      "iteration 500 / 2000: training loss 1.280393 val loss: 1.290955\n",
      "iteration 550 / 2000: training loss 1.262706 val loss: 1.299652\n",
      "iteration 600 / 2000: training loss 1.229378 val loss: 1.295204\n",
      "iteration 650 / 2000: training loss 1.203591 val loss: 1.290787\n",
      "iteration 700 / 2000: training loss 1.271093 val loss: 1.274410\n",
      "iteration 750 / 2000: training loss 1.271209 val loss: 1.274814\n",
      "iteration 800 / 2000: training loss 1.255130 val loss: 1.292266\n",
      "iteration 850 / 2000: training loss 1.316065 val loss: 1.264985\n",
      "iteration 900 / 2000: training loss 1.230933 val loss: 1.273040\n",
      "iteration 950 / 2000: training loss 1.225435 val loss: 1.260432\n",
      "iteration 1000 / 2000: training loss 1.252791 val loss: 1.258532\n",
      "iteration 1050 / 2000: training loss 1.255769 val loss: 1.257772\n",
      "iteration 1100 / 2000: training loss 1.239649 val loss: 1.262499\n",
      "iteration 1150 / 2000: training loss 1.210466 val loss: 1.258936\n",
      "hidden_size: 75 lr: 3.000000e-01 lr_decay: 8.000000e-01 reg: 6.000000e-02 train accuracy: 0.900000 val accuracy: 0.903000\n",
      "iteration 0 / 2000: training loss 2.302586 val loss: 2.302719\n",
      "iteration 50 / 2000: training loss 1.150583 val loss: 1.159389\n",
      "iteration 100 / 2000: training loss 0.723165 val loss: 0.980274\n",
      "iteration 150 / 2000: training loss 1.273079 val loss: 0.903805\n",
      "iteration 200 / 2000: training loss 0.963023 val loss: 0.877973\n",
      "iteration 250 / 2000: training loss 0.884525 val loss: 0.898510\n",
      "iteration 300 / 2000: training loss 0.733719 val loss: 0.861404\n",
      "iteration 350 / 2000: training loss 0.761859 val loss: 0.752826\n",
      "iteration 400 / 2000: training loss 0.729690 val loss: 0.741086\n",
      "iteration 450 / 2000: training loss 0.794824 val loss: 0.756703\n",
      "iteration 500 / 2000: training loss 0.684422 val loss: 0.701970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 550 / 2000: training loss 0.694949 val loss: 0.716252\n",
      "iteration 600 / 2000: training loss 0.639865 val loss: 0.690329\n",
      "iteration 650 / 2000: training loss 0.628101 val loss: 0.787414\n",
      "iteration 700 / 2000: training loss 0.847289 val loss: 0.696636\n",
      "iteration 750 / 2000: training loss 0.644179 val loss: 0.680828\n",
      "iteration 800 / 2000: training loss 0.695442 val loss: 0.707624\n",
      "iteration 850 / 2000: training loss 0.775762 val loss: 0.715077\n",
      "iteration 900 / 2000: training loss 0.724057 val loss: 0.737552\n",
      "iteration 950 / 2000: training loss 0.582829 val loss: 0.705588\n",
      "iteration 1000 / 2000: training loss 0.756415 val loss: 0.932994\n",
      "iteration 1050 / 2000: training loss 0.629794 val loss: 0.763668\n",
      "iteration 1100 / 2000: training loss 0.661472 val loss: 0.716493\n",
      "iteration 1150 / 2000: training loss 0.643575 val loss: 0.666820\n",
      "hidden_size: 75 lr: 5.000000e-01 lr_decay: 9.800000e-01 reg: 1.000000e-02 train accuracy: 0.940000 val accuracy: 0.899000\n",
      "iteration 0 / 2000: training loss 2.302586 val loss: 2.302719\n",
      "iteration 50 / 2000: training loss 0.996115 val loss: 1.070892\n",
      "iteration 100 / 2000: training loss 0.682210 val loss: 0.832336\n",
      "iteration 150 / 2000: training loss 0.874069 val loss: 0.740087\n",
      "iteration 200 / 2000: training loss 0.593018 val loss: 0.648599\n",
      "iteration 250 / 2000: training loss 0.771533 val loss: 0.779351\n",
      "iteration 300 / 2000: training loss 0.618902 val loss: 0.718042\n",
      "iteration 350 / 2000: training loss 0.696009 val loss: 0.723309\n",
      "iteration 400 / 2000: training loss 0.573932 val loss: 0.592905\n",
      "iteration 450 / 2000: training loss 0.612640 val loss: 0.595193\n",
      "iteration 500 / 2000: training loss 0.563139 val loss: 0.624309\n",
      "iteration 550 / 2000: training loss 0.606131 val loss: 0.639094\n",
      "iteration 600 / 2000: training loss 0.478367 val loss: 0.608408\n",
      "iteration 650 / 2000: training loss 0.468377 val loss: 0.551656\n",
      "iteration 700 / 2000: training loss 0.609364 val loss: 0.586779\n",
      "iteration 750 / 2000: training loss 0.591309 val loss: 0.535270\n",
      "iteration 800 / 2000: training loss 0.579543 val loss: 0.557826\n",
      "iteration 850 / 2000: training loss 0.572959 val loss: 0.568440\n",
      "iteration 900 / 2000: training loss 0.521708 val loss: 0.543712\n",
      "iteration 950 / 2000: training loss 0.499001 val loss: 0.552070\n",
      "iteration 1000 / 2000: training loss 0.501033 val loss: 0.544793\n",
      "iteration 1050 / 2000: training loss 0.524132 val loss: 0.577300\n",
      "hidden_size: 75 lr: 5.000000e-01 lr_decay: 9.000000e-01 reg: 1.000000e-02 train accuracy: 0.950000 val accuracy: 0.926000\n",
      "iteration 0 / 2000: training loss 2.302586 val loss: 2.302719\n",
      "iteration 50 / 2000: training loss 0.971595 val loss: 0.970073\n",
      "iteration 100 / 2000: training loss 0.666150 val loss: 0.775307\n",
      "iteration 150 / 2000: training loss 0.834581 val loss: 0.732855\n",
      "iteration 200 / 2000: training loss 0.627113 val loss: 0.680467\n",
      "iteration 250 / 2000: training loss 0.646283 val loss: 0.649232\n",
      "iteration 300 / 2000: training loss 0.585874 val loss: 0.615858\n",
      "iteration 350 / 2000: training loss 0.571822 val loss: 0.643712\n",
      "iteration 400 / 2000: training loss 0.576629 val loss: 0.558904\n",
      "iteration 450 / 2000: training loss 0.605094 val loss: 0.591798\n",
      "iteration 500 / 2000: training loss 0.547623 val loss: 0.571252\n",
      "hidden_size: 75 lr: 5.000000e-01 lr_decay: 8.500000e-01 reg: 1.000000e-02 train accuracy: 0.960000 val accuracy: 0.889000\n",
      "iteration 0 / 2000: training loss 2.302586 val loss: 2.302719\n",
      "iteration 50 / 2000: training loss 0.945291 val loss: 1.011078\n",
      "iteration 100 / 2000: training loss 0.695628 val loss: 0.689650\n",
      "iteration 150 / 2000: training loss 0.778638 val loss: 0.636935\n",
      "iteration 200 / 2000: training loss 0.611413 val loss: 0.630636\n",
      "iteration 250 / 2000: training loss 0.615551 val loss: 0.649311\n",
      "iteration 300 / 2000: training loss 0.549250 val loss: 0.616687\n",
      "iteration 350 / 2000: training loss 0.565814 val loss: 0.576884\n",
      "iteration 400 / 2000: training loss 0.523185 val loss: 0.526447\n",
      "iteration 450 / 2000: training loss 0.596926 val loss: 0.582077\n",
      "iteration 500 / 2000: training loss 0.535163 val loss: 0.541422\n",
      "iteration 550 / 2000: training loss 0.513675 val loss: 0.531402\n",
      "iteration 600 / 2000: training loss 0.448778 val loss: 0.522344\n",
      "iteration 650 / 2000: training loss 0.426400 val loss: 0.510158\n",
      "iteration 700 / 2000: training loss 0.539983 val loss: 0.521196\n",
      "iteration 750 / 2000: training loss 0.501104 val loss: 0.514339\n",
      "iteration 800 / 2000: training loss 0.510305 val loss: 0.530644\n",
      "iteration 850 / 2000: training loss 0.496995 val loss: 0.509106\n",
      "iteration 900 / 2000: training loss 0.463130 val loss: 0.510513\n",
      "iteration 950 / 2000: training loss 0.461849 val loss: 0.502577\n",
      "iteration 1000 / 2000: training loss 0.472725 val loss: 0.500629\n",
      "iteration 1050 / 2000: training loss 0.475380 val loss: 0.501707\n",
      "hidden_size: 75 lr: 5.000000e-01 lr_decay: 8.000000e-01 reg: 1.000000e-02 train accuracy: 0.930000 val accuracy: 0.947000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302720\n",
      "iteration 50 / 2000: training loss 1.234035 val loss: 1.248220\n",
      "iteration 100 / 2000: training loss 1.049527 val loss: 1.004952\n",
      "iteration 150 / 2000: training loss 1.279511 val loss: 1.155784\n",
      "iteration 200 / 2000: training loss 0.890483 val loss: 1.125312\n",
      "iteration 250 / 2000: training loss 1.410579 val loss: 1.591840\n",
      "iteration 300 / 2000: training loss 1.026556 val loss: 1.144517\n",
      "iteration 350 / 2000: training loss 1.002281 val loss: 0.942653\n",
      "iteration 400 / 2000: training loss 1.025862 val loss: 0.897387\n",
      "iteration 450 / 2000: training loss 0.988389 val loss: 0.941856\n",
      "iteration 500 / 2000: training loss 0.942299 val loss: 1.025559\n",
      "iteration 550 / 2000: training loss 0.911922 val loss: 1.034957\n",
      "iteration 600 / 2000: training loss 0.873559 val loss: 0.986636\n",
      "iteration 650 / 2000: training loss 0.808108 val loss: 0.986120\n",
      "iteration 700 / 2000: training loss 1.098017 val loss: 1.073071\n",
      "iteration 750 / 2000: training loss 1.023910 val loss: 0.941733\n",
      "iteration 800 / 2000: training loss 0.903807 val loss: 0.931541\n",
      "iteration 850 / 2000: training loss 1.071010 val loss: 0.932762\n",
      "iteration 900 / 2000: training loss 0.888968 val loss: 1.039974\n",
      "iteration 950 / 2000: training loss 0.895470 val loss: 1.004250\n",
      "iteration 1000 / 2000: training loss 0.927929 val loss: 0.943399\n",
      "iteration 1050 / 2000: training loss 0.939911 val loss: 0.954492\n",
      "iteration 1100 / 2000: training loss 0.968841 val loss: 0.996378\n",
      "iteration 1150 / 2000: training loss 0.967017 val loss: 0.987913\n",
      "iteration 1200 / 2000: training loss 0.873899 val loss: 0.916073\n",
      "iteration 1250 / 2000: training loss 1.015292 val loss: 0.925016\n",
      "iteration 1300 / 2000: training loss 0.909829 val loss: 1.069571\n",
      "iteration 1350 / 2000: training loss 1.091563 val loss: 1.001778\n",
      "hidden_size: 75 lr: 5.000000e-01 lr_decay: 9.800000e-01 reg: 2.000000e-02 train accuracy: 0.810000 val accuracy: 0.839000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302720\n",
      "iteration 50 / 2000: training loss 1.175661 val loss: 1.193737\n",
      "iteration 100 / 2000: training loss 0.995254 val loss: 1.015982\n",
      "iteration 150 / 2000: training loss 1.153700 val loss: 1.061038\n",
      "iteration 200 / 2000: training loss 0.957310 val loss: 0.964768\n",
      "iteration 250 / 2000: training loss 0.901198 val loss: 1.017144\n",
      "iteration 300 / 2000: training loss 0.883678 val loss: 1.061368\n",
      "iteration 350 / 2000: training loss 0.892103 val loss: 0.908564\n",
      "iteration 400 / 2000: training loss 0.852676 val loss: 0.846259\n",
      "iteration 450 / 2000: training loss 0.887853 val loss: 0.877383\n",
      "iteration 500 / 2000: training loss 0.796299 val loss: 0.858053\n",
      "iteration 550 / 2000: training loss 0.824025 val loss: 0.823349\n",
      "iteration 600 / 2000: training loss 0.783540 val loss: 0.874694\n",
      "iteration 650 / 2000: training loss 0.680156 val loss: 0.776326\n",
      "iteration 700 / 2000: training loss 0.876457 val loss: 0.850598\n",
      "iteration 750 / 2000: training loss 0.772753 val loss: 0.778888\n",
      "iteration 800 / 2000: training loss 0.783309 val loss: 0.790905\n",
      "iteration 850 / 2000: training loss 0.831317 val loss: 0.769494\n",
      "iteration 900 / 2000: training loss 0.786647 val loss: 0.768562\n",
      "iteration 950 / 2000: training loss 0.749972 val loss: 0.784486\n",
      "iteration 1000 / 2000: training loss 0.733293 val loss: 0.761446\n",
      "iteration 1050 / 2000: training loss 0.713510 val loss: 0.757751\n",
      "iteration 1100 / 2000: training loss 0.745901 val loss: 0.759313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1150 / 2000: training loss 0.693394 val loss: 0.759493\n",
      "iteration 1200 / 2000: training loss 0.754707 val loss: 0.747077\n",
      "iteration 1250 / 2000: training loss 0.794239 val loss: 0.752309\n",
      "iteration 1300 / 2000: training loss 0.766263 val loss: 0.755743\n",
      "iteration 1350 / 2000: training loss 0.785602 val loss: 0.744539\n",
      "iteration 1400 / 2000: training loss 0.688274 val loss: 0.738673\n",
      "iteration 1450 / 2000: training loss 0.777444 val loss: 0.752301\n",
      "iteration 1500 / 2000: training loss 0.726276 val loss: 0.728569\n",
      "iteration 1550 / 2000: training loss 0.760795 val loss: 0.734917\n",
      "iteration 1600 / 2000: training loss 0.750116 val loss: 0.738806\n",
      "iteration 1650 / 2000: training loss 0.722447 val loss: 0.725624\n",
      "iteration 1700 / 2000: training loss 0.673341 val loss: 0.725404\n",
      "hidden_size: 75 lr: 5.000000e-01 lr_decay: 9.000000e-01 reg: 2.000000e-02 train accuracy: 0.920000 val accuracy: 0.922000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302720\n",
      "iteration 50 / 2000: training loss 1.133221 val loss: 1.163407\n",
      "iteration 100 / 2000: training loss 0.861434 val loss: 0.994248\n",
      "iteration 150 / 2000: training loss 1.134205 val loss: 0.997690\n",
      "iteration 200 / 2000: training loss 0.831342 val loss: 0.851756\n",
      "iteration 250 / 2000: training loss 0.866268 val loss: 0.926937\n",
      "iteration 300 / 2000: training loss 0.809236 val loss: 0.866194\n",
      "iteration 350 / 2000: training loss 0.881982 val loss: 0.873458\n",
      "iteration 400 / 2000: training loss 0.791808 val loss: 0.785518\n",
      "iteration 450 / 2000: training loss 0.820362 val loss: 0.793806\n",
      "iteration 500 / 2000: training loss 0.752579 val loss: 0.780713\n",
      "hidden_size: 75 lr: 5.000000e-01 lr_decay: 8.500000e-01 reg: 2.000000e-02 train accuracy: 0.920000 val accuracy: 0.854000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302720\n",
      "iteration 50 / 2000: training loss 1.168294 val loss: 1.176674\n",
      "iteration 100 / 2000: training loss 0.866982 val loss: 0.951835\n",
      "iteration 150 / 2000: training loss 1.074936 val loss: 0.983482\n",
      "iteration 200 / 2000: training loss 0.776561 val loss: 0.813691\n",
      "iteration 250 / 2000: training loss 0.872241 val loss: 0.832601\n",
      "iteration 300 / 2000: training loss 0.773369 val loss: 0.809720\n",
      "iteration 350 / 2000: training loss 0.798456 val loss: 0.874498\n",
      "iteration 400 / 2000: training loss 0.788326 val loss: 0.763404\n",
      "iteration 450 / 2000: training loss 0.834258 val loss: 0.787269\n",
      "iteration 500 / 2000: training loss 0.761593 val loss: 0.762086\n",
      "iteration 550 / 2000: training loss 0.733090 val loss: 0.743764\n",
      "iteration 600 / 2000: training loss 0.657545 val loss: 0.745330\n",
      "iteration 650 / 2000: training loss 0.647230 val loss: 0.749272\n",
      "iteration 700 / 2000: training loss 0.753080 val loss: 0.728137\n",
      "iteration 750 / 2000: training loss 0.721653 val loss: 0.734115\n",
      "iteration 800 / 2000: training loss 0.715891 val loss: 0.751473\n",
      "iteration 850 / 2000: training loss 0.727139 val loss: 0.725000\n",
      "iteration 900 / 2000: training loss 0.694341 val loss: 0.728044\n",
      "iteration 950 / 2000: training loss 0.673568 val loss: 0.716877\n",
      "iteration 1000 / 2000: training loss 0.692480 val loss: 0.714720\n",
      "iteration 1050 / 2000: training loss 0.697100 val loss: 0.713572\n",
      "iteration 1100 / 2000: training loss 0.710500 val loss: 0.719721\n",
      "iteration 1150 / 2000: training loss 0.666144 val loss: 0.716230\n",
      "iteration 1200 / 2000: training loss 0.744160 val loss: 0.717046\n",
      "iteration 1250 / 2000: training loss 0.754987 val loss: 0.720438\n",
      "iteration 1300 / 2000: training loss 0.707400 val loss: 0.713976\n",
      "iteration 1350 / 2000: training loss 0.750553 val loss: 0.716556\n",
      "iteration 1400 / 2000: training loss 0.671393 val loss: 0.712108\n",
      "iteration 1450 / 2000: training loss 0.737561 val loss: 0.718188\n",
      "iteration 1500 / 2000: training loss 0.685003 val loss: 0.709680\n",
      "iteration 1550 / 2000: training loss 0.738161 val loss: 0.711712\n",
      "iteration 1600 / 2000: training loss 0.731293 val loss: 0.707599\n",
      "iteration 1650 / 2000: training loss 0.705970 val loss: 0.706185\n",
      "iteration 1700 / 2000: training loss 0.666917 val loss: 0.708367\n",
      "iteration 1750 / 2000: training loss 0.631113 val loss: 0.707582\n",
      "iteration 1800 / 2000: training loss 0.736491 val loss: 0.705762\n",
      "iteration 1850 / 2000: training loss 0.704139 val loss: 0.706496\n",
      "iteration 1900 / 2000: training loss 0.619134 val loss: 0.706219\n",
      "iteration 1950 / 2000: training loss 0.700572 val loss: 0.706390\n",
      "hidden_size: 75 lr: 5.000000e-01 lr_decay: 8.000000e-01 reg: 2.000000e-02 train accuracy: 0.930000 val accuracy: 0.935000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302722\n",
      "iteration 50 / 2000: training loss 1.568620 val loss: 1.512954\n",
      "iteration 100 / 2000: training loss 1.477901 val loss: 1.413313\n",
      "iteration 150 / 2000: training loss 1.356868 val loss: 1.320575\n",
      "iteration 200 / 2000: training loss 1.363563 val loss: 1.446861\n",
      "iteration 250 / 2000: training loss 1.354945 val loss: 1.493378\n",
      "iteration 300 / 2000: training loss 1.356418 val loss: 1.479686\n",
      "iteration 350 / 2000: training loss 1.352025 val loss: 1.303460\n",
      "iteration 400 / 2000: training loss 1.431710 val loss: 1.413038\n",
      "iteration 450 / 2000: training loss 1.328926 val loss: 1.316993\n",
      "iteration 500 / 2000: training loss 1.484885 val loss: 1.385300\n",
      "iteration 550 / 2000: training loss 1.299818 val loss: 1.483965\n",
      "iteration 600 / 2000: training loss 1.350047 val loss: 1.440058\n",
      "hidden_size: 75 lr: 5.000000e-01 lr_decay: 9.800000e-01 reg: 4.000000e-02 train accuracy: 0.770000 val accuracy: 0.688000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302722\n",
      "iteration 50 / 2000: training loss 1.468762 val loss: 1.373645\n",
      "iteration 100 / 2000: training loss 1.210554 val loss: 1.367968\n",
      "iteration 150 / 2000: training loss 1.269215 val loss: 1.320148\n",
      "iteration 200 / 2000: training loss 1.183580 val loss: 1.257036\n",
      "iteration 250 / 2000: training loss 1.267508 val loss: 1.408348\n",
      "iteration 300 / 2000: training loss 1.202618 val loss: 1.405929\n",
      "iteration 350 / 2000: training loss 1.348414 val loss: 1.184043\n",
      "iteration 400 / 2000: training loss 1.273230 val loss: 1.157884\n",
      "iteration 450 / 2000: training loss 1.191413 val loss: 1.160166\n",
      "iteration 500 / 2000: training loss 1.090497 val loss: 1.152878\n",
      "iteration 550 / 2000: training loss 1.119311 val loss: 1.106142\n",
      "iteration 600 / 2000: training loss 1.128165 val loss: 1.156108\n",
      "iteration 650 / 2000: training loss 1.045205 val loss: 1.117929\n",
      "iteration 700 / 2000: training loss 1.157692 val loss: 1.142466\n",
      "iteration 750 / 2000: training loss 1.101865 val loss: 1.086920\n",
      "iteration 800 / 2000: training loss 1.080130 val loss: 1.097471\n",
      "iteration 850 / 2000: training loss 1.130796 val loss: 1.081089\n",
      "iteration 900 / 2000: training loss 1.132962 val loss: 1.088544\n",
      "iteration 950 / 2000: training loss 1.007402 val loss: 1.082880\n",
      "iteration 1000 / 2000: training loss 1.106055 val loss: 1.098738\n",
      "iteration 1050 / 2000: training loss 1.039697 val loss: 1.082346\n",
      "iteration 1100 / 2000: training loss 1.051514 val loss: 1.073431\n",
      "iteration 1150 / 2000: training loss 1.011402 val loss: 1.107642\n",
      "iteration 1200 / 2000: training loss 1.091819 val loss: 1.057858\n",
      "iteration 1250 / 2000: training loss 1.128073 val loss: 1.106148\n",
      "iteration 1300 / 2000: training loss 1.090620 val loss: 1.071069\n",
      "iteration 1350 / 2000: training loss 1.110744 val loss: 1.080001\n",
      "iteration 1400 / 2000: training loss 1.053807 val loss: 1.049719\n",
      "hidden_size: 75 lr: 5.000000e-01 lr_decay: 9.000000e-01 reg: 4.000000e-02 train accuracy: 0.940000 val accuracy: 0.898000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302722\n",
      "iteration 50 / 2000: training loss 1.466046 val loss: 1.299893\n",
      "iteration 100 / 2000: training loss 1.228350 val loss: 1.333744\n",
      "iteration 150 / 2000: training loss 1.533802 val loss: 1.323191\n",
      "iteration 200 / 2000: training loss 1.172713 val loss: 1.150080\n",
      "iteration 250 / 2000: training loss 1.292347 val loss: 1.199182\n",
      "iteration 300 / 2000: training loss 1.105891 val loss: 1.245397\n",
      "iteration 350 / 2000: training loss 1.236816 val loss: 1.216934\n",
      "iteration 400 / 2000: training loss 1.178859 val loss: 1.126117\n",
      "iteration 450 / 2000: training loss 1.173746 val loss: 1.149024\n",
      "iteration 500 / 2000: training loss 1.089121 val loss: 1.116392\n",
      "iteration 550 / 2000: training loss 1.048150 val loss: 1.117912\n",
      "iteration 600 / 2000: training loss 1.066476 val loss: 1.141300\n",
      "iteration 650 / 2000: training loss 0.979341 val loss: 1.054700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 700 / 2000: training loss 1.080531 val loss: 1.103763\n",
      "iteration 750 / 2000: training loss 1.103276 val loss: 1.064598\n",
      "iteration 800 / 2000: training loss 1.079149 val loss: 1.062764\n",
      "hidden_size: 75 lr: 5.000000e-01 lr_decay: 8.500000e-01 reg: 4.000000e-02 train accuracy: 0.880000 val accuracy: 0.886000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302722\n",
      "iteration 50 / 2000: training loss 1.578628 val loss: 1.527323\n",
      "iteration 100 / 2000: training loss 1.193321 val loss: 1.358533\n",
      "iteration 150 / 2000: training loss 1.356123 val loss: 1.321398\n",
      "iteration 200 / 2000: training loss 1.147202 val loss: 1.133422\n",
      "iteration 250 / 2000: training loss 1.187759 val loss: 1.170261\n",
      "iteration 300 / 2000: training loss 1.064580 val loss: 1.137171\n",
      "iteration 350 / 2000: training loss 1.162277 val loss: 1.137595\n",
      "iteration 400 / 2000: training loss 1.125218 val loss: 1.109062\n",
      "iteration 450 / 2000: training loss 1.155569 val loss: 1.111123\n",
      "iteration 500 / 2000: training loss 1.037072 val loss: 1.067603\n",
      "hidden_size: 75 lr: 5.000000e-01 lr_decay: 8.000000e-01 reg: 4.000000e-02 train accuracy: 0.890000 val accuracy: 0.846000\n",
      "iteration 0 / 2000: training loss 2.302591 val loss: 2.302723\n",
      "iteration 50 / 2000: training loss 1.742058 val loss: 1.717350\n",
      "iteration 100 / 2000: training loss 1.812044 val loss: 1.758269\n",
      "iteration 150 / 2000: training loss 1.729350 val loss: 1.663745\n",
      "iteration 200 / 2000: training loss 1.538468 val loss: 1.604978\n",
      "iteration 250 / 2000: training loss 1.610752 val loss: 1.753056\n",
      "iteration 300 / 2000: training loss 1.914119 val loss: 1.668885\n",
      "iteration 350 / 2000: training loss 1.661947 val loss: 1.664574\n",
      "hidden_size: 75 lr: 5.000000e-01 lr_decay: 9.800000e-01 reg: 6.000000e-02 train accuracy: 0.680000 val accuracy: 0.649000\n",
      "iteration 0 / 2000: training loss 2.302591 val loss: 2.302723\n",
      "iteration 50 / 2000: training loss 1.667550 val loss: 1.595333\n",
      "iteration 100 / 2000: training loss 1.589431 val loss: 1.516026\n",
      "iteration 150 / 2000: training loss 1.587580 val loss: 1.578967\n",
      "iteration 200 / 2000: training loss 1.459306 val loss: 1.505498\n",
      "iteration 250 / 2000: training loss 1.519095 val loss: 1.615700\n",
      "iteration 300 / 2000: training loss 1.440289 val loss: 1.522668\n",
      "iteration 350 / 2000: training loss 1.523014 val loss: 1.382212\n",
      "iteration 400 / 2000: training loss 1.473994 val loss: 1.460105\n",
      "iteration 450 / 2000: training loss 1.484709 val loss: 1.455183\n",
      "iteration 500 / 2000: training loss 1.328237 val loss: 1.393440\n",
      "iteration 550 / 2000: training loss 1.359571 val loss: 1.381527\n",
      "iteration 600 / 2000: training loss 1.372181 val loss: 1.424907\n",
      "iteration 650 / 2000: training loss 1.334122 val loss: 1.376789\n",
      "iteration 700 / 2000: training loss 1.427418 val loss: 1.436636\n",
      "iteration 750 / 2000: training loss 1.336322 val loss: 1.342688\n",
      "iteration 800 / 2000: training loss 1.309212 val loss: 1.334998\n",
      "hidden_size: 75 lr: 5.000000e-01 lr_decay: 9.000000e-01 reg: 6.000000e-02 train accuracy: 0.860000 val accuracy: 0.826000\n",
      "iteration 0 / 2000: training loss 2.302591 val loss: 2.302723\n",
      "iteration 50 / 2000: training loss 1.631661 val loss: 1.714220\n",
      "iteration 100 / 2000: training loss 1.489937 val loss: 1.506201\n",
      "iteration 150 / 2000: training loss 1.655059 val loss: 1.471405\n",
      "iteration 200 / 2000: training loss 1.400771 val loss: 1.418988\n",
      "iteration 250 / 2000: training loss 1.583025 val loss: 1.426826\n",
      "iteration 300 / 2000: training loss 1.373788 val loss: 1.544593\n",
      "iteration 350 / 2000: training loss 1.605676 val loss: 1.480841\n",
      "iteration 400 / 2000: training loss 1.454029 val loss: 1.398902\n",
      "iteration 450 / 2000: training loss 1.394211 val loss: 1.382192\n",
      "iteration 500 / 2000: training loss 1.296697 val loss: 1.345003\n",
      "iteration 550 / 2000: training loss 1.292776 val loss: 1.363546\n",
      "iteration 600 / 2000: training loss 1.337297 val loss: 1.375980\n",
      "iteration 650 / 2000: training loss 1.218517 val loss: 1.292308\n",
      "iteration 700 / 2000: training loss 1.336598 val loss: 1.375691\n",
      "iteration 750 / 2000: training loss 1.374353 val loss: 1.318316\n",
      "iteration 800 / 2000: training loss 1.307762 val loss: 1.306490\n",
      "iteration 850 / 2000: training loss 1.335987 val loss: 1.315633\n",
      "iteration 900 / 2000: training loss 1.322069 val loss: 1.315384\n",
      "iteration 950 / 2000: training loss 1.254852 val loss: 1.303838\n",
      "iteration 1000 / 2000: training loss 1.264959 val loss: 1.315883\n",
      "iteration 1050 / 2000: training loss 1.264126 val loss: 1.281346\n",
      "iteration 1100 / 2000: training loss 1.270343 val loss: 1.293437\n",
      "iteration 1150 / 2000: training loss 1.226699 val loss: 1.304636\n",
      "iteration 1200 / 2000: training loss 1.282996 val loss: 1.273774\n",
      "iteration 1250 / 2000: training loss 1.314051 val loss: 1.276877\n",
      "iteration 1300 / 2000: training loss 1.241579 val loss: 1.280082\n",
      "iteration 1350 / 2000: training loss 1.369762 val loss: 1.286930\n",
      "iteration 1400 / 2000: training loss 1.284530 val loss: 1.267789\n",
      "iteration 1450 / 2000: training loss 1.326928 val loss: 1.267075\n",
      "iteration 1500 / 2000: training loss 1.263043 val loss: 1.262992\n",
      "iteration 1550 / 2000: training loss 1.253097 val loss: 1.263741\n",
      "iteration 1600 / 2000: training loss 1.303879 val loss: 1.265089\n",
      "iteration 1650 / 2000: training loss 1.259503 val loss: 1.266551\n",
      "iteration 1700 / 2000: training loss 1.226930 val loss: 1.262423\n",
      "iteration 1750 / 2000: training loss 1.172730 val loss: 1.258535\n",
      "iteration 1800 / 2000: training loss 1.301023 val loss: 1.259731\n",
      "iteration 1850 / 2000: training loss 1.273988 val loss: 1.254773\n",
      "hidden_size: 75 lr: 5.000000e-01 lr_decay: 8.500000e-01 reg: 6.000000e-02 train accuracy: 0.900000 val accuracy: 0.893000\n",
      "iteration 0 / 2000: training loss 2.302591 val loss: 2.302723\n",
      "iteration 50 / 2000: training loss 1.665462 val loss: 1.673415\n",
      "iteration 100 / 2000: training loss 1.463361 val loss: 1.515217\n",
      "iteration 150 / 2000: training loss 1.678253 val loss: 1.524424\n",
      "iteration 200 / 2000: training loss 1.393669 val loss: 1.393130\n",
      "iteration 250 / 2000: training loss 1.479179 val loss: 1.418673\n",
      "iteration 300 / 2000: training loss 1.317828 val loss: 1.427369\n",
      "iteration 350 / 2000: training loss 1.441593 val loss: 1.448207\n",
      "iteration 400 / 2000: training loss 1.398340 val loss: 1.370843\n",
      "iteration 450 / 2000: training loss 1.418048 val loss: 1.357257\n",
      "iteration 500 / 2000: training loss 1.269451 val loss: 1.332704\n",
      "iteration 550 / 2000: training loss 1.249272 val loss: 1.327502\n",
      "iteration 600 / 2000: training loss 1.259287 val loss: 1.326544\n",
      "iteration 650 / 2000: training loss 1.229657 val loss: 1.300434\n",
      "iteration 700 / 2000: training loss 1.290888 val loss: 1.299970\n",
      "iteration 750 / 2000: training loss 1.287761 val loss: 1.271523\n",
      "iteration 800 / 2000: training loss 1.296743 val loss: 1.293080\n",
      "iteration 850 / 2000: training loss 1.312410 val loss: 1.276185\n",
      "iteration 900 / 2000: training loss 1.259934 val loss: 1.287707\n",
      "iteration 950 / 2000: training loss 1.244670 val loss: 1.281061\n",
      "iteration 1000 / 2000: training loss 1.263626 val loss: 1.271074\n",
      "iteration 1050 / 2000: training loss 1.259987 val loss: 1.267941\n",
      "iteration 1100 / 2000: training loss 1.250969 val loss: 1.265892\n",
      "iteration 1150 / 2000: training loss 1.213857 val loss: 1.268851\n",
      "iteration 1200 / 2000: training loss 1.312381 val loss: 1.271644\n",
      "iteration 1250 / 2000: training loss 1.311900 val loss: 1.263888\n",
      "iteration 1300 / 2000: training loss 1.262280 val loss: 1.261379\n",
      "iteration 1350 / 2000: training loss 1.341537 val loss: 1.274694\n",
      "hidden_size: 75 lr: 5.000000e-01 lr_decay: 8.000000e-01 reg: 6.000000e-02 train accuracy: 0.830000 val accuracy: 0.894000\n",
      "iteration 0 / 2000: training loss 2.302586 val loss: 2.302846\n",
      "iteration 50 / 2000: training loss 1.461039 val loss: 1.559613\n",
      "iteration 100 / 2000: training loss 0.879396 val loss: 1.086415\n",
      "iteration 150 / 2000: training loss 1.542307 val loss: 1.326896\n",
      "iteration 200 / 2000: training loss 1.381272 val loss: 1.282640\n",
      "iteration 250 / 2000: training loss 1.333817 val loss: 1.433889\n",
      "iteration 300 / 2000: training loss 1.048404 val loss: 1.394884\n",
      "iteration 350 / 2000: training loss 2.800064 val loss: 2.103763\n",
      "iteration 400 / 2000: training loss 0.820206 val loss: 0.961660\n",
      "iteration 450 / 2000: training loss 0.988446 val loss: 0.916512\n",
      "iteration 500 / 2000: training loss 1.156326 val loss: 1.263929\n",
      "iteration 550 / 2000: training loss 0.966453 val loss: 0.984483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 600 / 2000: training loss 0.927847 val loss: 0.817636\n",
      "hidden_size: 75 lr: 7.000000e-01 lr_decay: 9.800000e-01 reg: 1.000000e-02 train accuracy: 0.850000 val accuracy: 0.818000\n",
      "iteration 0 / 2000: training loss 2.302586 val loss: 2.302846\n",
      "iteration 50 / 2000: training loss 1.762331 val loss: 1.838466\n",
      "iteration 100 / 2000: training loss 1.065351 val loss: 1.036033\n",
      "iteration 150 / 2000: training loss 1.516210 val loss: 1.290384\n",
      "iteration 200 / 2000: training loss 0.853695 val loss: 1.022144\n",
      "iteration 250 / 2000: training loss 0.792642 val loss: 0.807941\n",
      "iteration 300 / 2000: training loss 0.721161 val loss: 0.839842\n",
      "iteration 350 / 2000: training loss 1.029770 val loss: 0.867707\n",
      "iteration 400 / 2000: training loss 0.652900 val loss: 0.703435\n",
      "iteration 450 / 2000: training loss 0.729004 val loss: 0.701373\n",
      "iteration 500 / 2000: training loss 0.595410 val loss: 0.662699\n",
      "hidden_size: 75 lr: 7.000000e-01 lr_decay: 9.000000e-01 reg: 1.000000e-02 train accuracy: 0.950000 val accuracy: 0.877000\n",
      "iteration 0 / 2000: training loss 2.302586 val loss: 2.302846\n",
      "iteration 50 / 2000: training loss 1.501207 val loss: 1.368868\n",
      "iteration 100 / 2000: training loss 0.961184 val loss: 0.996011\n",
      "iteration 150 / 2000: training loss 0.930656 val loss: 1.021502\n",
      "iteration 200 / 2000: training loss 0.724611 val loss: 0.769076\n",
      "iteration 250 / 2000: training loss 0.814855 val loss: 0.719246\n",
      "iteration 300 / 2000: training loss 0.688620 val loss: 0.814235\n",
      "iteration 350 / 2000: training loss 0.682483 val loss: 0.762023\n",
      "iteration 400 / 2000: training loss 0.589172 val loss: 0.609466\n",
      "iteration 450 / 2000: training loss 0.591965 val loss: 0.647505\n",
      "iteration 500 / 2000: training loss 0.546019 val loss: 0.603899\n",
      "hidden_size: 75 lr: 7.000000e-01 lr_decay: 8.500000e-01 reg: 1.000000e-02 train accuracy: 0.920000 val accuracy: 0.858000\n",
      "iteration 0 / 2000: training loss 2.302586 val loss: 2.302846\n",
      "iteration 50 / 2000: training loss 1.527336 val loss: 1.593714\n",
      "iteration 100 / 2000: training loss 0.894157 val loss: 1.147541\n",
      "iteration 150 / 2000: training loss 0.975054 val loss: 0.888780\n",
      "iteration 200 / 2000: training loss 0.648415 val loss: 0.709387\n",
      "iteration 250 / 2000: training loss 0.774006 val loss: 0.714389\n",
      "iteration 300 / 2000: training loss 0.641251 val loss: 0.692219\n",
      "iteration 350 / 2000: training loss 0.551569 val loss: 0.688926\n",
      "iteration 400 / 2000: training loss 0.570166 val loss: 0.568424\n",
      "iteration 450 / 2000: training loss 0.613074 val loss: 0.591435\n",
      "iteration 500 / 2000: training loss 0.549955 val loss: 0.567748\n",
      "iteration 550 / 2000: training loss 0.526749 val loss: 0.536900\n",
      "iteration 600 / 2000: training loss 0.450348 val loss: 0.539794\n",
      "iteration 650 / 2000: training loss 0.439669 val loss: 0.531109\n",
      "iteration 700 / 2000: training loss 0.542652 val loss: 0.518655\n",
      "iteration 750 / 2000: training loss 0.513410 val loss: 0.527909\n",
      "iteration 800 / 2000: training loss 0.520830 val loss: 0.534108\n",
      "iteration 850 / 2000: training loss 0.503565 val loss: 0.525671\n",
      "iteration 900 / 2000: training loss 0.473381 val loss: 0.518992\n",
      "iteration 950 / 2000: training loss 0.472228 val loss: 0.507915\n",
      "iteration 1000 / 2000: training loss 0.478372 val loss: 0.505323\n",
      "iteration 1050 / 2000: training loss 0.477863 val loss: 0.506044\n",
      "iteration 1100 / 2000: training loss 0.500939 val loss: 0.508890\n",
      "iteration 1150 / 2000: training loss 0.447624 val loss: 0.509688\n",
      "iteration 1200 / 2000: training loss 0.516642 val loss: 0.508484\n",
      "iteration 1250 / 2000: training loss 0.540382 val loss: 0.512165\n",
      "iteration 1300 / 2000: training loss 0.477799 val loss: 0.502967\n",
      "iteration 1350 / 2000: training loss 0.530007 val loss: 0.507583\n",
      "iteration 1400 / 2000: training loss 0.437397 val loss: 0.501890\n",
      "iteration 1450 / 2000: training loss 0.505512 val loss: 0.508337\n",
      "iteration 1500 / 2000: training loss 0.475184 val loss: 0.499030\n",
      "iteration 1550 / 2000: training loss 0.542375 val loss: 0.500772\n",
      "iteration 1600 / 2000: training loss 0.499354 val loss: 0.496701\n",
      "iteration 1650 / 2000: training loss 0.493669 val loss: 0.496430\n",
      "iteration 1700 / 2000: training loss 0.461727 val loss: 0.496868\n",
      "iteration 1750 / 2000: training loss 0.418240 val loss: 0.497076\n",
      "iteration 1800 / 2000: training loss 0.505550 val loss: 0.496142\n",
      "iteration 1850 / 2000: training loss 0.479070 val loss: 0.496451\n",
      "iteration 1900 / 2000: training loss 0.399003 val loss: 0.496264\n",
      "iteration 1950 / 2000: training loss 0.467384 val loss: 0.497200\n",
      "hidden_size: 75 lr: 7.000000e-01 lr_decay: 8.000000e-01 reg: 1.000000e-02 train accuracy: 0.960000 val accuracy: 0.947000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302847\n",
      "iteration 50 / 2000: training loss 2.015768 val loss: 1.784197\n",
      "iteration 100 / 2000: training loss 1.443319 val loss: 1.350550\n",
      "iteration 150 / 2000: training loss 1.398391 val loss: 1.507026\n",
      "iteration 200 / 2000: training loss 1.364494 val loss: 1.319405\n",
      "iteration 250 / 2000: training loss 1.325159 val loss: 1.600311\n",
      "iteration 300 / 2000: training loss 1.536302 val loss: 1.562014\n",
      "iteration 350 / 2000: training loss 1.626882 val loss: 1.812110\n",
      "iteration 400 / 2000: training loss 1.636104 val loss: 1.562332\n",
      "iteration 450 / 2000: training loss 1.789453 val loss: 1.620940\n",
      "hidden_size: 75 lr: 7.000000e-01 lr_decay: 9.800000e-01 reg: 2.000000e-02 train accuracy: 0.790000 val accuracy: 0.757000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302847\n",
      "iteration 50 / 2000: training loss 1.801113 val loss: 1.827094\n",
      "iteration 100 / 2000: training loss 1.430577 val loss: 1.347439\n",
      "iteration 150 / 2000: training loss 1.238435 val loss: 1.321045\n",
      "iteration 200 / 2000: training loss 0.939568 val loss: 1.090295\n",
      "iteration 250 / 2000: training loss 1.129665 val loss: 1.168592\n",
      "iteration 300 / 2000: training loss 1.269638 val loss: 1.591324\n",
      "iteration 350 / 2000: training loss 1.109768 val loss: 0.983764\n",
      "hidden_size: 75 lr: 7.000000e-01 lr_decay: 9.000000e-01 reg: 2.000000e-02 train accuracy: 0.890000 val accuracy: 0.799000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302847\n",
      "iteration 50 / 2000: training loss 1.621598 val loss: 1.620279\n",
      "iteration 100 / 2000: training loss 1.286125 val loss: 1.353229\n",
      "iteration 150 / 2000: training loss 1.330611 val loss: 1.117838\n",
      "iteration 200 / 2000: training loss 0.941985 val loss: 1.002172\n",
      "iteration 250 / 2000: training loss 0.959144 val loss: 1.142067\n",
      "iteration 300 / 2000: training loss 0.985219 val loss: 1.103295\n",
      "iteration 350 / 2000: training loss 0.911419 val loss: 0.906567\n",
      "iteration 400 / 2000: training loss 0.849229 val loss: 0.823278\n",
      "iteration 450 / 2000: training loss 0.902704 val loss: 0.947419\n",
      "iteration 500 / 2000: training loss 0.782660 val loss: 0.849234\n",
      "iteration 550 / 2000: training loss 0.789083 val loss: 0.838925\n",
      "iteration 600 / 2000: training loss 0.764263 val loss: 0.865319\n",
      "iteration 650 / 2000: training loss 0.686208 val loss: 0.764077\n",
      "iteration 700 / 2000: training loss 0.824570 val loss: 0.831432\n",
      "iteration 750 / 2000: training loss 0.777141 val loss: 0.744714\n",
      "iteration 800 / 2000: training loss 0.784602 val loss: 0.777387\n",
      "iteration 850 / 2000: training loss 0.795248 val loss: 0.774701\n",
      "iteration 900 / 2000: training loss 0.758234 val loss: 0.754410\n",
      "iteration 950 / 2000: training loss 0.700790 val loss: 0.754069\n",
      "iteration 1000 / 2000: training loss 0.710227 val loss: 0.754830\n",
      "iteration 1050 / 2000: training loss 0.719939 val loss: 0.762283\n",
      "iteration 1100 / 2000: training loss 0.756088 val loss: 0.756028\n",
      "iteration 1150 / 2000: training loss 0.678704 val loss: 0.752854\n",
      "iteration 1200 / 2000: training loss 0.749901 val loss: 0.729393\n",
      "iteration 1250 / 2000: training loss 0.776853 val loss: 0.740659\n",
      "iteration 1300 / 2000: training loss 0.705226 val loss: 0.731641\n",
      "iteration 1350 / 2000: training loss 0.788566 val loss: 0.736045\n",
      "iteration 1400 / 2000: training loss 0.682334 val loss: 0.725313\n",
      "iteration 1450 / 2000: training loss 0.774593 val loss: 0.725438\n",
      "iteration 1500 / 2000: training loss 0.711911 val loss: 0.724244\n",
      "iteration 1550 / 2000: training loss 0.747628 val loss: 0.720409\n",
      "iteration 1600 / 2000: training loss 0.745506 val loss: 0.715889\n",
      "iteration 1650 / 2000: training loss 0.715109 val loss: 0.715365\n",
      "iteration 1700 / 2000: training loss 0.668036 val loss: 0.715431\n",
      "iteration 1750 / 2000: training loss 0.633613 val loss: 0.710275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1800 / 2000: training loss 0.732916 val loss: 0.714461\n",
      "iteration 1850 / 2000: training loss 0.710540 val loss: 0.710979\n",
      "iteration 1900 / 2000: training loss 0.618932 val loss: 0.712928\n",
      "iteration 1950 / 2000: training loss 0.704795 val loss: 0.713657\n",
      "hidden_size: 75 lr: 7.000000e-01 lr_decay: 8.500000e-01 reg: 2.000000e-02 train accuracy: 0.930000 val accuracy: 0.936000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302847\n",
      "iteration 50 / 2000: training loss 1.324846 val loss: 1.367498\n",
      "iteration 100 / 2000: training loss 1.129076 val loss: 1.186701\n",
      "iteration 150 / 2000: training loss 1.164830 val loss: 1.121457\n",
      "iteration 200 / 2000: training loss 0.913821 val loss: 0.913327\n",
      "iteration 250 / 2000: training loss 0.919939 val loss: 1.012608\n",
      "iteration 300 / 2000: training loss 0.789429 val loss: 0.931063\n",
      "iteration 350 / 2000: training loss 0.920509 val loss: 0.882487\n",
      "iteration 400 / 2000: training loss 0.804201 val loss: 0.788192\n",
      "iteration 450 / 2000: training loss 0.822685 val loss: 0.808909\n",
      "iteration 500 / 2000: training loss 0.750611 val loss: 0.782034\n",
      "iteration 550 / 2000: training loss 0.727122 val loss: 0.771299\n",
      "iteration 600 / 2000: training loss 0.669828 val loss: 0.787661\n",
      "iteration 650 / 2000: training loss 0.666153 val loss: 0.754653\n",
      "iteration 700 / 2000: training loss 0.744093 val loss: 0.735640\n",
      "iteration 750 / 2000: training loss 0.732587 val loss: 0.733429\n",
      "iteration 800 / 2000: training loss 0.748647 val loss: 0.750556\n",
      "iteration 850 / 2000: training loss 0.727974 val loss: 0.742589\n",
      "iteration 900 / 2000: training loss 0.700265 val loss: 0.740889\n",
      "iteration 950 / 2000: training loss 0.695737 val loss: 0.730705\n",
      "iteration 1000 / 2000: training loss 0.701168 val loss: 0.722736\n",
      "iteration 1050 / 2000: training loss 0.697615 val loss: 0.722887\n",
      "iteration 1100 / 2000: training loss 0.721266 val loss: 0.723502\n",
      "iteration 1150 / 2000: training loss 0.667579 val loss: 0.722706\n",
      "iteration 1200 / 2000: training loss 0.752169 val loss: 0.725912\n",
      "iteration 1250 / 2000: training loss 0.764668 val loss: 0.723097\n",
      "iteration 1300 / 2000: training loss 0.706325 val loss: 0.715587\n",
      "iteration 1350 / 2000: training loss 0.765470 val loss: 0.723360\n",
      "iteration 1400 / 2000: training loss 0.674523 val loss: 0.713525\n",
      "iteration 1450 / 2000: training loss 0.745895 val loss: 0.718738\n",
      "iteration 1500 / 2000: training loss 0.691827 val loss: 0.712446\n",
      "iteration 1550 / 2000: training loss 0.739825 val loss: 0.712982\n",
      "iteration 1600 / 2000: training loss 0.724324 val loss: 0.708198\n",
      "iteration 1650 / 2000: training loss 0.711448 val loss: 0.707381\n",
      "iteration 1700 / 2000: training loss 0.667517 val loss: 0.708988\n",
      "iteration 1750 / 2000: training loss 0.634585 val loss: 0.709236\n",
      "iteration 1800 / 2000: training loss 0.735286 val loss: 0.707395\n",
      "iteration 1850 / 2000: training loss 0.706251 val loss: 0.707569\n",
      "iteration 1900 / 2000: training loss 0.619839 val loss: 0.707503\n",
      "iteration 1950 / 2000: training loss 0.702795 val loss: 0.707741\n",
      "hidden_size: 75 lr: 7.000000e-01 lr_decay: 8.000000e-01 reg: 2.000000e-02 train accuracy: 0.940000 val accuracy: 0.936000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302849\n",
      "iteration 50 / 2000: training loss 2.720078 val loss: 2.318992\n",
      "iteration 100 / 2000: training loss 3.038228 val loss: 2.997554\n",
      "iteration 150 / 2000: training loss 3.110090 val loss: 2.922893\n",
      "iteration 200 / 2000: training loss 5.413395 val loss: 5.966464\n",
      "iteration 250 / 2000: training loss 4.442565 val loss: 4.530926\n",
      "hidden_size: 75 lr: 7.000000e-01 lr_decay: 9.800000e-01 reg: 4.000000e-02 train accuracy: 0.550000 val accuracy: 0.535000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302849\n",
      "iteration 50 / 2000: training loss 3.043937 val loss: 2.884865\n",
      "iteration 100 / 2000: training loss 1.648614 val loss: 1.703649\n",
      "iteration 150 / 2000: training loss 1.574371 val loss: 1.690953\n",
      "iteration 200 / 2000: training loss 1.339256 val loss: 1.504210\n",
      "iteration 250 / 2000: training loss 1.338894 val loss: 1.566122\n",
      "iteration 300 / 2000: training loss 1.538660 val loss: 1.571426\n",
      "iteration 350 / 2000: training loss 1.352126 val loss: 1.306449\n",
      "iteration 400 / 2000: training loss 1.394466 val loss: 1.368809\n",
      "iteration 450 / 2000: training loss 1.256256 val loss: 1.260133\n",
      "iteration 500 / 2000: training loss 1.241866 val loss: 1.398466\n",
      "iteration 550 / 2000: training loss 1.217277 val loss: 1.222176\n",
      "iteration 600 / 2000: training loss 1.222214 val loss: 1.342428\n",
      "iteration 650 / 2000: training loss 1.180865 val loss: 1.214704\n",
      "iteration 700 / 2000: training loss 1.296652 val loss: 1.293115\n",
      "iteration 750 / 2000: training loss 1.149394 val loss: 1.164988\n",
      "iteration 800 / 2000: training loss 1.122764 val loss: 1.169849\n",
      "iteration 850 / 2000: training loss 1.195769 val loss: 1.161839\n",
      "iteration 900 / 2000: training loss 1.134257 val loss: 1.134917\n",
      "iteration 950 / 2000: training loss 1.061228 val loss: 1.161913\n",
      "iteration 1000 / 2000: training loss 1.116666 val loss: 1.243958\n",
      "iteration 1050 / 2000: training loss 1.139224 val loss: 1.118889\n",
      "iteration 1100 / 2000: training loss 1.053757 val loss: 1.105218\n",
      "iteration 1150 / 2000: training loss 1.036003 val loss: 1.134904\n",
      "iteration 1200 / 2000: training loss 1.119085 val loss: 1.078866\n",
      "iteration 1250 / 2000: training loss 1.104759 val loss: 1.082423\n",
      "iteration 1300 / 2000: training loss 1.162544 val loss: 1.112964\n",
      "iteration 1350 / 2000: training loss 1.112597 val loss: 1.078844\n",
      "iteration 1400 / 2000: training loss 1.106298 val loss: 1.079341\n",
      "iteration 1450 / 2000: training loss 1.133834 val loss: 1.106421\n",
      "iteration 1500 / 2000: training loss 1.071142 val loss: 1.048474\n",
      "iteration 1550 / 2000: training loss 1.064233 val loss: 1.045775\n",
      "iteration 1600 / 2000: training loss 1.152405 val loss: 1.102008\n",
      "iteration 1650 / 2000: training loss 1.034658 val loss: 1.060093\n",
      "iteration 1700 / 2000: training loss 1.008843 val loss: 1.064416\n",
      "iteration 1750 / 2000: training loss 0.958004 val loss: 1.070270\n",
      "iteration 1800 / 2000: training loss 1.091901 val loss: 1.073584\n",
      "iteration 1850 / 2000: training loss 1.034535 val loss: 1.050690\n",
      "iteration 1900 / 2000: training loss 0.984478 val loss: 1.043923\n",
      "iteration 1950 / 2000: training loss 1.023101 val loss: 1.054986\n",
      "hidden_size: 75 lr: 7.000000e-01 lr_decay: 9.000000e-01 reg: 4.000000e-02 train accuracy: 0.910000 val accuracy: 0.909000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302849\n",
      "iteration 50 / 2000: training loss 2.265712 val loss: 1.794613\n",
      "iteration 100 / 2000: training loss 1.490044 val loss: 1.509585\n",
      "iteration 150 / 2000: training loss 1.590923 val loss: 1.428664\n",
      "iteration 200 / 2000: training loss 1.484460 val loss: 1.603511\n",
      "iteration 250 / 2000: training loss 1.306396 val loss: 1.374329\n",
      "iteration 300 / 2000: training loss 1.251091 val loss: 1.342651\n",
      "iteration 350 / 2000: training loss 1.267209 val loss: 1.195382\n",
      "iteration 400 / 2000: training loss 1.212418 val loss: 1.158733\n",
      "iteration 450 / 2000: training loss 1.214744 val loss: 1.200235\n",
      "iteration 500 / 2000: training loss 1.095024 val loss: 1.121087\n",
      "iteration 550 / 2000: training loss 1.111147 val loss: 1.117516\n",
      "iteration 600 / 2000: training loss 1.089937 val loss: 1.131661\n",
      "iteration 650 / 2000: training loss 1.017094 val loss: 1.099869\n",
      "iteration 700 / 2000: training loss 1.159335 val loss: 1.132286\n",
      "iteration 750 / 2000: training loss 1.109664 val loss: 1.065903\n",
      "iteration 800 / 2000: training loss 1.067766 val loss: 1.084952\n",
      "hidden_size: 75 lr: 7.000000e-01 lr_decay: 8.500000e-01 reg: 4.000000e-02 train accuracy: 0.910000 val accuracy: 0.873000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302849\n",
      "iteration 50 / 2000: training loss 2.164883 val loss: 1.844580\n",
      "iteration 100 / 2000: training loss 1.369014 val loss: 1.328310\n",
      "iteration 150 / 2000: training loss 1.372223 val loss: 1.361442\n",
      "iteration 200 / 2000: training loss 1.198629 val loss: 1.213321\n",
      "iteration 250 / 2000: training loss 1.240469 val loss: 1.359719\n",
      "iteration 300 / 2000: training loss 1.104895 val loss: 1.298116\n",
      "iteration 350 / 2000: training loss 1.329263 val loss: 1.228342\n",
      "iteration 400 / 2000: training loss 1.172219 val loss: 1.130661\n",
      "iteration 450 / 2000: training loss 1.175113 val loss: 1.166988\n",
      "iteration 500 / 2000: training loss 1.093839 val loss: 1.127452\n",
      "iteration 550 / 2000: training loss 1.036359 val loss: 1.102640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 600 / 2000: training loss 1.037880 val loss: 1.122311\n",
      "iteration 650 / 2000: training loss 0.981843 val loss: 1.061095\n",
      "iteration 700 / 2000: training loss 1.060347 val loss: 1.083293\n",
      "iteration 750 / 2000: training loss 1.062509 val loss: 1.045871\n",
      "iteration 800 / 2000: training loss 1.083793 val loss: 1.058734\n",
      "iteration 850 / 2000: training loss 1.059356 val loss: 1.067598\n",
      "iteration 900 / 2000: training loss 1.036553 val loss: 1.053556\n",
      "iteration 950 / 2000: training loss 1.015842 val loss: 1.047957\n",
      "iteration 1000 / 2000: training loss 1.021240 val loss: 1.041172\n",
      "iteration 1050 / 2000: training loss 1.020322 val loss: 1.042822\n",
      "iteration 1100 / 2000: training loss 1.024373 val loss: 1.032049\n",
      "iteration 1150 / 2000: training loss 0.973427 val loss: 1.036712\n",
      "iteration 1200 / 2000: training loss 1.072854 val loss: 1.039224\n",
      "iteration 1250 / 2000: training loss 1.074859 val loss: 1.030365\n",
      "iteration 1300 / 2000: training loss 1.028676 val loss: 1.029156\n",
      "iteration 1350 / 2000: training loss 1.113584 val loss: 1.045503\n",
      "hidden_size: 75 lr: 7.000000e-01 lr_decay: 8.000000e-01 reg: 4.000000e-02 train accuracy: 0.880000 val accuracy: 0.910000\n",
      "iteration 0 / 2000: training loss 2.302591 val loss: 2.302851\n",
      "iteration 50 / 2000: training loss 3.013200 val loss: 2.923106\n",
      "iteration 100 / 2000: training loss 2.319405 val loss: 3.205312\n",
      "iteration 150 / 2000: training loss 3.117125 val loss: 3.417527\n",
      "iteration 200 / 2000: training loss 3.032914 val loss: 2.820523\n",
      "iteration 250 / 2000: training loss 2.572536 val loss: 2.586160\n",
      "iteration 300 / 2000: training loss 2.536284 val loss: 2.514381\n",
      "iteration 350 / 2000: training loss 2.590076 val loss: 2.467114\n",
      "iteration 400 / 2000: training loss 2.305524 val loss: 2.343149\n",
      "iteration 450 / 2000: training loss 2.245921 val loss: 2.182967\n",
      "iteration 500 / 2000: training loss 2.263075 val loss: 1.882766\n",
      "iteration 550 / 2000: training loss 2.257924 val loss: 2.222976\n",
      "iteration 600 / 2000: training loss 2.928696 val loss: 2.598284\n",
      "hidden_size: 75 lr: 7.000000e-01 lr_decay: 9.800000e-01 reg: 6.000000e-02 train accuracy: 0.570000 val accuracy: 0.418000\n",
      "iteration 0 / 2000: training loss 2.302591 val loss: 2.302851\n",
      "iteration 50 / 2000: training loss 2.200269 val loss: 2.313038\n",
      "iteration 100 / 2000: training loss 1.982417 val loss: 2.253896\n",
      "iteration 150 / 2000: training loss 2.036171 val loss: 2.106964\n",
      "iteration 200 / 2000: training loss 1.569339 val loss: 1.668259\n",
      "iteration 250 / 2000: training loss 1.672562 val loss: 1.694183\n",
      "iteration 300 / 2000: training loss 1.880984 val loss: 1.764201\n",
      "iteration 350 / 2000: training loss 1.680433 val loss: 1.641382\n",
      "iteration 400 / 2000: training loss 1.608682 val loss: 1.454435\n",
      "iteration 450 / 2000: training loss 1.500749 val loss: 1.515779\n",
      "iteration 500 / 2000: training loss 1.453661 val loss: 1.557179\n",
      "iteration 550 / 2000: training loss 1.560292 val loss: 1.620193\n",
      "iteration 600 / 2000: training loss 1.564748 val loss: 1.639140\n",
      "iteration 650 / 2000: training loss 1.426634 val loss: 1.481835\n",
      "iteration 700 / 2000: training loss 1.525776 val loss: 1.501429\n",
      "iteration 750 / 2000: training loss 1.452876 val loss: 1.449324\n",
      "iteration 800 / 2000: training loss 1.387707 val loss: 1.428067\n",
      "iteration 850 / 2000: training loss 1.456471 val loss: 1.393628\n",
      "iteration 900 / 2000: training loss 1.405252 val loss: 1.411819\n",
      "iteration 950 / 2000: training loss 1.297514 val loss: 1.453349\n",
      "iteration 1000 / 2000: training loss 1.350283 val loss: 1.462786\n",
      "iteration 1050 / 2000: training loss 1.350177 val loss: 1.364883\n",
      "iteration 1100 / 2000: training loss 1.294318 val loss: 1.352461\n",
      "iteration 1150 / 2000: training loss 1.299484 val loss: 1.404213\n",
      "iteration 1200 / 2000: training loss 1.371835 val loss: 1.339606\n",
      "iteration 1250 / 2000: training loss 1.347101 val loss: 1.343023\n",
      "iteration 1300 / 2000: training loss 1.414305 val loss: 1.373488\n",
      "iteration 1350 / 2000: training loss 1.339247 val loss: 1.328575\n",
      "iteration 1400 / 2000: training loss 1.339603 val loss: 1.321776\n",
      "iteration 1450 / 2000: training loss 1.418896 val loss: 1.360240\n",
      "iteration 1500 / 2000: training loss 1.295032 val loss: 1.289149\n",
      "iteration 1550 / 2000: training loss 1.296614 val loss: 1.286260\n",
      "iteration 1600 / 2000: training loss 1.386037 val loss: 1.326688\n",
      "iteration 1650 / 2000: training loss 1.266876 val loss: 1.306019\n",
      "iteration 1700 / 2000: training loss 1.243785 val loss: 1.298584\n",
      "hidden_size: 75 lr: 7.000000e-01 lr_decay: 9.000000e-01 reg: 6.000000e-02 train accuracy: 0.850000 val accuracy: 0.856000\n",
      "iteration 0 / 2000: training loss 2.302591 val loss: 2.302851\n",
      "iteration 50 / 2000: training loss 2.160707 val loss: 1.939595\n",
      "iteration 100 / 2000: training loss 2.014689 val loss: 1.926175\n",
      "iteration 150 / 2000: training loss 1.722480 val loss: 1.706290\n",
      "iteration 200 / 2000: training loss 1.703716 val loss: 1.560669\n",
      "iteration 250 / 2000: training loss 1.560334 val loss: 1.813324\n",
      "iteration 300 / 2000: training loss 1.524573 val loss: 1.679250\n",
      "iteration 350 / 2000: training loss 1.551281 val loss: 1.453002\n",
      "iteration 400 / 2000: training loss 1.499547 val loss: 1.460505\n",
      "iteration 450 / 2000: training loss 1.519632 val loss: 1.389480\n",
      "iteration 500 / 2000: training loss 1.321043 val loss: 1.371866\n",
      "iteration 550 / 2000: training loss 1.338316 val loss: 1.397286\n",
      "iteration 600 / 2000: training loss 1.345643 val loss: 1.388840\n",
      "iteration 650 / 2000: training loss 1.279416 val loss: 1.348658\n",
      "iteration 700 / 2000: training loss 1.432282 val loss: 1.439699\n",
      "iteration 750 / 2000: training loss 1.341876 val loss: 1.313660\n",
      "iteration 800 / 2000: training loss 1.310637 val loss: 1.319975\n",
      "hidden_size: 75 lr: 7.000000e-01 lr_decay: 8.500000e-01 reg: 6.000000e-02 train accuracy: 0.870000 val accuracy: 0.822000\n",
      "iteration 0 / 2000: training loss 2.302591 val loss: 2.302851\n",
      "iteration 50 / 2000: training loss 2.009911 val loss: 2.063689\n",
      "iteration 100 / 2000: training loss 1.713882 val loss: 1.691358\n",
      "iteration 150 / 2000: training loss 1.644623 val loss: 1.594298\n",
      "iteration 200 / 2000: training loss 1.440176 val loss: 1.552183\n",
      "iteration 250 / 2000: training loss 1.465308 val loss: 1.559934\n",
      "iteration 300 / 2000: training loss 1.383211 val loss: 1.643476\n",
      "iteration 350 / 2000: training loss 1.655080 val loss: 1.456671\n",
      "iteration 400 / 2000: training loss 1.468422 val loss: 1.390319\n",
      "iteration 450 / 2000: training loss 1.396904 val loss: 1.404696\n",
      "iteration 500 / 2000: training loss 1.289640 val loss: 1.341812\n",
      "iteration 550 / 2000: training loss 1.269904 val loss: 1.362041\n",
      "iteration 600 / 2000: training loss 1.323485 val loss: 1.369651\n",
      "iteration 650 / 2000: training loss 1.213147 val loss: 1.294278\n",
      "iteration 700 / 2000: training loss 1.296271 val loss: 1.327777\n",
      "iteration 750 / 2000: training loss 1.311662 val loss: 1.297621\n",
      "iteration 800 / 2000: training loss 1.317995 val loss: 1.296670\n",
      "iteration 850 / 2000: training loss 1.300246 val loss: 1.300913\n",
      "iteration 900 / 2000: training loss 1.283866 val loss: 1.289701\n",
      "iteration 950 / 2000: training loss 1.257996 val loss: 1.287292\n",
      "iteration 1000 / 2000: training loss 1.261394 val loss: 1.285158\n",
      "iteration 1050 / 2000: training loss 1.273852 val loss: 1.285320\n",
      "iteration 1100 / 2000: training loss 1.257518 val loss: 1.265763\n",
      "iteration 1150 / 2000: training loss 1.209092 val loss: 1.280503\n",
      "iteration 1200 / 2000: training loss 1.315073 val loss: 1.279282\n",
      "iteration 1250 / 2000: training loss 1.310314 val loss: 1.267317\n",
      "iteration 1300 / 2000: training loss 1.260477 val loss: 1.267831\n",
      "iteration 1350 / 2000: training loss 1.367115 val loss: 1.285740\n",
      "hidden_size: 75 lr: 7.000000e-01 lr_decay: 8.000000e-01 reg: 6.000000e-02 train accuracy: 0.810000 val accuracy: 0.892000\n",
      "iteration 0 / 2000: training loss 2.302586 val loss: 2.303017\n",
      "iteration 50 / 2000: training loss 2.856244 val loss: 2.804292\n",
      "iteration 100 / 2000: training loss 1.917207 val loss: 1.892439\n",
      "iteration 150 / 2000: training loss 3.055062 val loss: 2.883233\n",
      "iteration 200 / 2000: training loss 1.629642 val loss: 1.602203\n",
      "iteration 250 / 2000: training loss 1.664150 val loss: 1.906496\n",
      "iteration 300 / 2000: training loss 1.705854 val loss: 1.938642\n",
      "iteration 350 / 2000: training loss 1.494212 val loss: 2.046736\n",
      "iteration 400 / 2000: training loss 1.384490 val loss: 1.928923\n",
      "iteration 450 / 2000: training loss 2.721957 val loss: 2.542288\n",
      "hidden_size: 75 lr: 9.000000e-01 lr_decay: 9.800000e-01 reg: 1.000000e-02 train accuracy: 0.420000 val accuracy: 0.493000\n",
      "iteration 0 / 2000: training loss 2.302586 val loss: 2.303017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 50 / 2000: training loss 2.247919 val loss: 2.390066\n",
      "iteration 100 / 2000: training loss 1.349198 val loss: 1.384768\n",
      "iteration 150 / 2000: training loss 2.119108 val loss: 1.613720\n",
      "iteration 200 / 2000: training loss 1.091852 val loss: 0.981271\n",
      "iteration 250 / 2000: training loss 1.255567 val loss: 1.468024\n",
      "iteration 300 / 2000: training loss 0.849132 val loss: 0.999267\n",
      "iteration 350 / 2000: training loss 0.801587 val loss: 1.164834\n",
      "iteration 400 / 2000: training loss 0.792065 val loss: 0.847763\n",
      "iteration 450 / 2000: training loss 1.062081 val loss: 0.846043\n",
      "iteration 500 / 2000: training loss 0.903713 val loss: 0.739883\n",
      "iteration 550 / 2000: training loss 0.731915 val loss: 0.720373\n",
      "iteration 600 / 2000: training loss 0.600800 val loss: 0.666387\n",
      "iteration 650 / 2000: training loss 0.596694 val loss: 0.703306\n",
      "iteration 700 / 2000: training loss 0.810668 val loss: 0.664061\n",
      "hidden_size: 75 lr: 9.000000e-01 lr_decay: 9.000000e-01 reg: 1.000000e-02 train accuracy: 0.910000 val accuracy: 0.883000\n",
      "iteration 0 / 2000: training loss 2.302586 val loss: 2.303017\n",
      "iteration 50 / 2000: training loss 2.141908 val loss: 2.124755\n",
      "iteration 100 / 2000: training loss 1.496190 val loss: 1.505024\n",
      "iteration 150 / 2000: training loss 1.431137 val loss: 1.269106\n",
      "iteration 200 / 2000: training loss 0.916803 val loss: 0.974056\n",
      "iteration 250 / 2000: training loss 1.091613 val loss: 1.082392\n",
      "iteration 300 / 2000: training loss 0.763509 val loss: 1.013844\n",
      "iteration 350 / 2000: training loss 0.774202 val loss: 0.776203\n",
      "iteration 400 / 2000: training loss 0.739217 val loss: 0.744160\n",
      "iteration 450 / 2000: training loss 0.822569 val loss: 0.831558\n",
      "iteration 500 / 2000: training loss 0.590949 val loss: 0.721996\n",
      "iteration 550 / 2000: training loss 0.592369 val loss: 0.709760\n",
      "iteration 600 / 2000: training loss 0.554045 val loss: 0.672512\n",
      "iteration 650 / 2000: training loss 0.468404 val loss: 0.574596\n",
      "iteration 700 / 2000: training loss 0.658128 val loss: 0.627234\n",
      "iteration 750 / 2000: training loss 0.576850 val loss: 0.552116\n",
      "iteration 800 / 2000: training loss 0.583479 val loss: 0.624255\n",
      "iteration 850 / 2000: training loss 0.603223 val loss: 0.592135\n",
      "iteration 900 / 2000: training loss 0.536027 val loss: 0.593558\n",
      "iteration 950 / 2000: training loss 0.498343 val loss: 0.547897\n",
      "iteration 1000 / 2000: training loss 0.499142 val loss: 0.544253\n",
      "iteration 1050 / 2000: training loss 0.526107 val loss: 0.575809\n",
      "iteration 1100 / 2000: training loss 0.565675 val loss: 0.564706\n",
      "iteration 1150 / 2000: training loss 0.461213 val loss: 0.546236\n",
      "iteration 1200 / 2000: training loss 0.536330 val loss: 0.549143\n",
      "iteration 1250 / 2000: training loss 0.592235 val loss: 0.556547\n",
      "iteration 1300 / 2000: training loss 0.491286 val loss: 0.529436\n",
      "iteration 1350 / 2000: training loss 0.565275 val loss: 0.533787\n",
      "iteration 1400 / 2000: training loss 0.449672 val loss: 0.522827\n",
      "iteration 1450 / 2000: training loss 0.540163 val loss: 0.520464\n",
      "iteration 1500 / 2000: training loss 0.512364 val loss: 0.528390\n",
      "iteration 1550 / 2000: training loss 0.558311 val loss: 0.520380\n",
      "iteration 1600 / 2000: training loss 0.534744 val loss: 0.514749\n",
      "hidden_size: 75 lr: 9.000000e-01 lr_decay: 8.500000e-01 reg: 1.000000e-02 train accuracy: 0.960000 val accuracy: 0.935000\n",
      "iteration 0 / 2000: training loss 2.302586 val loss: 2.303017\n",
      "iteration 50 / 2000: training loss 1.889145 val loss: 1.618347\n",
      "iteration 100 / 2000: training loss 1.005452 val loss: 0.967934\n",
      "iteration 150 / 2000: training loss 1.400970 val loss: 0.998643\n",
      "iteration 200 / 2000: training loss 0.773667 val loss: 0.760120\n",
      "iteration 250 / 2000: training loss 0.828439 val loss: 0.743365\n",
      "iteration 300 / 2000: training loss 0.636147 val loss: 0.774229\n",
      "iteration 350 / 2000: training loss 0.755924 val loss: 0.692393\n",
      "iteration 400 / 2000: training loss 0.572705 val loss: 0.581479\n",
      "iteration 450 / 2000: training loss 0.603639 val loss: 0.601727\n",
      "iteration 500 / 2000: training loss 0.527877 val loss: 0.590499\n",
      "iteration 550 / 2000: training loss 0.545545 val loss: 0.579937\n",
      "iteration 600 / 2000: training loss 0.450855 val loss: 0.584098\n",
      "iteration 650 / 2000: training loss 0.462266 val loss: 0.553827\n",
      "iteration 700 / 2000: training loss 0.532387 val loss: 0.524008\n",
      "iteration 750 / 2000: training loss 0.506815 val loss: 0.539998\n",
      "iteration 800 / 2000: training loss 0.535487 val loss: 0.549324\n",
      "iteration 850 / 2000: training loss 0.498761 val loss: 0.540553\n",
      "iteration 900 / 2000: training loss 0.474145 val loss: 0.531106\n",
      "iteration 950 / 2000: training loss 0.486924 val loss: 0.514506\n",
      "iteration 1000 / 2000: training loss 0.483507 val loss: 0.511501\n",
      "iteration 1050 / 2000: training loss 0.477551 val loss: 0.514569\n",
      "iteration 1100 / 2000: training loss 0.512698 val loss: 0.515336\n",
      "iteration 1150 / 2000: training loss 0.449810 val loss: 0.515235\n",
      "iteration 1200 / 2000: training loss 0.518355 val loss: 0.515867\n",
      "iteration 1250 / 2000: training loss 0.546627 val loss: 0.515564\n",
      "iteration 1300 / 2000: training loss 0.479467 val loss: 0.506702\n",
      "iteration 1350 / 2000: training loss 0.535192 val loss: 0.510993\n",
      "iteration 1400 / 2000: training loss 0.439758 val loss: 0.503656\n",
      "iteration 1450 / 2000: training loss 0.514199 val loss: 0.507902\n",
      "iteration 1500 / 2000: training loss 0.479224 val loss: 0.501261\n",
      "iteration 1550 / 2000: training loss 0.546041 val loss: 0.501803\n",
      "iteration 1600 / 2000: training loss 0.499995 val loss: 0.497228\n",
      "iteration 1650 / 2000: training loss 0.497380 val loss: 0.496632\n",
      "iteration 1700 / 2000: training loss 0.459856 val loss: 0.496752\n",
      "iteration 1750 / 2000: training loss 0.424973 val loss: 0.498500\n",
      "iteration 1800 / 2000: training loss 0.515710 val loss: 0.496950\n",
      "iteration 1850 / 2000: training loss 0.475135 val loss: 0.497573\n",
      "iteration 1900 / 2000: training loss 0.400506 val loss: 0.497223\n",
      "iteration 1950 / 2000: training loss 0.473004 val loss: 0.498335\n",
      "hidden_size: 75 lr: 9.000000e-01 lr_decay: 8.000000e-01 reg: 1.000000e-02 train accuracy: 0.950000 val accuracy: 0.944000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.303018\n",
      "iteration 50 / 2000: training loss 2.905029 val loss: 2.866047\n",
      "iteration 100 / 2000: training loss 5.159590 val loss: 5.357492\n",
      "iteration 150 / 2000: training loss 2.271841 val loss: 2.634498\n",
      "iteration 200 / 2000: training loss 1.727545 val loss: 1.903569\n",
      "iteration 250 / 2000: training loss 2.486154 val loss: 2.379945\n",
      "iteration 300 / 2000: training loss 3.473628 val loss: 3.337030\n",
      "iteration 350 / 2000: training loss 2.315175 val loss: 3.108314\n",
      "hidden_size: 75 lr: 9.000000e-01 lr_decay: 9.800000e-01 reg: 2.000000e-02 train accuracy: 0.420000 val accuracy: 0.390000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.303018\n",
      "iteration 50 / 2000: training loss 2.608217 val loss: 2.173032\n",
      "iteration 100 / 2000: training loss 2.080427 val loss: 2.005532\n",
      "iteration 150 / 2000: training loss 1.470849 val loss: 1.626280\n",
      "iteration 200 / 2000: training loss 1.689302 val loss: 1.360176\n",
      "iteration 250 / 2000: training loss 1.424484 val loss: 1.509246\n",
      "iteration 300 / 2000: training loss 1.091277 val loss: 1.208776\n",
      "iteration 350 / 2000: training loss 1.071560 val loss: 1.013921\n",
      "iteration 400 / 2000: training loss 1.139053 val loss: 1.165788\n",
      "iteration 450 / 2000: training loss 1.106892 val loss: 0.968990\n",
      "iteration 500 / 2000: training loss 1.118158 val loss: 1.305796\n",
      "iteration 550 / 2000: training loss 1.061642 val loss: 1.197210\n",
      "iteration 600 / 2000: training loss 0.918686 val loss: 0.990711\n",
      "iteration 650 / 2000: training loss 0.806165 val loss: 0.931000\n",
      "iteration 700 / 2000: training loss 1.252398 val loss: 1.009077\n",
      "iteration 750 / 2000: training loss 0.900213 val loss: 1.044250\n",
      "iteration 800 / 2000: training loss 0.857260 val loss: 0.976803\n",
      "iteration 850 / 2000: training loss 0.945037 val loss: 0.903265\n",
      "iteration 900 / 2000: training loss 0.827548 val loss: 0.880488\n",
      "iteration 950 / 2000: training loss 0.730557 val loss: 0.855805\n",
      "iteration 1000 / 2000: training loss 0.802139 val loss: 0.963965\n",
      "iteration 1050 / 2000: training loss 0.838457 val loss: 0.838012\n",
      "iteration 1100 / 2000: training loss 0.780894 val loss: 0.808304\n",
      "iteration 1150 / 2000: training loss 0.757638 val loss: 0.816695\n",
      "iteration 1200 / 2000: training loss 0.821266 val loss: 0.785436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1250 / 2000: training loss 0.826765 val loss: 0.789922\n",
      "iteration 1300 / 2000: training loss 0.835824 val loss: 0.802925\n",
      "iteration 1350 / 2000: training loss 0.794537 val loss: 0.800798\n",
      "iteration 1400 / 2000: training loss 0.774003 val loss: 0.771148\n",
      "iteration 1450 / 2000: training loss 0.824562 val loss: 0.806736\n",
      "iteration 1500 / 2000: training loss 0.761983 val loss: 0.741825\n",
      "iteration 1550 / 2000: training loss 0.785768 val loss: 0.743503\n",
      "iteration 1600 / 2000: training loss 0.834169 val loss: 0.821755\n",
      "iteration 1650 / 2000: training loss 0.740720 val loss: 0.747721\n",
      "iteration 1700 / 2000: training loss 0.693170 val loss: 0.768003\n",
      "iteration 1750 / 2000: training loss 0.665816 val loss: 0.786459\n",
      "iteration 1800 / 2000: training loss 0.764631 val loss: 0.763022\n",
      "iteration 1850 / 2000: training loss 0.728059 val loss: 0.746883\n",
      "iteration 1900 / 2000: training loss 0.647628 val loss: 0.732200\n",
      "iteration 1950 / 2000: training loss 0.720011 val loss: 0.740877\n",
      "hidden_size: 75 lr: 9.000000e-01 lr_decay: 9.000000e-01 reg: 2.000000e-02 train accuracy: 0.930000 val accuracy: 0.922000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.303018\n",
      "iteration 50 / 2000: training loss 5.265303 val loss: 5.435252\n",
      "iteration 100 / 2000: training loss 1.419391 val loss: 1.585366\n",
      "iteration 150 / 2000: training loss 1.504434 val loss: 1.568774\n",
      "iteration 200 / 2000: training loss 1.047446 val loss: 1.182951\n",
      "iteration 250 / 2000: training loss 1.262112 val loss: 1.190659\n",
      "iteration 300 / 2000: training loss 1.053292 val loss: 1.222258\n",
      "iteration 350 / 2000: training loss 1.031700 val loss: 0.935668\n",
      "hidden_size: 75 lr: 9.000000e-01 lr_decay: 8.500000e-01 reg: 2.000000e-02 train accuracy: 0.880000 val accuracy: 0.785000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.303018\n",
      "iteration 50 / 2000: training loss 3.613268 val loss: 3.569138\n",
      "iteration 100 / 2000: training loss 1.459956 val loss: 1.358877\n",
      "iteration 150 / 2000: training loss 1.185951 val loss: 1.171626\n",
      "iteration 200 / 2000: training loss 0.991818 val loss: 0.996452\n",
      "iteration 250 / 2000: training loss 1.133184 val loss: 1.152779\n",
      "iteration 300 / 2000: training loss 0.913503 val loss: 1.090014\n",
      "iteration 350 / 2000: training loss 1.127144 val loss: 0.980955\n",
      "iteration 400 / 2000: training loss 0.849905 val loss: 0.863732\n",
      "iteration 450 / 2000: training loss 0.902121 val loss: 0.924366\n",
      "iteration 500 / 2000: training loss 0.773979 val loss: 0.818349\n",
      "iteration 550 / 2000: training loss 0.731224 val loss: 0.818125\n",
      "iteration 600 / 2000: training loss 0.748448 val loss: 0.852501\n",
      "iteration 650 / 2000: training loss 0.671462 val loss: 0.751451\n",
      "iteration 700 / 2000: training loss 0.763422 val loss: 0.772035\n",
      "iteration 750 / 2000: training loss 0.763334 val loss: 0.740724\n",
      "iteration 800 / 2000: training loss 0.786901 val loss: 0.755564\n",
      "iteration 850 / 2000: training loss 0.742971 val loss: 0.754663\n",
      "iteration 900 / 2000: training loss 0.729407 val loss: 0.747847\n",
      "iteration 950 / 2000: training loss 0.706865 val loss: 0.738128\n",
      "iteration 1000 / 2000: training loss 0.704832 val loss: 0.731386\n",
      "iteration 1050 / 2000: training loss 0.702290 val loss: 0.741041\n",
      "hidden_size: 75 lr: 9.000000e-01 lr_decay: 8.000000e-01 reg: 2.000000e-02 train accuracy: 0.920000 val accuracy: 0.922000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.303019\n",
      "iteration 50 / 2000: training loss 3.616450 val loss: 3.718345\n",
      "iteration 100 / 2000: training loss 3.086517 val loss: 3.289865\n",
      "iteration 150 / 2000: training loss 3.845742 val loss: 3.580995\n",
      "iteration 200 / 2000: training loss 3.507655 val loss: 4.628035\n",
      "iteration 250 / 2000: training loss 2.700361 val loss: 2.700438\n",
      "iteration 300 / 2000: training loss 3.443513 val loss: 2.925964\n",
      "iteration 350 / 2000: training loss 3.737814 val loss: 3.960063\n",
      "iteration 400 / 2000: training loss 2.529114 val loss: 2.442772\n",
      "iteration 450 / 2000: training loss 2.354769 val loss: 2.373963\n",
      "hidden_size: 75 lr: 9.000000e-01 lr_decay: 9.800000e-01 reg: 4.000000e-02 train accuracy: 0.070000 val accuracy: 0.046000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.303019\n",
      "iteration 50 / 2000: training loss 3.955462 val loss: 4.202009\n",
      "iteration 100 / 2000: training loss 8.818988 val loss: 7.884922\n",
      "iteration 150 / 2000: training loss 14.277411 val loss: 12.037955\n",
      "iteration 200 / 2000: training loss 2.835461 val loss: 2.802667\n",
      "iteration 250 / 2000: training loss 2.716355 val loss: 2.664449\n",
      "iteration 300 / 2000: training loss 2.453431 val loss: 2.360482\n",
      "iteration 350 / 2000: training loss 2.211366 val loss: 2.267163\n",
      "iteration 400 / 2000: training loss 2.781363 val loss: 2.794818\n",
      "iteration 450 / 2000: training loss 2.023749 val loss: 2.055089\n",
      "iteration 500 / 2000: training loss 1.795008 val loss: 1.922670\n",
      "iteration 550 / 2000: training loss 1.662481 val loss: 1.702559\n",
      "iteration 600 / 2000: training loss 1.843595 val loss: 1.841350\n",
      "iteration 650 / 2000: training loss 1.331502 val loss: 1.476814\n",
      "iteration 700 / 2000: training loss 1.435971 val loss: 1.783834\n",
      "iteration 750 / 2000: training loss 1.489847 val loss: 1.555839\n",
      "iteration 800 / 2000: training loss 1.417235 val loss: 1.562845\n",
      "iteration 850 / 2000: training loss 1.407211 val loss: 1.347264\n",
      "iteration 900 / 2000: training loss 1.371806 val loss: 1.395410\n",
      "iteration 950 / 2000: training loss 1.210739 val loss: 1.567565\n",
      "iteration 1000 / 2000: training loss 1.443207 val loss: 1.377176\n",
      "iteration 1050 / 2000: training loss 1.162579 val loss: 1.303963\n",
      "iteration 1100 / 2000: training loss 1.175274 val loss: 1.252125\n",
      "iteration 1150 / 2000: training loss 1.377199 val loss: 1.296044\n",
      "iteration 1200 / 2000: training loss 1.376248 val loss: 1.241310\n",
      "iteration 1250 / 2000: training loss 1.295238 val loss: 1.292508\n",
      "iteration 1300 / 2000: training loss 1.330203 val loss: 1.264086\n",
      "iteration 1350 / 2000: training loss 1.295562 val loss: 1.225647\n",
      "hidden_size: 75 lr: 9.000000e-01 lr_decay: 9.000000e-01 reg: 4.000000e-02 train accuracy: 0.770000 val accuracy: 0.773000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.303019\n",
      "iteration 50 / 2000: training loss 2.766294 val loss: 3.026422\n",
      "iteration 100 / 2000: training loss 2.258568 val loss: 2.216669\n",
      "iteration 150 / 2000: training loss 1.610527 val loss: 1.771184\n",
      "iteration 200 / 2000: training loss 2.123286 val loss: 2.096013\n",
      "iteration 250 / 2000: training loss 1.744289 val loss: 1.857487\n",
      "iteration 300 / 2000: training loss 1.348576 val loss: 1.471560\n",
      "iteration 350 / 2000: training loss 1.520279 val loss: 1.394935\n",
      "iteration 400 / 2000: training loss 1.326181 val loss: 1.230709\n",
      "iteration 450 / 2000: training loss 1.309231 val loss: 1.208925\n",
      "iteration 500 / 2000: training loss 1.120962 val loss: 1.357097\n",
      "iteration 550 / 2000: training loss 1.131333 val loss: 1.206959\n",
      "iteration 600 / 2000: training loss 1.097968 val loss: 1.215415\n",
      "iteration 650 / 2000: training loss 1.073564 val loss: 1.140525\n",
      "iteration 700 / 2000: training loss 1.217144 val loss: 1.223837\n",
      "iteration 750 / 2000: training loss 1.094297 val loss: 1.093728\n",
      "iteration 800 / 2000: training loss 1.097924 val loss: 1.118003\n",
      "iteration 850 / 2000: training loss 1.137438 val loss: 1.086099\n",
      "iteration 900 / 2000: training loss 1.161597 val loss: 1.100992\n",
      "iteration 950 / 2000: training loss 1.008540 val loss: 1.080107\n",
      "iteration 1000 / 2000: training loss 1.083969 val loss: 1.089975\n",
      "iteration 1050 / 2000: training loss 1.039917 val loss: 1.069818\n",
      "iteration 1100 / 2000: training loss 1.045269 val loss: 1.071595\n",
      "iteration 1150 / 2000: training loss 1.006897 val loss: 1.089854\n",
      "iteration 1200 / 2000: training loss 1.060603 val loss: 1.054672\n",
      "iteration 1250 / 2000: training loss 1.102308 val loss: 1.066794\n",
      "iteration 1300 / 2000: training loss 1.054037 val loss: 1.060031\n",
      "iteration 1350 / 2000: training loss 1.116878 val loss: 1.058473\n",
      "iteration 1400 / 2000: training loss 1.027228 val loss: 1.046734\n",
      "iteration 1450 / 2000: training loss 1.104535 val loss: 1.047141\n",
      "iteration 1500 / 2000: training loss 1.036004 val loss: 1.037050\n",
      "iteration 1550 / 2000: training loss 1.047303 val loss: 1.040614\n",
      "iteration 1600 / 2000: training loss 1.066738 val loss: 1.047256\n",
      "iteration 1650 / 2000: training loss 1.033678 val loss: 1.043394\n",
      "iteration 1700 / 2000: training loss 0.991364 val loss: 1.031764\n",
      "iteration 1750 / 2000: training loss 0.944312 val loss: 1.025561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1800 / 2000: training loss 1.077193 val loss: 1.042558\n",
      "hidden_size: 75 lr: 9.000000e-01 lr_decay: 8.500000e-01 reg: 4.000000e-02 train accuracy: 0.910000 val accuracy: 0.908000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.303019\n",
      "iteration 50 / 2000: training loss 2.401655 val loss: 2.415547\n",
      "iteration 100 / 2000: training loss 2.564458 val loss: 2.524911\n",
      "iteration 150 / 2000: training loss 1.720459 val loss: 1.539433\n",
      "iteration 200 / 2000: training loss 1.324664 val loss: 1.428058\n",
      "iteration 250 / 2000: training loss 1.363819 val loss: 1.515005\n",
      "iteration 300 / 2000: training loss 1.285244 val loss: 1.431954\n",
      "iteration 350 / 2000: training loss 1.266775 val loss: 1.177598\n",
      "iteration 400 / 2000: training loss 1.218520 val loss: 1.158746\n",
      "iteration 450 / 2000: training loss 1.203395 val loss: 1.232556\n",
      "iteration 500 / 2000: training loss 1.085753 val loss: 1.142386\n",
      "iteration 550 / 2000: training loss 1.074770 val loss: 1.123416\n",
      "iteration 600 / 2000: training loss 1.091286 val loss: 1.140878\n",
      "iteration 650 / 2000: training loss 0.997121 val loss: 1.084342\n",
      "iteration 700 / 2000: training loss 1.095573 val loss: 1.134160\n",
      "iteration 750 / 2000: training loss 1.105258 val loss: 1.078148\n",
      "iteration 800 / 2000: training loss 1.077240 val loss: 1.064603\n",
      "iteration 850 / 2000: training loss 1.083133 val loss: 1.076135\n",
      "iteration 900 / 2000: training loss 1.067695 val loss: 1.055020\n",
      "iteration 950 / 2000: training loss 1.014296 val loss: 1.053619\n",
      "iteration 1000 / 2000: training loss 1.028993 val loss: 1.054491\n",
      "iteration 1050 / 2000: training loss 1.029028 val loss: 1.053713\n",
      "hidden_size: 75 lr: 9.000000e-01 lr_decay: 8.000000e-01 reg: 4.000000e-02 train accuracy: 0.870000 val accuracy: 0.902000\n",
      "iteration 0 / 2000: training loss 2.302591 val loss: 2.303021\n",
      "iteration 50 / 2000: training loss 4.918780 val loss: 4.957165\n",
      "iteration 100 / 2000: training loss 15.264688 val loss: 11.845101\n",
      "iteration 150 / 2000: training loss nan val loss: nan\n",
      "iteration 200 / 2000: training loss nan val loss: nan\n",
      "iteration 250 / 2000: training loss nan val loss: nan\n",
      "iteration 300 / 2000: training loss nan val loss: nan\n",
      "iteration 350 / 2000: training loss nan val loss: nan\n",
      "iteration 400 / 2000: training loss nan val loss: nan\n",
      "iteration 450 / 2000: training loss nan val loss: nan\n",
      "iteration 500 / 2000: training loss nan val loss: nan\n",
      "iteration 550 / 2000: training loss nan val loss: nan\n",
      "iteration 600 / 2000: training loss nan val loss: nan\n",
      "iteration 650 / 2000: training loss nan val loss: nan\n",
      "iteration 700 / 2000: training loss nan val loss: nan\n",
      "iteration 750 / 2000: training loss nan val loss: nan\n",
      "iteration 800 / 2000: training loss nan val loss: nan\n",
      "iteration 850 / 2000: training loss nan val loss: nan\n",
      "iteration 900 / 2000: training loss nan val loss: nan\n",
      "iteration 950 / 2000: training loss nan val loss: nan\n",
      "iteration 1000 / 2000: training loss nan val loss: nan\n",
      "iteration 1050 / 2000: training loss nan val loss: nan\n",
      "iteration 1100 / 2000: training loss nan val loss: nan\n",
      "iteration 1150 / 2000: training loss nan val loss: nan\n",
      "iteration 1200 / 2000: training loss nan val loss: nan\n",
      "iteration 1250 / 2000: training loss nan val loss: nan\n",
      "iteration 1300 / 2000: training loss nan val loss: nan\n",
      "iteration 1350 / 2000: training loss nan val loss: nan\n",
      "iteration 1400 / 2000: training loss nan val loss: nan\n",
      "iteration 1450 / 2000: training loss nan val loss: nan\n",
      "iteration 1500 / 2000: training loss nan val loss: nan\n",
      "iteration 1550 / 2000: training loss nan val loss: nan\n",
      "iteration 1600 / 2000: training loss nan val loss: nan\n",
      "iteration 1650 / 2000: training loss nan val loss: nan\n",
      "iteration 1700 / 2000: training loss nan val loss: nan\n",
      "iteration 1750 / 2000: training loss nan val loss: nan\n",
      "iteration 1800 / 2000: training loss nan val loss: nan\n",
      "iteration 1850 / 2000: training loss nan val loss: nan\n",
      "iteration 1900 / 2000: training loss nan val loss: nan\n",
      "iteration 1950 / 2000: training loss nan val loss: nan\n",
      "hidden_size: 75 lr: 9.000000e-01 lr_decay: 9.800000e-01 reg: 6.000000e-02 train accuracy: 0.080000 val accuracy: 0.100000\n",
      "iteration 0 / 2000: training loss 2.302591 val loss: 2.303021\n",
      "iteration 50 / 2000: training loss 4.737844 val loss: 3.968178\n",
      "iteration 100 / 2000: training loss 6.337229 val loss: 7.771461\n",
      "iteration 150 / 2000: training loss 9.166506 val loss: 7.099928\n",
      "iteration 200 / 2000: training loss 3.024979 val loss: 2.934520\n",
      "iteration 250 / 2000: training loss 2.578150 val loss: 2.931483\n",
      "iteration 300 / 2000: training loss 2.425076 val loss: 2.304573\n",
      "iteration 350 / 2000: training loss 2.472716 val loss: 2.440008\n",
      "iteration 400 / 2000: training loss 2.328390 val loss: 2.308093\n",
      "iteration 450 / 2000: training loss 2.360003 val loss: 2.321559\n",
      "hidden_size: 75 lr: 9.000000e-01 lr_decay: 9.000000e-01 reg: 6.000000e-02 train accuracy: 0.260000 val accuracy: 0.239000\n",
      "iteration 0 / 2000: training loss 2.302591 val loss: 2.303021\n",
      "iteration 50 / 2000: training loss 4.151581 val loss: 3.960206\n",
      "iteration 100 / 2000: training loss 5.945676 val loss: 5.387606\n",
      "iteration 150 / 2000: training loss 2.536702 val loss: 2.362640\n",
      "iteration 200 / 2000: training loss 2.154727 val loss: 2.150375\n",
      "iteration 250 / 2000: training loss 2.025056 val loss: 2.150160\n",
      "iteration 300 / 2000: training loss 1.588933 val loss: 1.615269\n",
      "iteration 350 / 2000: training loss 1.639513 val loss: 1.527529\n",
      "iteration 400 / 2000: training loss 1.604108 val loss: 1.473343\n",
      "iteration 450 / 2000: training loss 1.522069 val loss: 1.451678\n",
      "iteration 500 / 2000: training loss 1.405496 val loss: 1.511981\n",
      "iteration 550 / 2000: training loss 1.417807 val loss: 1.479356\n",
      "iteration 600 / 2000: training loss 1.392429 val loss: 1.577676\n",
      "iteration 650 / 2000: training loss 1.376018 val loss: 1.409914\n",
      "iteration 700 / 2000: training loss 1.461192 val loss: 1.429796\n",
      "iteration 750 / 2000: training loss 1.344365 val loss: 1.369564\n",
      "iteration 800 / 2000: training loss 1.328346 val loss: 1.369950\n",
      "hidden_size: 75 lr: 9.000000e-01 lr_decay: 8.500000e-01 reg: 6.000000e-02 train accuracy: 0.850000 val accuracy: 0.818000\n",
      "iteration 0 / 2000: training loss 2.302591 val loss: 2.303021\n",
      "iteration 50 / 2000: training loss 3.849170 val loss: 3.921816\n",
      "iteration 100 / 2000: training loss 2.514765 val loss: 2.300848\n",
      "iteration 150 / 2000: training loss 1.923631 val loss: 1.679926\n",
      "iteration 200 / 2000: training loss 1.765161 val loss: 1.626659\n",
      "iteration 250 / 2000: training loss 1.557059 val loss: 1.689484\n",
      "iteration 300 / 2000: training loss 1.530211 val loss: 1.739916\n",
      "iteration 350 / 2000: training loss 1.717900 val loss: 1.491742\n",
      "iteration 400 / 2000: training loss 1.553080 val loss: 1.487594\n",
      "iteration 450 / 2000: training loss 1.490858 val loss: 1.385407\n",
      "iteration 500 / 2000: training loss 1.338255 val loss: 1.368802\n",
      "iteration 550 / 2000: training loss 1.337457 val loss: 1.375846\n",
      "iteration 600 / 2000: training loss 1.347342 val loss: 1.385123\n",
      "iteration 650 / 2000: training loss 1.243490 val loss: 1.312290\n",
      "iteration 700 / 2000: training loss 1.368652 val loss: 1.418370\n",
      "iteration 750 / 2000: training loss 1.372232 val loss: 1.312346\n",
      "iteration 800 / 2000: training loss 1.308787 val loss: 1.306178\n",
      "iteration 850 / 2000: training loss 1.346050 val loss: 1.306520\n",
      "iteration 900 / 2000: training loss 1.321060 val loss: 1.323282\n",
      "iteration 950 / 2000: training loss 1.256809 val loss: 1.302078\n",
      "iteration 1000 / 2000: training loss 1.263991 val loss: 1.306188\n",
      "iteration 1050 / 2000: training loss 1.256414 val loss: 1.277400\n",
      "iteration 1100 / 2000: training loss 1.271447 val loss: 1.284565\n",
      "iteration 1150 / 2000: training loss 1.225860 val loss: 1.298974\n",
      "iteration 1200 / 2000: training loss 1.306217 val loss: 1.278907\n",
      "iteration 1250 / 2000: training loss 1.320842 val loss: 1.285092\n",
      "iteration 1300 / 2000: training loss 1.247738 val loss: 1.277632\n",
      "iteration 1350 / 2000: training loss 1.365644 val loss: 1.282515\n",
      "iteration 1400 / 2000: training loss 1.285914 val loss: 1.267581\n",
      "iteration 1450 / 2000: training loss 1.334457 val loss: 1.267932\n",
      "iteration 1500 / 2000: training loss 1.259975 val loss: 1.263309\n",
      "iteration 1550 / 2000: training loss 1.253933 val loss: 1.267427\n",
      "iteration 1600 / 2000: training loss 1.293661 val loss: 1.259827\n",
      "iteration 1650 / 2000: training loss 1.268572 val loss: 1.256597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1700 / 2000: training loss 1.220984 val loss: 1.261559\n",
      "hidden_size: 75 lr: 9.000000e-01 lr_decay: 8.000000e-01 reg: 6.000000e-02 train accuracy: 0.880000 val accuracy: 0.910000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302592\n",
      "iteration 50 / 2000: training loss 1.920003 val loss: 1.875631\n",
      "iteration 100 / 2000: training loss 0.711167 val loss: 0.778214\n",
      "iteration 150 / 2000: training loss 0.467971 val loss: 0.578143\n",
      "iteration 200 / 2000: training loss 0.503312 val loss: 0.571307\n",
      "iteration 250 / 2000: training loss 0.473909 val loss: 0.555465\n",
      "iteration 300 / 2000: training loss 0.496647 val loss: 0.570674\n",
      "iteration 350 / 2000: training loss 0.532587 val loss: 0.567349\n",
      "iteration 400 / 2000: training loss 0.501057 val loss: 0.540710\n",
      "iteration 450 / 2000: training loss 0.540074 val loss: 0.560526\n",
      "iteration 500 / 2000: training loss 0.521996 val loss: 0.529520\n",
      "iteration 550 / 2000: training loss 0.514848 val loss: 0.529631\n",
      "iteration 600 / 2000: training loss 0.447201 val loss: 0.532276\n",
      "iteration 650 / 2000: training loss 0.426933 val loss: 0.524969\n",
      "iteration 700 / 2000: training loss 0.547692 val loss: 0.529495\n",
      "iteration 750 / 2000: training loss 0.515139 val loss: 0.539280\n",
      "iteration 800 / 2000: training loss 0.509449 val loss: 0.542827\n",
      "iteration 850 / 2000: training loss 0.510642 val loss: 0.543535\n",
      "iteration 900 / 2000: training loss 0.483806 val loss: 0.531436\n",
      "iteration 950 / 2000: training loss 0.492306 val loss: 0.521060\n",
      "iteration 1000 / 2000: training loss 0.489893 val loss: 0.530380\n",
      "iteration 1050 / 2000: training loss 0.491390 val loss: 0.539216\n",
      "iteration 1100 / 2000: training loss 0.548249 val loss: 0.542887\n",
      "iteration 1150 / 2000: training loss 0.456302 val loss: 0.532933\n",
      "iteration 1200 / 2000: training loss 0.525010 val loss: 0.519509\n",
      "iteration 1250 / 2000: training loss 0.578855 val loss: 0.536408\n",
      "iteration 1300 / 2000: training loss 0.507503 val loss: 0.521818\n",
      "iteration 1350 / 2000: training loss 0.560470 val loss: 0.520839\n",
      "iteration 1400 / 2000: training loss 0.456361 val loss: 0.526061\n",
      "iteration 1450 / 2000: training loss 0.533291 val loss: 0.536913\n",
      "iteration 1500 / 2000: training loss 0.513623 val loss: 0.515715\n",
      "iteration 1550 / 2000: training loss 0.556518 val loss: 0.518515\n",
      "iteration 1600 / 2000: training loss 0.528310 val loss: 0.526898\n",
      "iteration 1650 / 2000: training loss 0.515241 val loss: 0.511330\n",
      "iteration 1700 / 2000: training loss 0.466696 val loss: 0.511604\n",
      "hidden_size: 150 lr: 1.000000e-01 lr_decay: 9.800000e-01 reg: 1.000000e-02 train accuracy: 0.930000 val accuracy: 0.930000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302592\n",
      "iteration 50 / 2000: training loss 2.009077 val loss: 1.983763\n",
      "iteration 100 / 2000: training loss 0.694733 val loss: 0.762034\n",
      "iteration 150 / 2000: training loss 0.460514 val loss: 0.574136\n",
      "iteration 200 / 2000: training loss 0.503484 val loss: 0.571095\n",
      "iteration 250 / 2000: training loss 0.468678 val loss: 0.547954\n",
      "iteration 300 / 2000: training loss 0.485579 val loss: 0.570130\n",
      "iteration 350 / 2000: training loss 0.528677 val loss: 0.540367\n",
      "iteration 400 / 2000: training loss 0.516079 val loss: 0.529358\n",
      "iteration 450 / 2000: training loss 0.531672 val loss: 0.538470\n",
      "iteration 500 / 2000: training loss 0.492016 val loss: 0.525564\n",
      "iteration 550 / 2000: training loss 0.526294 val loss: 0.521341\n",
      "iteration 600 / 2000: training loss 0.434965 val loss: 0.525067\n",
      "iteration 650 / 2000: training loss 0.432166 val loss: 0.519599\n",
      "iteration 700 / 2000: training loss 0.519602 val loss: 0.521442\n",
      "iteration 750 / 2000: training loss 0.500269 val loss: 0.519591\n",
      "iteration 800 / 2000: training loss 0.516776 val loss: 0.524874\n",
      "iteration 850 / 2000: training loss 0.511356 val loss: 0.516795\n",
      "iteration 900 / 2000: training loss 0.479240 val loss: 0.521208\n",
      "iteration 950 / 2000: training loss 0.470029 val loss: 0.512472\n",
      "iteration 1000 / 2000: training loss 0.484033 val loss: 0.512525\n",
      "iteration 1050 / 2000: training loss 0.482674 val loss: 0.512360\n",
      "iteration 1100 / 2000: training loss 0.500190 val loss: 0.514225\n",
      "iteration 1150 / 2000: training loss 0.460755 val loss: 0.514948\n",
      "iteration 1200 / 2000: training loss 0.513833 val loss: 0.516226\n",
      "iteration 1250 / 2000: training loss 0.552441 val loss: 0.521090\n",
      "hidden_size: 150 lr: 1.000000e-01 lr_decay: 9.000000e-01 reg: 1.000000e-02 train accuracy: 0.960000 val accuracy: 0.938000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302592\n",
      "iteration 50 / 2000: training loss 2.070323 val loss: 2.055760\n",
      "iteration 100 / 2000: training loss 0.676442 val loss: 0.738792\n",
      "iteration 150 / 2000: training loss 0.461059 val loss: 0.576814\n",
      "iteration 200 / 2000: training loss 0.504881 val loss: 0.560762\n",
      "iteration 250 / 2000: training loss 0.472401 val loss: 0.549669\n",
      "iteration 300 / 2000: training loss 0.472102 val loss: 0.564113\n",
      "iteration 350 / 2000: training loss 0.538455 val loss: 0.538034\n",
      "iteration 400 / 2000: training loss 0.530678 val loss: 0.535868\n",
      "iteration 450 / 2000: training loss 0.538891 val loss: 0.534828\n",
      "iteration 500 / 2000: training loss 0.489281 val loss: 0.529101\n",
      "iteration 550 / 2000: training loss 0.523044 val loss: 0.530905\n",
      "iteration 600 / 2000: training loss 0.428646 val loss: 0.521559\n",
      "iteration 650 / 2000: training loss 0.431476 val loss: 0.522987\n",
      "iteration 700 / 2000: training loss 0.515955 val loss: 0.519728\n",
      "iteration 750 / 2000: training loss 0.505873 val loss: 0.516648\n",
      "iteration 800 / 2000: training loss 0.545509 val loss: 0.528675\n",
      "iteration 850 / 2000: training loss 0.514488 val loss: 0.518771\n",
      "iteration 900 / 2000: training loss 0.477068 val loss: 0.518257\n",
      "iteration 950 / 2000: training loss 0.456117 val loss: 0.515996\n",
      "iteration 1000 / 2000: training loss 0.482889 val loss: 0.514363\n",
      "iteration 1050 / 2000: training loss 0.482489 val loss: 0.512746\n",
      "iteration 1100 / 2000: training loss 0.500795 val loss: 0.512920\n",
      "iteration 1150 / 2000: training loss 0.463108 val loss: 0.513697\n",
      "iteration 1200 / 2000: training loss 0.508919 val loss: 0.512292\n",
      "iteration 1250 / 2000: training loss 0.538758 val loss: 0.514284\n",
      "iteration 1300 / 2000: training loss 0.497838 val loss: 0.512984\n",
      "iteration 1350 / 2000: training loss 0.515794 val loss: 0.513806\n",
      "iteration 1400 / 2000: training loss 0.450895 val loss: 0.513077\n",
      "iteration 1450 / 2000: training loss 0.519884 val loss: 0.515303\n",
      "iteration 1500 / 2000: training loss 0.474746 val loss: 0.510055\n",
      "iteration 1550 / 2000: training loss 0.546771 val loss: 0.512529\n",
      "iteration 1600 / 2000: training loss 0.516449 val loss: 0.510447\n",
      "iteration 1650 / 2000: training loss 0.493153 val loss: 0.508737\n",
      "iteration 1700 / 2000: training loss 0.468209 val loss: 0.510036\n",
      "iteration 1750 / 2000: training loss 0.424373 val loss: 0.508285\n",
      "iteration 1800 / 2000: training loss 0.539785 val loss: 0.508745\n",
      "iteration 1850 / 2000: training loss 0.484674 val loss: 0.508616\n",
      "iteration 1900 / 2000: training loss 0.413370 val loss: 0.508473\n",
      "iteration 1950 / 2000: training loss 0.489200 val loss: 0.508858\n",
      "hidden_size: 150 lr: 1.000000e-01 lr_decay: 8.500000e-01 reg: 1.000000e-02 train accuracy: 0.960000 val accuracy: 0.944000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302592\n",
      "iteration 50 / 2000: training loss 2.130230 val loss: 2.118149\n",
      "iteration 100 / 2000: training loss 0.675738 val loss: 0.729414\n",
      "iteration 150 / 2000: training loss 0.461666 val loss: 0.584691\n",
      "iteration 200 / 2000: training loss 0.507964 val loss: 0.563094\n",
      "iteration 250 / 2000: training loss 0.475244 val loss: 0.556061\n",
      "iteration 300 / 2000: training loss 0.462812 val loss: 0.557764\n",
      "iteration 350 / 2000: training loss 0.549369 val loss: 0.541499\n",
      "iteration 400 / 2000: training loss 0.533265 val loss: 0.540626\n",
      "iteration 450 / 2000: training loss 0.537696 val loss: 0.532154\n",
      "iteration 500 / 2000: training loss 0.491379 val loss: 0.526525\n",
      "iteration 550 / 2000: training loss 0.513319 val loss: 0.532187\n",
      "iteration 600 / 2000: training loss 0.432860 val loss: 0.520624\n",
      "hidden_size: 150 lr: 1.000000e-01 lr_decay: 8.000000e-01 reg: 1.000000e-02 train accuracy: 0.940000 val accuracy: 0.932000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302594\n",
      "iteration 50 / 2000: training loss 1.961670 val loss: 1.926556\n",
      "iteration 100 / 2000: training loss 0.926061 val loss: 0.950637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 150 / 2000: training loss 0.705697 val loss: 0.762565\n",
      "iteration 200 / 2000: training loss 0.724106 val loss: 0.765792\n",
      "iteration 250 / 2000: training loss 0.696847 val loss: 0.777957\n",
      "iteration 300 / 2000: training loss 0.692219 val loss: 0.765825\n",
      "iteration 350 / 2000: training loss 0.754080 val loss: 0.785858\n",
      "iteration 400 / 2000: training loss 0.725536 val loss: 0.737438\n",
      "iteration 450 / 2000: training loss 0.817482 val loss: 0.776657\n",
      "hidden_size: 150 lr: 1.000000e-01 lr_decay: 9.800000e-01 reg: 2.000000e-02 train accuracy: 0.900000 val accuracy: 0.916000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302594\n",
      "iteration 50 / 2000: training loss 2.046079 val loss: 2.028591\n",
      "iteration 100 / 2000: training loss 0.899027 val loss: 0.942258\n",
      "iteration 150 / 2000: training loss 0.694281 val loss: 0.775753\n",
      "iteration 200 / 2000: training loss 0.720522 val loss: 0.770532\n",
      "iteration 250 / 2000: training loss 0.675281 val loss: 0.755603\n",
      "iteration 300 / 2000: training loss 0.698434 val loss: 0.773022\n",
      "iteration 350 / 2000: training loss 0.748180 val loss: 0.753708\n",
      "iteration 400 / 2000: training loss 0.732984 val loss: 0.741016\n",
      "iteration 450 / 2000: training loss 0.756560 val loss: 0.744178\n",
      "iteration 500 / 2000: training loss 0.719023 val loss: 0.729545\n",
      "iteration 550 / 2000: training loss 0.721679 val loss: 0.724730\n",
      "iteration 600 / 2000: training loss 0.649383 val loss: 0.732582\n",
      "iteration 650 / 2000: training loss 0.639519 val loss: 0.726426\n",
      "iteration 700 / 2000: training loss 0.740129 val loss: 0.738161\n",
      "iteration 750 / 2000: training loss 0.725735 val loss: 0.727979\n",
      "iteration 800 / 2000: training loss 0.720499 val loss: 0.740500\n",
      "iteration 850 / 2000: training loss 0.738987 val loss: 0.723436\n",
      "iteration 900 / 2000: training loss 0.693202 val loss: 0.727380\n",
      "iteration 950 / 2000: training loss 0.682148 val loss: 0.718956\n",
      "iteration 1000 / 2000: training loss 0.700868 val loss: 0.718051\n",
      "iteration 1050 / 2000: training loss 0.700446 val loss: 0.718284\n",
      "iteration 1100 / 2000: training loss 0.710889 val loss: 0.723756\n",
      "iteration 1150 / 2000: training loss 0.675014 val loss: 0.722592\n",
      "iteration 1200 / 2000: training loss 0.744600 val loss: 0.725169\n",
      "iteration 1250 / 2000: training loss 0.766451 val loss: 0.727021\n",
      "iteration 1300 / 2000: training loss 0.723147 val loss: 0.721067\n",
      "iteration 1350 / 2000: training loss 0.762031 val loss: 0.726096\n",
      "iteration 1400 / 2000: training loss 0.682146 val loss: 0.716988\n",
      "iteration 1450 / 2000: training loss 0.764803 val loss: 0.723110\n",
      "iteration 1500 / 2000: training loss 0.692728 val loss: 0.718371\n",
      "iteration 1550 / 2000: training loss 0.747973 val loss: 0.719980\n",
      "iteration 1600 / 2000: training loss 0.737571 val loss: 0.715262\n",
      "iteration 1650 / 2000: training loss 0.718465 val loss: 0.712124\n",
      "iteration 1700 / 2000: training loss 0.667267 val loss: 0.712802\n",
      "iteration 1750 / 2000: training loss 0.637141 val loss: 0.713554\n",
      "iteration 1800 / 2000: training loss 0.760710 val loss: 0.713563\n",
      "iteration 1850 / 2000: training loss 0.711387 val loss: 0.713211\n",
      "iteration 1900 / 2000: training loss 0.624385 val loss: 0.712555\n",
      "iteration 1950 / 2000: training loss 0.713596 val loss: 0.717491\n",
      "hidden_size: 150 lr: 1.000000e-01 lr_decay: 9.000000e-01 reg: 2.000000e-02 train accuracy: 0.930000 val accuracy: 0.933000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302594\n",
      "iteration 50 / 2000: training loss 2.101445 val loss: 2.091161\n",
      "iteration 100 / 2000: training loss 0.880840 val loss: 0.923468\n",
      "iteration 150 / 2000: training loss 0.687990 val loss: 0.780461\n",
      "iteration 200 / 2000: training loss 0.713662 val loss: 0.769272\n",
      "iteration 250 / 2000: training loss 0.675235 val loss: 0.749686\n",
      "iteration 300 / 2000: training loss 0.685999 val loss: 0.769236\n",
      "iteration 350 / 2000: training loss 0.742540 val loss: 0.744158\n",
      "iteration 400 / 2000: training loss 0.749558 val loss: 0.739116\n",
      "iteration 450 / 2000: training loss 0.762480 val loss: 0.742365\n",
      "iteration 500 / 2000: training loss 0.705150 val loss: 0.734721\n",
      "iteration 550 / 2000: training loss 0.730287 val loss: 0.732161\n",
      "iteration 600 / 2000: training loss 0.649476 val loss: 0.731180\n",
      "hidden_size: 150 lr: 1.000000e-01 lr_decay: 8.500000e-01 reg: 2.000000e-02 train accuracy: 0.930000 val accuracy: 0.921000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302594\n",
      "iteration 50 / 2000: training loss 2.156274 val loss: 2.145092\n",
      "iteration 100 / 2000: training loss 0.881234 val loss: 0.919405\n",
      "iteration 150 / 2000: training loss 0.685856 val loss: 0.786632\n",
      "iteration 200 / 2000: training loss 0.713622 val loss: 0.763479\n",
      "iteration 250 / 2000: training loss 0.679880 val loss: 0.752257\n",
      "iteration 300 / 2000: training loss 0.678340 val loss: 0.764164\n",
      "iteration 350 / 2000: training loss 0.751477 val loss: 0.742682\n",
      "iteration 400 / 2000: training loss 0.766461 val loss: 0.744283\n",
      "iteration 450 / 2000: training loss 0.764689 val loss: 0.737557\n",
      "iteration 500 / 2000: training loss 0.707875 val loss: 0.730795\n",
      "iteration 550 / 2000: training loss 0.720891 val loss: 0.736627\n",
      "iteration 600 / 2000: training loss 0.646797 val loss: 0.724723\n",
      "iteration 650 / 2000: training loss 0.642575 val loss: 0.729962\n",
      "iteration 700 / 2000: training loss 0.743042 val loss: 0.731516\n",
      "iteration 750 / 2000: training loss 0.726651 val loss: 0.723536\n",
      "iteration 800 / 2000: training loss 0.751613 val loss: 0.732345\n",
      "iteration 850 / 2000: training loss 0.736244 val loss: 0.724369\n",
      "iteration 900 / 2000: training loss 0.700020 val loss: 0.723758\n",
      "iteration 950 / 2000: training loss 0.669659 val loss: 0.723695\n",
      "iteration 1000 / 2000: training loss 0.703768 val loss: 0.722604\n",
      "iteration 1050 / 2000: training loss 0.700459 val loss: 0.722054\n",
      "iteration 1100 / 2000: training loss 0.707029 val loss: 0.722871\n",
      "iteration 1150 / 2000: training loss 0.681124 val loss: 0.721088\n",
      "hidden_size: 150 lr: 1.000000e-01 lr_decay: 8.000000e-01 reg: 2.000000e-02 train accuracy: 0.940000 val accuracy: 0.932000\n",
      "iteration 0 / 2000: training loss 2.302593 val loss: 2.302598\n",
      "iteration 50 / 2000: training loss 2.036711 val loss: 2.017273\n",
      "iteration 100 / 2000: training loss 1.216970 val loss: 1.208363\n",
      "iteration 150 / 2000: training loss 1.045517 val loss: 1.074241\n",
      "iteration 200 / 2000: training loss 1.036671 val loss: 1.095065\n",
      "iteration 250 / 2000: training loss 1.037867 val loss: 1.092062\n",
      "iteration 300 / 2000: training loss 1.005303 val loss: 1.076585\n",
      "iteration 350 / 2000: training loss 1.084123 val loss: 1.104746\n",
      "iteration 400 / 2000: training loss 1.097253 val loss: 1.051145\n",
      "iteration 450 / 2000: training loss 1.138838 val loss: 1.088918\n",
      "hidden_size: 150 lr: 1.000000e-01 lr_decay: 9.800000e-01 reg: 4.000000e-02 train accuracy: 0.890000 val accuracy: 0.890000\n",
      "iteration 0 / 2000: training loss 2.302593 val loss: 2.302598\n",
      "iteration 50 / 2000: training loss 2.108821 val loss: 2.102057\n",
      "iteration 100 / 2000: training loss 1.188097 val loss: 1.206756\n",
      "iteration 150 / 2000: training loss 1.028853 val loss: 1.077889\n",
      "iteration 200 / 2000: training loss 1.039576 val loss: 1.073548\n",
      "iteration 250 / 2000: training loss 0.989603 val loss: 1.068202\n",
      "iteration 300 / 2000: training loss 1.004739 val loss: 1.082752\n",
      "iteration 350 / 2000: training loss 1.077992 val loss: 1.070349\n",
      "iteration 400 / 2000: training loss 1.059656 val loss: 1.051822\n",
      "iteration 450 / 2000: training loss 1.095002 val loss: 1.053108\n",
      "hidden_size: 150 lr: 1.000000e-01 lr_decay: 9.000000e-01 reg: 4.000000e-02 train accuracy: 0.880000 val accuracy: 0.905000\n",
      "iteration 0 / 2000: training loss 2.302593 val loss: 2.302598\n",
      "iteration 50 / 2000: training loss 2.155112 val loss: 2.147982\n",
      "iteration 100 / 2000: training loss 1.180503 val loss: 1.202095\n",
      "iteration 150 / 2000: training loss 1.019519 val loss: 1.084509\n",
      "iteration 200 / 2000: training loss 1.041218 val loss: 1.075981\n",
      "iteration 250 / 2000: training loss 0.985205 val loss: 1.059203\n",
      "iteration 300 / 2000: training loss 0.994863 val loss: 1.078094\n",
      "iteration 350 / 2000: training loss 1.070789 val loss: 1.061432\n",
      "iteration 400 / 2000: training loss 1.074530 val loss: 1.050565\n",
      "iteration 450 / 2000: training loss 1.083830 val loss: 1.049201\n",
      "iteration 500 / 2000: training loss 1.020658 val loss: 1.041794\n",
      "iteration 550 / 2000: training loss 1.028742 val loss: 1.033182\n",
      "iteration 600 / 2000: training loss 0.967627 val loss: 1.038969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 650 / 2000: training loss 0.958609 val loss: 1.037818\n",
      "iteration 700 / 2000: training loss 1.044780 val loss: 1.040085\n",
      "iteration 750 / 2000: training loss 1.042180 val loss: 1.031279\n",
      "iteration 800 / 2000: training loss 1.035671 val loss: 1.045662\n",
      "iteration 850 / 2000: training loss 1.073439 val loss: 1.032730\n",
      "iteration 900 / 2000: training loss 1.009524 val loss: 1.036216\n",
      "iteration 950 / 2000: training loss 0.989674 val loss: 1.028194\n",
      "iteration 1000 / 2000: training loss 1.025691 val loss: 1.026324\n",
      "iteration 1050 / 2000: training loss 1.016679 val loss: 1.028190\n",
      "iteration 1100 / 2000: training loss 1.004065 val loss: 1.027256\n",
      "iteration 1150 / 2000: training loss 0.979784 val loss: 1.025376\n",
      "iteration 1200 / 2000: training loss 1.055790 val loss: 1.025308\n",
      "iteration 1250 / 2000: training loss 1.062176 val loss: 1.028347\n",
      "iteration 1300 / 2000: training loss 1.035121 val loss: 1.027786\n",
      "iteration 1350 / 2000: training loss 1.069734 val loss: 1.026669\n",
      "iteration 1400 / 2000: training loss 1.019072 val loss: 1.026744\n",
      "iteration 1450 / 2000: training loss 1.062662 val loss: 1.030104\n",
      "iteration 1500 / 2000: training loss 1.005474 val loss: 1.023638\n",
      "iteration 1550 / 2000: training loss 1.026753 val loss: 1.024651\n",
      "iteration 1600 / 2000: training loss 1.058126 val loss: 1.022414\n",
      "iteration 1650 / 2000: training loss 1.024936 val loss: 1.020822\n",
      "iteration 1700 / 2000: training loss 0.979624 val loss: 1.023296\n",
      "iteration 1750 / 2000: training loss 0.934320 val loss: 1.021749\n",
      "iteration 1800 / 2000: training loss 1.072090 val loss: 1.020234\n",
      "iteration 1850 / 2000: training loss 1.023538 val loss: 1.021073\n",
      "iteration 1900 / 2000: training loss 0.943536 val loss: 1.020943\n",
      "iteration 1950 / 2000: training loss 1.025889 val loss: 1.020483\n",
      "hidden_size: 150 lr: 1.000000e-01 lr_decay: 8.500000e-01 reg: 4.000000e-02 train accuracy: 0.910000 val accuracy: 0.921000\n",
      "iteration 0 / 2000: training loss 2.302593 val loss: 2.302598\n",
      "iteration 50 / 2000: training loss 2.200564 val loss: 2.190197\n",
      "iteration 100 / 2000: training loss 1.192741 val loss: 1.211865\n",
      "iteration 150 / 2000: training loss 1.015616 val loss: 1.090736\n",
      "iteration 200 / 2000: training loss 1.032363 val loss: 1.074429\n",
      "iteration 250 / 2000: training loss 0.989625 val loss: 1.057711\n",
      "iteration 300 / 2000: training loss 0.989077 val loss: 1.074258\n",
      "iteration 350 / 2000: training loss 1.062786 val loss: 1.051993\n",
      "iteration 400 / 2000: training loss 1.098885 val loss: 1.054434\n",
      "iteration 450 / 2000: training loss 1.088307 val loss: 1.044480\n",
      "iteration 500 / 2000: training loss 1.017386 val loss: 1.037922\n",
      "iteration 550 / 2000: training loss 1.028773 val loss: 1.040533\n",
      "iteration 600 / 2000: training loss 0.962184 val loss: 1.031011\n",
      "hidden_size: 150 lr: 1.000000e-01 lr_decay: 8.000000e-01 reg: 4.000000e-02 train accuracy: 0.930000 val accuracy: 0.910000\n",
      "iteration 0 / 2000: training loss 2.302597 val loss: 2.302602\n",
      "iteration 50 / 2000: training loss 2.099075 val loss: 2.093069\n",
      "iteration 100 / 2000: training loss 1.426543 val loss: 1.408746\n",
      "iteration 150 / 2000: training loss 1.279713 val loss: 1.314280\n",
      "iteration 200 / 2000: training loss 1.269990 val loss: 1.335234\n",
      "iteration 250 / 2000: training loss 1.276012 val loss: 1.327048\n",
      "iteration 300 / 2000: training loss 1.240046 val loss: 1.313853\n",
      "iteration 350 / 2000: training loss 1.339861 val loss: 1.343975\n",
      "iteration 400 / 2000: training loss 1.362858 val loss: 1.291967\n",
      "iteration 450 / 2000: training loss 1.386504 val loss: 1.330167\n",
      "hidden_size: 150 lr: 1.000000e-01 lr_decay: 9.800000e-01 reg: 6.000000e-02 train accuracy: 0.890000 val accuracy: 0.865000\n",
      "iteration 0 / 2000: training loss 2.302597 val loss: 2.302602\n",
      "iteration 50 / 2000: training loss 2.159852 val loss: 2.156544\n",
      "iteration 100 / 2000: training loss 1.412639 val loss: 1.419553\n",
      "iteration 150 / 2000: training loss 1.274956 val loss: 1.324131\n",
      "iteration 200 / 2000: training loss 1.273782 val loss: 1.317612\n",
      "iteration 250 / 2000: training loss 1.238126 val loss: 1.304924\n",
      "iteration 300 / 2000: training loss 1.244751 val loss: 1.322806\n",
      "iteration 350 / 2000: training loss 1.324498 val loss: 1.312294\n",
      "iteration 400 / 2000: training loss 1.302788 val loss: 1.284552\n",
      "iteration 450 / 2000: training loss 1.348039 val loss: 1.292102\n",
      "iteration 500 / 2000: training loss 1.268644 val loss: 1.283904\n",
      "iteration 550 / 2000: training loss 1.249762 val loss: 1.277226\n",
      "iteration 600 / 2000: training loss 1.210524 val loss: 1.288083\n",
      "iteration 650 / 2000: training loss 1.197687 val loss: 1.281344\n",
      "iteration 700 / 2000: training loss 1.273026 val loss: 1.283580\n",
      "hidden_size: 150 lr: 1.000000e-01 lr_decay: 9.000000e-01 reg: 6.000000e-02 train accuracy: 0.900000 val accuracy: 0.884000\n",
      "iteration 0 / 2000: training loss 2.302597 val loss: 2.302602\n",
      "iteration 50 / 2000: training loss 2.199450 val loss: 2.192041\n",
      "iteration 100 / 2000: training loss 1.419975 val loss: 1.425627\n",
      "iteration 150 / 2000: training loss 1.263852 val loss: 1.324303\n",
      "iteration 200 / 2000: training loss 1.281070 val loss: 1.312952\n",
      "iteration 250 / 2000: training loss 1.229033 val loss: 1.298461\n",
      "iteration 300 / 2000: training loss 1.228275 val loss: 1.315569\n",
      "iteration 350 / 2000: training loss 1.324915 val loss: 1.303319\n",
      "hidden_size: 150 lr: 1.000000e-01 lr_decay: 8.500000e-01 reg: 6.000000e-02 train accuracy: 0.890000 val accuracy: 0.885000\n",
      "iteration 0 / 2000: training loss 2.302597 val loss: 2.302602\n",
      "iteration 50 / 2000: training loss 2.234879 val loss: 2.225437\n",
      "iteration 100 / 2000: training loss 1.456867 val loss: 1.454686\n",
      "iteration 150 / 2000: training loss 1.258413 val loss: 1.324875\n",
      "iteration 200 / 2000: training loss 1.281048 val loss: 1.314359\n",
      "iteration 250 / 2000: training loss 1.229729 val loss: 1.297435\n",
      "iteration 300 / 2000: training loss 1.223251 val loss: 1.311615\n",
      "iteration 350 / 2000: training loss 1.311734 val loss: 1.291795\n",
      "iteration 400 / 2000: training loss 1.343712 val loss: 1.293768\n",
      "iteration 450 / 2000: training loss 1.328690 val loss: 1.280026\n",
      "iteration 500 / 2000: training loss 1.251037 val loss: 1.273408\n",
      "iteration 550 / 2000: training loss 1.265585 val loss: 1.273522\n",
      "iteration 600 / 2000: training loss 1.202119 val loss: 1.267297\n",
      "hidden_size: 150 lr: 1.000000e-01 lr_decay: 8.000000e-01 reg: 6.000000e-02 train accuracy: 0.900000 val accuracy: 0.898000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302634\n",
      "iteration 50 / 2000: training loss 0.930432 val loss: 0.917965\n",
      "iteration 100 / 2000: training loss 0.602410 val loss: 0.665821\n",
      "iteration 150 / 2000: training loss 0.828701 val loss: 0.669899\n",
      "iteration 200 / 2000: training loss 0.588442 val loss: 0.621039\n",
      "iteration 250 / 2000: training loss 0.659700 val loss: 0.662472\n",
      "iteration 300 / 2000: training loss 0.651046 val loss: 0.649897\n",
      "iteration 350 / 2000: training loss 0.581907 val loss: 0.652311\n",
      "iteration 400 / 2000: training loss 0.561720 val loss: 0.568756\n",
      "iteration 450 / 2000: training loss 0.641571 val loss: 0.595215\n",
      "iteration 500 / 2000: training loss 0.627518 val loss: 0.643093\n",
      "iteration 550 / 2000: training loss 0.653256 val loss: 0.666182\n",
      "iteration 600 / 2000: training loss 0.511001 val loss: 0.623091\n",
      "iteration 650 / 2000: training loss 0.483514 val loss: 0.588707\n",
      "iteration 700 / 2000: training loss 0.688902 val loss: 0.631428\n",
      "iteration 750 / 2000: training loss 0.564459 val loss: 0.580062\n",
      "iteration 800 / 2000: training loss 0.590389 val loss: 0.612759\n",
      "iteration 850 / 2000: training loss 0.607174 val loss: 0.578192\n",
      "iteration 900 / 2000: training loss 0.577721 val loss: 0.578213\n",
      "iteration 950 / 2000: training loss 0.536355 val loss: 0.612462\n",
      "iteration 1000 / 2000: training loss 0.551912 val loss: 0.594036\n",
      "iteration 1050 / 2000: training loss 0.536097 val loss: 0.596206\n",
      "hidden_size: 150 lr: 3.000000e-01 lr_decay: 9.800000e-01 reg: 1.000000e-02 train accuracy: 0.970000 val accuracy: 0.920000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302634\n",
      "iteration 50 / 2000: training loss 0.831049 val loss: 0.839779\n",
      "iteration 100 / 2000: training loss 0.615828 val loss: 0.615630\n",
      "iteration 150 / 2000: training loss 0.691671 val loss: 0.671571\n",
      "iteration 200 / 2000: training loss 0.589371 val loss: 0.652086\n",
      "iteration 250 / 2000: training loss 0.557240 val loss: 0.638887\n",
      "iteration 300 / 2000: training loss 0.552840 val loss: 0.632262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 350 / 2000: training loss 0.560856 val loss: 0.589540\n",
      "iteration 400 / 2000: training loss 0.551678 val loss: 0.541097\n",
      "iteration 450 / 2000: training loss 0.603968 val loss: 0.591495\n",
      "hidden_size: 150 lr: 3.000000e-01 lr_decay: 9.000000e-01 reg: 1.000000e-02 train accuracy: 0.920000 val accuracy: 0.922000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302634\n",
      "iteration 50 / 2000: training loss 0.770584 val loss: 0.827668\n",
      "iteration 100 / 2000: training loss 0.605976 val loss: 0.592924\n",
      "iteration 150 / 2000: training loss 0.573450 val loss: 0.643584\n",
      "iteration 200 / 2000: training loss 0.536230 val loss: 0.625933\n",
      "iteration 250 / 2000: training loss 0.527747 val loss: 0.611668\n",
      "iteration 300 / 2000: training loss 0.506373 val loss: 0.582977\n",
      "iteration 350 / 2000: training loss 0.569579 val loss: 0.585493\n",
      "iteration 400 / 2000: training loss 0.510487 val loss: 0.532661\n",
      "iteration 450 / 2000: training loss 0.586378 val loss: 0.591433\n",
      "hidden_size: 150 lr: 3.000000e-01 lr_decay: 8.500000e-01 reg: 1.000000e-02 train accuracy: 0.920000 val accuracy: 0.921000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302634\n",
      "iteration 50 / 2000: training loss 0.777913 val loss: 0.888239\n",
      "iteration 100 / 2000: training loss 0.625307 val loss: 0.597111\n",
      "iteration 150 / 2000: training loss 0.572402 val loss: 0.607907\n",
      "iteration 200 / 2000: training loss 0.515238 val loss: 0.604201\n",
      "iteration 250 / 2000: training loss 0.535146 val loss: 0.580082\n",
      "iteration 300 / 2000: training loss 0.501774 val loss: 0.561430\n",
      "iteration 350 / 2000: training loss 0.537631 val loss: 0.572135\n",
      "iteration 400 / 2000: training loss 0.498760 val loss: 0.534311\n",
      "iteration 450 / 2000: training loss 0.522551 val loss: 0.549453\n",
      "iteration 500 / 2000: training loss 0.502251 val loss: 0.513182\n",
      "iteration 550 / 2000: training loss 0.513107 val loss: 0.509857\n",
      "iteration 600 / 2000: training loss 0.431339 val loss: 0.517318\n",
      "iteration 650 / 2000: training loss 0.428049 val loss: 0.512212\n",
      "iteration 700 / 2000: training loss 0.513610 val loss: 0.515594\n",
      "iteration 750 / 2000: training loss 0.486075 val loss: 0.509263\n",
      "iteration 800 / 2000: training loss 0.517170 val loss: 0.518966\n",
      "iteration 850 / 2000: training loss 0.509476 val loss: 0.509105\n",
      "iteration 900 / 2000: training loss 0.471657 val loss: 0.513072\n",
      "iteration 950 / 2000: training loss 0.452087 val loss: 0.505903\n",
      "iteration 1000 / 2000: training loss 0.475436 val loss: 0.503869\n",
      "iteration 1050 / 2000: training loss 0.475464 val loss: 0.502493\n",
      "iteration 1100 / 2000: training loss 0.494926 val loss: 0.501486\n",
      "iteration 1150 / 2000: training loss 0.449243 val loss: 0.503957\n",
      "iteration 1200 / 2000: training loss 0.500946 val loss: 0.502331\n",
      "iteration 1250 / 2000: training loss 0.525705 val loss: 0.504594\n",
      "hidden_size: 150 lr: 3.000000e-01 lr_decay: 8.000000e-01 reg: 1.000000e-02 train accuracy: 0.950000 val accuracy: 0.943000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302636\n",
      "iteration 50 / 2000: training loss 1.163190 val loss: 1.143373\n",
      "iteration 100 / 2000: training loss 0.888565 val loss: 0.904446\n",
      "iteration 150 / 2000: training loss 1.063628 val loss: 0.960341\n",
      "iteration 200 / 2000: training loss 0.803254 val loss: 0.836549\n",
      "iteration 250 / 2000: training loss 0.846963 val loss: 0.857994\n",
      "iteration 300 / 2000: training loss 0.843655 val loss: 0.967809\n",
      "iteration 350 / 2000: training loss 0.902186 val loss: 0.880746\n",
      "iteration 400 / 2000: training loss 0.852230 val loss: 0.848236\n",
      "iteration 450 / 2000: training loss 0.877799 val loss: 0.861467\n",
      "iteration 500 / 2000: training loss 0.780133 val loss: 0.831013\n",
      "hidden_size: 150 lr: 3.000000e-01 lr_decay: 9.800000e-01 reg: 2.000000e-02 train accuracy: 0.910000 val accuracy: 0.866000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302636\n",
      "iteration 50 / 2000: training loss 1.013726 val loss: 1.073161\n",
      "iteration 100 / 2000: training loss 0.879124 val loss: 0.854514\n",
      "iteration 150 / 2000: training loss 0.986893 val loss: 0.900271\n",
      "iteration 200 / 2000: training loss 0.792704 val loss: 0.846492\n",
      "iteration 250 / 2000: training loss 0.836534 val loss: 0.824006\n",
      "iteration 300 / 2000: training loss 0.776228 val loss: 0.816267\n",
      "iteration 350 / 2000: training loss 0.791731 val loss: 0.872061\n",
      "iteration 400 / 2000: training loss 0.794017 val loss: 0.780675\n",
      "iteration 450 / 2000: training loss 0.844068 val loss: 0.790738\n",
      "iteration 500 / 2000: training loss 0.741575 val loss: 0.761302\n",
      "iteration 550 / 2000: training loss 0.733788 val loss: 0.773629\n",
      "iteration 600 / 2000: training loss 0.679572 val loss: 0.807609\n",
      "iteration 650 / 2000: training loss 0.677833 val loss: 0.755494\n",
      "iteration 700 / 2000: training loss 0.756019 val loss: 0.753286\n",
      "iteration 750 / 2000: training loss 0.775821 val loss: 0.733620\n",
      "iteration 800 / 2000: training loss 0.787960 val loss: 0.741601\n",
      "iteration 850 / 2000: training loss 0.757264 val loss: 0.759094\n",
      "iteration 900 / 2000: training loss 0.761282 val loss: 0.736001\n",
      "iteration 950 / 2000: training loss 0.701672 val loss: 0.744587\n",
      "iteration 1000 / 2000: training loss 0.710032 val loss: 0.750801\n",
      "iteration 1050 / 2000: training loss 0.720760 val loss: 0.751901\n",
      "iteration 1100 / 2000: training loss 0.753207 val loss: 0.747960\n",
      "iteration 1150 / 2000: training loss 0.677501 val loss: 0.745971\n",
      "iteration 1200 / 2000: training loss 0.742037 val loss: 0.728248\n",
      "iteration 1250 / 2000: training loss 0.789334 val loss: 0.742961\n",
      "iteration 1300 / 2000: training loss 0.713511 val loss: 0.727400\n",
      "iteration 1350 / 2000: training loss 0.787000 val loss: 0.732140\n",
      "iteration 1400 / 2000: training loss 0.681870 val loss: 0.727833\n",
      "iteration 1450 / 2000: training loss 0.761958 val loss: 0.729929\n",
      "iteration 1500 / 2000: training loss 0.717869 val loss: 0.723475\n",
      "iteration 1550 / 2000: training loss 0.750757 val loss: 0.721125\n",
      "iteration 1600 / 2000: training loss 0.747607 val loss: 0.720099\n",
      "iteration 1650 / 2000: training loss 0.714566 val loss: 0.721814\n",
      "iteration 1700 / 2000: training loss 0.682832 val loss: 0.719602\n",
      "hidden_size: 150 lr: 3.000000e-01 lr_decay: 9.000000e-01 reg: 2.000000e-02 train accuracy: 0.890000 val accuracy: 0.927000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302636\n",
      "iteration 50 / 2000: training loss 0.996055 val loss: 1.050438\n",
      "iteration 100 / 2000: training loss 0.830654 val loss: 0.834251\n",
      "iteration 150 / 2000: training loss 0.919796 val loss: 0.870603\n",
      "iteration 200 / 2000: training loss 0.789672 val loss: 0.854752\n",
      "iteration 250 / 2000: training loss 0.772237 val loss: 0.843207\n",
      "iteration 300 / 2000: training loss 0.738324 val loss: 0.797449\n",
      "iteration 350 / 2000: training loss 0.758046 val loss: 0.806541\n",
      "iteration 400 / 2000: training loss 0.788985 val loss: 0.754993\n",
      "iteration 450 / 2000: training loss 0.838183 val loss: 0.799670\n",
      "iteration 500 / 2000: training loss 0.767331 val loss: 0.765306\n",
      "iteration 550 / 2000: training loss 0.731135 val loss: 0.742696\n",
      "iteration 600 / 2000: training loss 0.656016 val loss: 0.744786\n",
      "iteration 650 / 2000: training loss 0.649906 val loss: 0.751682\n",
      "iteration 700 / 2000: training loss 0.756202 val loss: 0.728066\n",
      "iteration 750 / 2000: training loss 0.721009 val loss: 0.739165\n",
      "iteration 800 / 2000: training loss 0.718895 val loss: 0.744597\n",
      "iteration 850 / 2000: training loss 0.722059 val loss: 0.735907\n",
      "iteration 900 / 2000: training loss 0.700866 val loss: 0.737226\n",
      "iteration 950 / 2000: training loss 0.681585 val loss: 0.725224\n",
      "iteration 1000 / 2000: training loss 0.696894 val loss: 0.721496\n",
      "iteration 1050 / 2000: training loss 0.695178 val loss: 0.719741\n",
      "iteration 1100 / 2000: training loss 0.717407 val loss: 0.722411\n",
      "iteration 1150 / 2000: training loss 0.669424 val loss: 0.724617\n",
      "iteration 1200 / 2000: training loss 0.748180 val loss: 0.727902\n",
      "iteration 1250 / 2000: training loss 0.765100 val loss: 0.722908\n",
      "iteration 1300 / 2000: training loss 0.720127 val loss: 0.720814\n",
      "iteration 1350 / 2000: training loss 0.773857 val loss: 0.728823\n",
      "iteration 1400 / 2000: training loss 0.678618 val loss: 0.713895\n",
      "iteration 1450 / 2000: training loss 0.756864 val loss: 0.719055\n",
      "iteration 1500 / 2000: training loss 0.689768 val loss: 0.714867\n",
      "iteration 1550 / 2000: training loss 0.747629 val loss: 0.717050\n",
      "iteration 1600 / 2000: training loss 0.731462 val loss: 0.711609\n",
      "iteration 1650 / 2000: training loss 0.715400 val loss: 0.708661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1700 / 2000: training loss 0.665058 val loss: 0.709038\n",
      "iteration 1750 / 2000: training loss 0.635892 val loss: 0.710573\n",
      "iteration 1800 / 2000: training loss 0.747400 val loss: 0.709853\n",
      "iteration 1850 / 2000: training loss 0.709163 val loss: 0.709084\n",
      "iteration 1900 / 2000: training loss 0.620237 val loss: 0.709149\n",
      "iteration 1950 / 2000: training loss 0.708120 val loss: 0.712903\n",
      "hidden_size: 150 lr: 3.000000e-01 lr_decay: 8.500000e-01 reg: 2.000000e-02 train accuracy: 0.930000 val accuracy: 0.933000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302636\n",
      "iteration 50 / 2000: training loss 1.029107 val loss: 1.090160\n",
      "iteration 100 / 2000: training loss 0.809212 val loss: 0.811651\n",
      "iteration 150 / 2000: training loss 0.858291 val loss: 0.856663\n",
      "iteration 200 / 2000: training loss 0.764465 val loss: 0.828571\n",
      "iteration 250 / 2000: training loss 0.741934 val loss: 0.823644\n",
      "iteration 300 / 2000: training loss 0.694650 val loss: 0.761092\n",
      "iteration 350 / 2000: training loss 0.765855 val loss: 0.789881\n",
      "iteration 400 / 2000: training loss 0.723660 val loss: 0.727424\n",
      "iteration 450 / 2000: training loss 0.806385 val loss: 0.778339\n",
      "hidden_size: 150 lr: 3.000000e-01 lr_decay: 8.000000e-01 reg: 2.000000e-02 train accuracy: 0.900000 val accuracy: 0.914000\n",
      "iteration 0 / 2000: training loss 2.302593 val loss: 2.302640\n",
      "iteration 50 / 2000: training loss 1.283220 val loss: 1.430755\n",
      "iteration 100 / 2000: training loss 1.171147 val loss: 1.302598\n",
      "iteration 150 / 2000: training loss 1.375172 val loss: 1.360913\n",
      "iteration 200 / 2000: training loss 1.139669 val loss: 1.142032\n",
      "iteration 250 / 2000: training loss 1.256821 val loss: 1.192351\n",
      "iteration 300 / 2000: training loss 1.101271 val loss: 1.288929\n",
      "iteration 350 / 2000: training loss 1.286095 val loss: 1.233895\n",
      "iteration 400 / 2000: training loss 1.276459 val loss: 1.143502\n",
      "iteration 450 / 2000: training loss 1.226621 val loss: 1.169326\n",
      "iteration 500 / 2000: training loss 1.093760 val loss: 1.176375\n",
      "iteration 550 / 2000: training loss 1.097429 val loss: 1.129645\n",
      "iteration 600 / 2000: training loss 1.099388 val loss: 1.173604\n",
      "iteration 650 / 2000: training loss 1.152303 val loss: 1.146782\n",
      "iteration 700 / 2000: training loss 1.244025 val loss: 1.217178\n",
      "iteration 750 / 2000: training loss 1.151188 val loss: 1.153389\n",
      "iteration 800 / 2000: training loss 1.089978 val loss: 1.147799\n",
      "iteration 850 / 2000: training loss 1.181024 val loss: 1.150447\n",
      "iteration 900 / 2000: training loss 1.113032 val loss: 1.167050\n",
      "iteration 950 / 2000: training loss 1.064295 val loss: 1.211245\n",
      "iteration 1000 / 2000: training loss 1.202574 val loss: 1.344811\n",
      "iteration 1050 / 2000: training loss 1.133216 val loss: 1.145565\n",
      "iteration 1100 / 2000: training loss 1.064803 val loss: 1.106740\n",
      "iteration 1150 / 2000: training loss 1.124758 val loss: 1.168151\n",
      "iteration 1200 / 2000: training loss 1.172190 val loss: 1.126395\n",
      "iteration 1250 / 2000: training loss 1.179238 val loss: 1.098616\n",
      "iteration 1300 / 2000: training loss 1.231658 val loss: 1.103228\n",
      "iteration 1350 / 2000: training loss 1.194109 val loss: 1.184994\n",
      "iteration 1400 / 2000: training loss 1.116981 val loss: 1.175981\n",
      "iteration 1450 / 2000: training loss 1.251603 val loss: 1.150517\n",
      "iteration 1500 / 2000: training loss 1.114654 val loss: 1.161244\n",
      "iteration 1550 / 2000: training loss 1.245655 val loss: 1.106477\n",
      "iteration 1600 / 2000: training loss 1.191174 val loss: 1.159835\n",
      "iteration 1650 / 2000: training loss 1.101766 val loss: 1.106946\n",
      "iteration 1700 / 2000: training loss 1.019968 val loss: 1.119179\n",
      "iteration 1750 / 2000: training loss 1.116824 val loss: 1.144775\n",
      "iteration 1800 / 2000: training loss 1.144271 val loss: 1.109322\n",
      "iteration 1850 / 2000: training loss 1.110049 val loss: 1.118888\n",
      "iteration 1900 / 2000: training loss 1.007972 val loss: 1.122055\n",
      "iteration 1950 / 2000: training loss 1.085526 val loss: 1.121953\n",
      "hidden_size: 150 lr: 3.000000e-01 lr_decay: 9.800000e-01 reg: 4.000000e-02 train accuracy: 0.900000 val accuracy: 0.890000\n",
      "iteration 0 / 2000: training loss 2.302593 val loss: 2.302640\n",
      "iteration 50 / 2000: training loss 1.254579 val loss: 1.336780\n",
      "iteration 100 / 2000: training loss 1.197037 val loss: 1.231065\n",
      "iteration 150 / 2000: training loss 1.256534 val loss: 1.263791\n",
      "iteration 200 / 2000: training loss 1.101351 val loss: 1.121876\n",
      "iteration 250 / 2000: training loss 1.188585 val loss: 1.142603\n",
      "iteration 300 / 2000: training loss 1.048604 val loss: 1.172179\n",
      "iteration 350 / 2000: training loss 1.145512 val loss: 1.122406\n",
      "iteration 400 / 2000: training loss 1.157715 val loss: 1.111890\n",
      "iteration 450 / 2000: training loss 1.140196 val loss: 1.119766\n",
      "iteration 500 / 2000: training loss 1.079039 val loss: 1.109599\n",
      "hidden_size: 150 lr: 3.000000e-01 lr_decay: 9.000000e-01 reg: 4.000000e-02 train accuracy: 0.890000 val accuracy: 0.837000\n",
      "iteration 0 / 2000: training loss 2.302593 val loss: 2.302640\n",
      "iteration 50 / 2000: training loss 1.318061 val loss: 1.315827\n",
      "iteration 100 / 2000: training loss 1.200003 val loss: 1.180326\n",
      "iteration 150 / 2000: training loss 1.216519 val loss: 1.248422\n",
      "iteration 200 / 2000: training loss 1.102294 val loss: 1.150515\n",
      "iteration 250 / 2000: training loss 1.139776 val loss: 1.101992\n",
      "iteration 300 / 2000: training loss 1.050038 val loss: 1.101585\n",
      "iteration 350 / 2000: training loss 1.112408 val loss: 1.178771\n",
      "iteration 400 / 2000: training loss 1.122559 val loss: 1.085293\n",
      "iteration 450 / 2000: training loss 1.158800 val loss: 1.101761\n",
      "iteration 500 / 2000: training loss 1.054008 val loss: 1.064729\n",
      "hidden_size: 150 lr: 3.000000e-01 lr_decay: 8.500000e-01 reg: 4.000000e-02 train accuracy: 0.930000 val accuracy: 0.855000\n",
      "iteration 0 / 2000: training loss 2.302593 val loss: 2.302640\n",
      "iteration 50 / 2000: training loss 1.385616 val loss: 1.370107\n",
      "iteration 100 / 2000: training loss 1.150700 val loss: 1.140285\n",
      "iteration 150 / 2000: training loss 1.163618 val loss: 1.190947\n",
      "iteration 200 / 2000: training loss 1.087961 val loss: 1.153674\n",
      "iteration 250 / 2000: training loss 1.095298 val loss: 1.112002\n",
      "iteration 300 / 2000: training loss 1.034536 val loss: 1.083019\n",
      "iteration 350 / 2000: training loss 1.056580 val loss: 1.110564\n",
      "iteration 400 / 2000: training loss 1.111614 val loss: 1.054397\n",
      "iteration 450 / 2000: training loss 1.144198 val loss: 1.097066\n",
      "iteration 500 / 2000: training loss 1.039931 val loss: 1.048371\n",
      "iteration 550 / 2000: training loss 1.015167 val loss: 1.056319\n",
      "iteration 600 / 2000: training loss 0.973725 val loss: 1.051794\n",
      "iteration 650 / 2000: training loss 0.948737 val loss: 1.039544\n",
      "iteration 700 / 2000: training loss 1.043923 val loss: 1.037962\n",
      "iteration 750 / 2000: training loss 1.033618 val loss: 1.034951\n",
      "iteration 800 / 2000: training loss 1.021769 val loss: 1.054208\n",
      "iteration 850 / 2000: training loss 1.065860 val loss: 1.026541\n",
      "iteration 900 / 2000: training loss 0.998378 val loss: 1.032937\n",
      "iteration 950 / 2000: training loss 0.985955 val loss: 1.022163\n",
      "iteration 1000 / 2000: training loss 1.014085 val loss: 1.019704\n",
      "iteration 1050 / 2000: training loss 1.012565 val loss: 1.020924\n",
      "iteration 1100 / 2000: training loss 1.003911 val loss: 1.025193\n",
      "iteration 1150 / 2000: training loss 0.973331 val loss: 1.020078\n",
      "iteration 1200 / 2000: training loss 1.052683 val loss: 1.021505\n",
      "iteration 1250 / 2000: training loss 1.062861 val loss: 1.024801\n",
      "iteration 1300 / 2000: training loss 1.027784 val loss: 1.023007\n",
      "iteration 1350 / 2000: training loss 1.070595 val loss: 1.022291\n",
      "iteration 1400 / 2000: training loss 1.014503 val loss: 1.020863\n",
      "iteration 1450 / 2000: training loss 1.058471 val loss: 1.024932\n",
      "iteration 1500 / 2000: training loss 1.000217 val loss: 1.017976\n",
      "iteration 1550 / 2000: training loss 1.019259 val loss: 1.018966\n",
      "iteration 1600 / 2000: training loss 1.051789 val loss: 1.016764\n",
      "iteration 1650 / 2000: training loss 1.020285 val loss: 1.014922\n",
      "iteration 1700 / 2000: training loss 0.976318 val loss: 1.017219\n",
      "iteration 1750 / 2000: training loss 0.932184 val loss: 1.015425\n",
      "iteration 1800 / 2000: training loss 1.063550 val loss: 1.014531\n",
      "iteration 1850 / 2000: training loss 1.021136 val loss: 1.015357\n",
      "iteration 1900 / 2000: training loss 0.940271 val loss: 1.015224\n",
      "iteration 1950 / 2000: training loss 1.018154 val loss: 1.014735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_size: 150 lr: 3.000000e-01 lr_decay: 8.000000e-01 reg: 4.000000e-02 train accuracy: 0.910000 val accuracy: 0.923000\n",
      "iteration 0 / 2000: training loss 2.302597 val loss: 2.302644\n",
      "iteration 50 / 2000: training loss 1.461001 val loss: 1.583583\n",
      "iteration 100 / 2000: training loss 1.494095 val loss: 1.583693\n",
      "iteration 150 / 2000: training loss 1.580091 val loss: 1.547150\n",
      "iteration 200 / 2000: training loss 1.413013 val loss: 1.410071\n",
      "iteration 250 / 2000: training loss 1.537259 val loss: 1.414403\n",
      "iteration 300 / 2000: training loss 1.433772 val loss: 1.528149\n",
      "iteration 350 / 2000: training loss 1.614056 val loss: 1.415923\n",
      "iteration 400 / 2000: training loss 1.519056 val loss: 1.400081\n",
      "iteration 450 / 2000: training loss 1.442962 val loss: 1.405982\n",
      "iteration 500 / 2000: training loss 1.329266 val loss: 1.369001\n",
      "iteration 550 / 2000: training loss 1.365625 val loss: 1.401415\n",
      "iteration 600 / 2000: training loss 1.393286 val loss: 1.444842\n",
      "iteration 650 / 2000: training loss 1.346636 val loss: 1.402648\n",
      "iteration 700 / 2000: training loss 1.446781 val loss: 1.478259\n",
      "iteration 750 / 2000: training loss 1.422667 val loss: 1.419400\n",
      "iteration 800 / 2000: training loss 1.341543 val loss: 1.377714\n",
      "iteration 850 / 2000: training loss 1.452163 val loss: 1.394004\n",
      "iteration 900 / 2000: training loss 1.426376 val loss: 1.420784\n",
      "iteration 950 / 2000: training loss 1.343383 val loss: 1.523119\n",
      "iteration 1000 / 2000: training loss 1.463691 val loss: 1.507397\n",
      "iteration 1050 / 2000: training loss 1.417384 val loss: 1.431349\n",
      "iteration 1100 / 2000: training loss 1.285318 val loss: 1.339348\n",
      "iteration 1150 / 2000: training loss 1.416996 val loss: 1.408351\n",
      "iteration 1200 / 2000: training loss 1.447983 val loss: 1.362480\n",
      "iteration 1250 / 2000: training loss 1.408098 val loss: 1.363852\n",
      "hidden_size: 150 lr: 3.000000e-01 lr_decay: 9.800000e-01 reg: 6.000000e-02 train accuracy: 0.930000 val accuracy: 0.829000\n",
      "iteration 0 / 2000: training loss 2.302597 val loss: 2.302644\n",
      "iteration 50 / 2000: training loss 1.508898 val loss: 1.500065\n",
      "iteration 100 / 2000: training loss 1.398880 val loss: 1.502202\n",
      "iteration 150 / 2000: training loss 1.483313 val loss: 1.504917\n",
      "iteration 200 / 2000: training loss 1.349222 val loss: 1.347553\n",
      "iteration 250 / 2000: training loss 1.430137 val loss: 1.381286\n",
      "iteration 300 / 2000: training loss 1.363587 val loss: 1.435421\n",
      "iteration 350 / 2000: training loss 1.432615 val loss: 1.448239\n",
      "iteration 400 / 2000: training loss 1.447164 val loss: 1.361271\n",
      "iteration 450 / 2000: training loss 1.413681 val loss: 1.366749\n",
      "iteration 500 / 2000: training loss 1.300440 val loss: 1.341910\n",
      "iteration 550 / 2000: training loss 1.284679 val loss: 1.348080\n",
      "iteration 600 / 2000: training loss 1.322429 val loss: 1.368707\n",
      "iteration 650 / 2000: training loss 1.221537 val loss: 1.290090\n",
      "iteration 700 / 2000: training loss 1.330525 val loss: 1.359107\n",
      "iteration 750 / 2000: training loss 1.369191 val loss: 1.311648\n",
      "iteration 800 / 2000: training loss 1.304482 val loss: 1.304964\n",
      "iteration 850 / 2000: training loss 1.359532 val loss: 1.316118\n",
      "iteration 900 / 2000: training loss 1.350076 val loss: 1.324079\n",
      "iteration 950 / 2000: training loss 1.269168 val loss: 1.316921\n",
      "iteration 1000 / 2000: training loss 1.306222 val loss: 1.349252\n",
      "iteration 1050 / 2000: training loss 1.283521 val loss: 1.293969\n",
      "iteration 1100 / 2000: training loss 1.255052 val loss: 1.286290\n",
      "iteration 1150 / 2000: training loss 1.236072 val loss: 1.302869\n",
      "iteration 1200 / 2000: training loss 1.280030 val loss: 1.282097\n",
      "iteration 1250 / 2000: training loss 1.317595 val loss: 1.287044\n",
      "iteration 1300 / 2000: training loss 1.278278 val loss: 1.293445\n",
      "iteration 1350 / 2000: training loss 1.352735 val loss: 1.283064\n",
      "iteration 1400 / 2000: training loss 1.277600 val loss: 1.284019\n",
      "iteration 1450 / 2000: training loss 1.336337 val loss: 1.277963\n",
      "iteration 1500 / 2000: training loss 1.275020 val loss: 1.272527\n",
      "iteration 1550 / 2000: training loss 1.291542 val loss: 1.294839\n",
      "iteration 1600 / 2000: training loss 1.305236 val loss: 1.284743\n",
      "iteration 1650 / 2000: training loss 1.269816 val loss: 1.284573\n",
      "iteration 1700 / 2000: training loss 1.232373 val loss: 1.272129\n",
      "iteration 1750 / 2000: training loss 1.174785 val loss: 1.265559\n",
      "iteration 1800 / 2000: training loss 1.321403 val loss: 1.285440\n",
      "hidden_size: 150 lr: 3.000000e-01 lr_decay: 9.000000e-01 reg: 6.000000e-02 train accuracy: 0.880000 val accuracy: 0.882000\n",
      "iteration 0 / 2000: training loss 2.302597 val loss: 2.302644\n",
      "iteration 50 / 2000: training loss 1.569940 val loss: 1.545047\n",
      "iteration 100 / 2000: training loss 1.405441 val loss: 1.426154\n",
      "iteration 150 / 2000: training loss 1.448357 val loss: 1.504207\n",
      "iteration 200 / 2000: training loss 1.350910 val loss: 1.391628\n",
      "iteration 250 / 2000: training loss 1.426655 val loss: 1.356864\n",
      "iteration 300 / 2000: training loss 1.288428 val loss: 1.362226\n",
      "iteration 350 / 2000: training loss 1.382808 val loss: 1.414175\n",
      "iteration 400 / 2000: training loss 1.359467 val loss: 1.327316\n",
      "iteration 450 / 2000: training loss 1.404997 val loss: 1.336301\n",
      "hidden_size: 150 lr: 3.000000e-01 lr_decay: 8.500000e-01 reg: 6.000000e-02 train accuracy: 0.880000 val accuracy: 0.866000\n",
      "iteration 0 / 2000: training loss 2.302597 val loss: 2.302644\n",
      "iteration 50 / 2000: training loss 1.595435 val loss: 1.594627\n",
      "iteration 100 / 2000: training loss 1.405878 val loss: 1.397231\n",
      "iteration 150 / 2000: training loss 1.408920 val loss: 1.463890\n",
      "iteration 200 / 2000: training loss 1.332985 val loss: 1.404624\n",
      "iteration 250 / 2000: training loss 1.330016 val loss: 1.317110\n",
      "iteration 300 / 2000: training loss 1.278584 val loss: 1.335793\n",
      "iteration 350 / 2000: training loss 1.295901 val loss: 1.362821\n",
      "iteration 400 / 2000: training loss 1.381759 val loss: 1.289337\n",
      "iteration 450 / 2000: training loss 1.385696 val loss: 1.330629\n",
      "iteration 500 / 2000: training loss 1.279092 val loss: 1.291933\n",
      "iteration 550 / 2000: training loss 1.260931 val loss: 1.299415\n",
      "iteration 600 / 2000: training loss 1.230171 val loss: 1.295436\n",
      "iteration 650 / 2000: training loss 1.203940 val loss: 1.291819\n",
      "iteration 700 / 2000: training loss 1.269797 val loss: 1.274413\n",
      "iteration 750 / 2000: training loss 1.272230 val loss: 1.274456\n",
      "iteration 800 / 2000: training loss 1.254992 val loss: 1.292263\n",
      "iteration 850 / 2000: training loss 1.315562 val loss: 1.265140\n",
      "iteration 900 / 2000: training loss 1.228977 val loss: 1.272851\n",
      "iteration 950 / 2000: training loss 1.226764 val loss: 1.260298\n",
      "iteration 1000 / 2000: training loss 1.252575 val loss: 1.258274\n",
      "iteration 1050 / 2000: training loss 1.254797 val loss: 1.257805\n",
      "iteration 1100 / 2000: training loss 1.240344 val loss: 1.262336\n",
      "iteration 1150 / 2000: training loss 1.210041 val loss: 1.258845\n",
      "iteration 1200 / 2000: training loss 1.294804 val loss: 1.260374\n",
      "iteration 1250 / 2000: training loss 1.302915 val loss: 1.262347\n",
      "iteration 1300 / 2000: training loss 1.261796 val loss: 1.261034\n",
      "iteration 1350 / 2000: training loss 1.313643 val loss: 1.259682\n",
      "iteration 1400 / 2000: training loss 1.270341 val loss: 1.258081\n",
      "iteration 1450 / 2000: training loss 1.300268 val loss: 1.261327\n",
      "iteration 1500 / 2000: training loss 1.240977 val loss: 1.255292\n",
      "iteration 1550 / 2000: training loss 1.238636 val loss: 1.255389\n",
      "iteration 1600 / 2000: training loss 1.289387 val loss: 1.253484\n",
      "iteration 1650 / 2000: training loss 1.257422 val loss: 1.251893\n",
      "iteration 1700 / 2000: training loss 1.217768 val loss: 1.253710\n",
      "iteration 1750 / 2000: training loss 1.164053 val loss: 1.252259\n",
      "iteration 1800 / 2000: training loss 1.298487 val loss: 1.251397\n",
      "iteration 1850 / 2000: training loss 1.265597 val loss: 1.252134\n",
      "iteration 1900 / 2000: training loss 1.181596 val loss: 1.252120\n",
      "iteration 1950 / 2000: training loss 1.252833 val loss: 1.251366\n",
      "hidden_size: 150 lr: 3.000000e-01 lr_decay: 8.000000e-01 reg: 6.000000e-02 train accuracy: 0.890000 val accuracy: 0.913000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302719\n",
      "iteration 50 / 2000: training loss 1.056022 val loss: 1.213561\n",
      "iteration 100 / 2000: training loss 0.828897 val loss: 0.868345\n",
      "iteration 150 / 2000: training loss 1.415438 val loss: 0.849642\n",
      "iteration 200 / 2000: training loss 0.808116 val loss: 0.878855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 250 / 2000: training loss 0.747730 val loss: 0.813245\n",
      "iteration 300 / 2000: training loss 0.754513 val loss: 0.818482\n",
      "iteration 350 / 2000: training loss 0.767115 val loss: 0.850567\n",
      "iteration 400 / 2000: training loss 0.724598 val loss: 0.707727\n",
      "iteration 450 / 2000: training loss 0.745884 val loss: 0.797916\n",
      "hidden_size: 150 lr: 5.000000e-01 lr_decay: 9.800000e-01 reg: 1.000000e-02 train accuracy: 0.920000 val accuracy: 0.870000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302719\n",
      "iteration 50 / 2000: training loss 1.130972 val loss: 1.154833\n",
      "iteration 100 / 2000: training loss 0.783773 val loss: 0.815873\n",
      "iteration 150 / 2000: training loss 0.844277 val loss: 0.802700\n",
      "iteration 200 / 2000: training loss 0.628852 val loss: 0.665377\n",
      "iteration 250 / 2000: training loss 0.705769 val loss: 0.689409\n",
      "iteration 300 / 2000: training loss 0.612582 val loss: 0.710719\n",
      "iteration 350 / 2000: training loss 0.623624 val loss: 0.694756\n",
      "iteration 400 / 2000: training loss 0.564400 val loss: 0.588254\n",
      "iteration 450 / 2000: training loss 0.630893 val loss: 0.580659\n",
      "iteration 500 / 2000: training loss 0.553609 val loss: 0.607343\n",
      "iteration 550 / 2000: training loss 0.608577 val loss: 0.632462\n",
      "iteration 600 / 2000: training loss 0.480625 val loss: 0.610708\n",
      "iteration 650 / 2000: training loss 0.468800 val loss: 0.546411\n",
      "iteration 700 / 2000: training loss 0.595307 val loss: 0.580008\n",
      "iteration 750 / 2000: training loss 0.588409 val loss: 0.532674\n",
      "iteration 800 / 2000: training loss 0.588477 val loss: 0.561988\n",
      "iteration 850 / 2000: training loss 0.578550 val loss: 0.576671\n",
      "iteration 900 / 2000: training loss 0.516455 val loss: 0.543872\n",
      "iteration 950 / 2000: training loss 0.502319 val loss: 0.560849\n",
      "iteration 1000 / 2000: training loss 0.497506 val loss: 0.546878\n",
      "iteration 1050 / 2000: training loss 0.524293 val loss: 0.576664\n",
      "hidden_size: 150 lr: 5.000000e-01 lr_decay: 9.000000e-01 reg: 1.000000e-02 train accuracy: 0.940000 val accuracy: 0.927000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302719\n",
      "iteration 50 / 2000: training loss 0.952722 val loss: 1.018202\n",
      "iteration 100 / 2000: training loss 0.666825 val loss: 0.685590\n",
      "iteration 150 / 2000: training loss 0.852460 val loss: 0.720857\n",
      "iteration 200 / 2000: training loss 0.659342 val loss: 0.687217\n",
      "iteration 250 / 2000: training loss 0.631680 val loss: 0.651058\n",
      "iteration 300 / 2000: training loss 0.661004 val loss: 0.657406\n",
      "iteration 350 / 2000: training loss 0.571383 val loss: 0.622916\n",
      "iteration 400 / 2000: training loss 0.577352 val loss: 0.563021\n",
      "iteration 450 / 2000: training loss 0.608954 val loss: 0.593416\n",
      "iteration 500 / 2000: training loss 0.553501 val loss: 0.571283\n",
      "iteration 550 / 2000: training loss 0.522519 val loss: 0.534656\n",
      "iteration 600 / 2000: training loss 0.450568 val loss: 0.548727\n",
      "iteration 650 / 2000: training loss 0.453912 val loss: 0.551882\n",
      "iteration 700 / 2000: training loss 0.535906 val loss: 0.516979\n",
      "iteration 750 / 2000: training loss 0.498902 val loss: 0.531715\n",
      "iteration 800 / 2000: training loss 0.537636 val loss: 0.539293\n",
      "iteration 850 / 2000: training loss 0.502281 val loss: 0.538256\n",
      "iteration 900 / 2000: training loss 0.483138 val loss: 0.533915\n",
      "iteration 950 / 2000: training loss 0.489056 val loss: 0.519190\n",
      "iteration 1000 / 2000: training loss 0.484697 val loss: 0.515786\n",
      "iteration 1050 / 2000: training loss 0.475927 val loss: 0.521335\n",
      "iteration 1100 / 2000: training loss 0.519674 val loss: 0.516998\n",
      "iteration 1150 / 2000: training loss 0.444314 val loss: 0.514374\n",
      "iteration 1200 / 2000: training loss 0.518500 val loss: 0.521364\n",
      "iteration 1250 / 2000: training loss 0.548297 val loss: 0.515328\n",
      "iteration 1300 / 2000: training loss 0.490923 val loss: 0.512440\n",
      "iteration 1350 / 2000: training loss 0.557811 val loss: 0.521290\n",
      "iteration 1400 / 2000: training loss 0.445800 val loss: 0.502326\n",
      "iteration 1450 / 2000: training loss 0.521281 val loss: 0.508088\n",
      "iteration 1500 / 2000: training loss 0.481801 val loss: 0.502648\n",
      "iteration 1550 / 2000: training loss 0.556386 val loss: 0.504020\n",
      "iteration 1600 / 2000: training loss 0.501135 val loss: 0.498345\n",
      "iteration 1650 / 2000: training loss 0.505590 val loss: 0.497012\n",
      "iteration 1700 / 2000: training loss 0.453695 val loss: 0.495566\n",
      "iteration 1750 / 2000: training loss 0.421321 val loss: 0.496208\n",
      "iteration 1800 / 2000: training loss 0.519880 val loss: 0.498049\n",
      "hidden_size: 150 lr: 5.000000e-01 lr_decay: 8.500000e-01 reg: 1.000000e-02 train accuracy: 0.960000 val accuracy: 0.944000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302719\n",
      "iteration 50 / 2000: training loss 0.948958 val loss: 1.075560\n",
      "iteration 100 / 2000: training loss 0.662286 val loss: 0.710922\n",
      "iteration 150 / 2000: training loss 0.794140 val loss: 0.682848\n",
      "iteration 200 / 2000: training loss 0.614309 val loss: 0.638497\n",
      "iteration 250 / 2000: training loss 0.609663 val loss: 0.647918\n",
      "iteration 300 / 2000: training loss 0.546199 val loss: 0.603322\n",
      "iteration 350 / 2000: training loss 0.558730 val loss: 0.572549\n",
      "iteration 400 / 2000: training loss 0.520757 val loss: 0.527593\n",
      "iteration 450 / 2000: training loss 0.595097 val loss: 0.591157\n",
      "hidden_size: 150 lr: 5.000000e-01 lr_decay: 8.000000e-01 reg: 1.000000e-02 train accuracy: 0.930000 val accuracy: 0.919000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302721\n",
      "iteration 50 / 2000: training loss 1.377037 val loss: 1.328429\n",
      "iteration 100 / 2000: training loss 0.966463 val loss: 1.148495\n",
      "iteration 150 / 2000: training loss 1.313526 val loss: 1.168041\n",
      "iteration 200 / 2000: training loss 0.894840 val loss: 1.016047\n",
      "iteration 250 / 2000: training loss 1.173151 val loss: 1.111197\n",
      "iteration 300 / 2000: training loss 1.034872 val loss: 1.261589\n",
      "iteration 350 / 2000: training loss 0.891011 val loss: 0.926010\n",
      "iteration 400 / 2000: training loss 0.996340 val loss: 0.906157\n",
      "iteration 450 / 2000: training loss 1.000122 val loss: 0.921495\n",
      "iteration 500 / 2000: training loss 1.245800 val loss: 1.254699\n",
      "iteration 550 / 2000: training loss 1.026069 val loss: 1.062687\n",
      "iteration 600 / 2000: training loss 0.901149 val loss: 1.042469\n",
      "iteration 650 / 2000: training loss 0.855816 val loss: 1.056211\n",
      "iteration 700 / 2000: training loss 1.135529 val loss: 1.131554\n",
      "iteration 750 / 2000: training loss 0.994707 val loss: 0.893441\n",
      "iteration 800 / 2000: training loss 0.903265 val loss: 0.966522\n",
      "iteration 850 / 2000: training loss 1.008019 val loss: 0.934318\n",
      "iteration 900 / 2000: training loss 0.934192 val loss: 1.045845\n",
      "iteration 950 / 2000: training loss 0.858165 val loss: 1.014025\n",
      "iteration 1000 / 2000: training loss 0.992777 val loss: 0.950707\n",
      "iteration 1050 / 2000: training loss 0.904831 val loss: 1.040655\n",
      "iteration 1100 / 2000: training loss 0.933328 val loss: 0.875981\n",
      "iteration 1150 / 2000: training loss 0.941499 val loss: 0.971186\n",
      "iteration 1200 / 2000: training loss 0.878378 val loss: 0.939528\n",
      "iteration 1250 / 2000: training loss 1.023665 val loss: 0.862484\n",
      "iteration 1300 / 2000: training loss 0.914955 val loss: 1.045190\n",
      "iteration 1350 / 2000: training loss 1.028210 val loss: 0.980733\n",
      "hidden_size: 150 lr: 5.000000e-01 lr_decay: 9.800000e-01 reg: 2.000000e-02 train accuracy: 0.850000 val accuracy: 0.850000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302721\n",
      "iteration 50 / 2000: training loss 1.200826 val loss: 1.293138\n",
      "iteration 100 / 2000: training loss 0.893859 val loss: 0.939005\n",
      "iteration 150 / 2000: training loss 1.246117 val loss: 1.170991\n",
      "iteration 200 / 2000: training loss 0.977683 val loss: 0.971928\n",
      "iteration 250 / 2000: training loss 0.982919 val loss: 1.046381\n",
      "iteration 300 / 2000: training loss 0.835042 val loss: 1.011838\n",
      "iteration 350 / 2000: training loss 0.893854 val loss: 0.882236\n",
      "iteration 400 / 2000: training loss 0.862695 val loss: 0.854981\n",
      "iteration 450 / 2000: training loss 0.883623 val loss: 0.882679\n",
      "iteration 500 / 2000: training loss 0.785534 val loss: 0.860784\n",
      "iteration 550 / 2000: training loss 0.826660 val loss: 0.832507\n",
      "iteration 600 / 2000: training loss 0.777746 val loss: 0.875956\n",
      "iteration 650 / 2000: training loss 0.680947 val loss: 0.780565\n",
      "iteration 700 / 2000: training loss 0.884053 val loss: 0.850083\n",
      "iteration 750 / 2000: training loss 0.768911 val loss: 0.774297\n",
      "iteration 800 / 2000: training loss 0.779207 val loss: 0.790238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 850 / 2000: training loss 0.833175 val loss: 0.768235\n",
      "iteration 900 / 2000: training loss 0.788073 val loss: 0.770008\n",
      "iteration 950 / 2000: training loss 0.749270 val loss: 0.783830\n",
      "iteration 1000 / 2000: training loss 0.736371 val loss: 0.762212\n",
      "iteration 1050 / 2000: training loss 0.710460 val loss: 0.758745\n",
      "iteration 1100 / 2000: training loss 0.745421 val loss: 0.760832\n",
      "iteration 1150 / 2000: training loss 0.694721 val loss: 0.759523\n",
      "iteration 1200 / 2000: training loss 0.753427 val loss: 0.746071\n",
      "iteration 1250 / 2000: training loss 0.796995 val loss: 0.754729\n",
      "iteration 1300 / 2000: training loss 0.764147 val loss: 0.754391\n",
      "iteration 1350 / 2000: training loss 0.786786 val loss: 0.743977\n",
      "iteration 1400 / 2000: training loss 0.687611 val loss: 0.737270\n",
      "iteration 1450 / 2000: training loss 0.777812 val loss: 0.752063\n",
      "iteration 1500 / 2000: training loss 0.726784 val loss: 0.727654\n",
      "iteration 1550 / 2000: training loss 0.760903 val loss: 0.735839\n",
      "iteration 1600 / 2000: training loss 0.747500 val loss: 0.738469\n",
      "iteration 1650 / 2000: training loss 0.722780 val loss: 0.724805\n",
      "iteration 1700 / 2000: training loss 0.671569 val loss: 0.724954\n",
      "hidden_size: 150 lr: 5.000000e-01 lr_decay: 9.000000e-01 reg: 2.000000e-02 train accuracy: 0.920000 val accuracy: 0.922000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302721\n",
      "iteration 50 / 2000: training loss 1.152948 val loss: 1.197947\n",
      "iteration 100 / 2000: training loss 0.872113 val loss: 0.971540\n",
      "iteration 150 / 2000: training loss 1.173670 val loss: 1.068438\n",
      "iteration 200 / 2000: training loss 0.825989 val loss: 0.855686\n",
      "iteration 250 / 2000: training loss 0.854244 val loss: 0.915128\n",
      "iteration 300 / 2000: training loss 0.770231 val loss: 0.872719\n",
      "iteration 350 / 2000: training loss 0.877206 val loss: 0.856790\n",
      "iteration 400 / 2000: training loss 0.797012 val loss: 0.789003\n",
      "iteration 450 / 2000: training loss 0.831348 val loss: 0.795426\n",
      "iteration 500 / 2000: training loss 0.753745 val loss: 0.779822\n",
      "iteration 550 / 2000: training loss 0.730722 val loss: 0.798636\n",
      "iteration 600 / 2000: training loss 0.682623 val loss: 0.818007\n",
      "iteration 650 / 2000: training loss 0.674806 val loss: 0.751608\n",
      "iteration 700 / 2000: training loss 0.753811 val loss: 0.752406\n",
      "iteration 750 / 2000: training loss 0.756795 val loss: 0.735296\n",
      "iteration 800 / 2000: training loss 0.783633 val loss: 0.743070\n",
      "iteration 850 / 2000: training loss 0.746425 val loss: 0.774834\n",
      "iteration 900 / 2000: training loss 0.738852 val loss: 0.744312\n",
      "iteration 950 / 2000: training loss 0.709737 val loss: 0.737674\n",
      "iteration 1000 / 2000: training loss 0.699203 val loss: 0.732391\n",
      "iteration 1050 / 2000: training loss 0.701304 val loss: 0.743435\n",
      "hidden_size: 150 lr: 5.000000e-01 lr_decay: 8.500000e-01 reg: 2.000000e-02 train accuracy: 0.890000 val accuracy: 0.923000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302721\n",
      "iteration 50 / 2000: training loss 1.075823 val loss: 1.118025\n",
      "iteration 100 / 2000: training loss 0.908364 val loss: 0.972031\n",
      "iteration 150 / 2000: training loss 0.998988 val loss: 1.009515\n",
      "iteration 200 / 2000: training loss 0.796748 val loss: 0.833930\n",
      "iteration 250 / 2000: training loss 0.838021 val loss: 0.828124\n",
      "iteration 300 / 2000: training loss 0.751402 val loss: 0.794910\n",
      "iteration 350 / 2000: training loss 0.800385 val loss: 0.868842\n",
      "iteration 400 / 2000: training loss 0.783433 val loss: 0.760539\n",
      "iteration 450 / 2000: training loss 0.833609 val loss: 0.790534\n",
      "iteration 500 / 2000: training loss 0.767275 val loss: 0.765935\n",
      "iteration 550 / 2000: training loss 0.737289 val loss: 0.747275\n",
      "iteration 600 / 2000: training loss 0.659018 val loss: 0.745096\n",
      "iteration 650 / 2000: training loss 0.644717 val loss: 0.747559\n",
      "iteration 700 / 2000: training loss 0.753552 val loss: 0.729847\n",
      "iteration 750 / 2000: training loss 0.721244 val loss: 0.734105\n",
      "iteration 800 / 2000: training loss 0.716297 val loss: 0.753523\n",
      "iteration 850 / 2000: training loss 0.727570 val loss: 0.724994\n",
      "iteration 900 / 2000: training loss 0.695090 val loss: 0.729055\n",
      "iteration 950 / 2000: training loss 0.674872 val loss: 0.717479\n",
      "iteration 1000 / 2000: training loss 0.692123 val loss: 0.715394\n",
      "iteration 1050 / 2000: training loss 0.695690 val loss: 0.714217\n",
      "iteration 1100 / 2000: training loss 0.709963 val loss: 0.720373\n",
      "iteration 1150 / 2000: training loss 0.664564 val loss: 0.716628\n",
      "iteration 1200 / 2000: training loss 0.743286 val loss: 0.717242\n",
      "iteration 1250 / 2000: training loss 0.756528 val loss: 0.721035\n",
      "iteration 1300 / 2000: training loss 0.706469 val loss: 0.714838\n",
      "iteration 1350 / 2000: training loss 0.749208 val loss: 0.717519\n",
      "iteration 1400 / 2000: training loss 0.669980 val loss: 0.713071\n",
      "iteration 1450 / 2000: training loss 0.736814 val loss: 0.719273\n",
      "iteration 1500 / 2000: training loss 0.685530 val loss: 0.710378\n",
      "iteration 1550 / 2000: training loss 0.737186 val loss: 0.712472\n",
      "iteration 1600 / 2000: training loss 0.729913 val loss: 0.708357\n",
      "iteration 1650 / 2000: training loss 0.706645 val loss: 0.706877\n",
      "iteration 1700 / 2000: training loss 0.665722 val loss: 0.709197\n",
      "iteration 1750 / 2000: training loss 0.630716 val loss: 0.708484\n",
      "iteration 1800 / 2000: training loss 0.737495 val loss: 0.706745\n",
      "iteration 1850 / 2000: training loss 0.704987 val loss: 0.707421\n",
      "iteration 1900 / 2000: training loss 0.620112 val loss: 0.707099\n",
      "iteration 1950 / 2000: training loss 0.700006 val loss: 0.707224\n",
      "hidden_size: 150 lr: 5.000000e-01 lr_decay: 8.000000e-01 reg: 2.000000e-02 train accuracy: 0.930000 val accuracy: 0.936000\n",
      "iteration 0 / 2000: training loss 2.302593 val loss: 2.302725\n",
      "iteration 50 / 2000: training loss 2.021176 val loss: 1.947389\n",
      "iteration 100 / 2000: training loss 1.502922 val loss: 1.455095\n",
      "iteration 150 / 2000: training loss 1.462341 val loss: 1.467280\n",
      "iteration 200 / 2000: training loss 1.429843 val loss: 1.543179\n",
      "iteration 250 / 2000: training loss 1.285819 val loss: 1.570538\n",
      "iteration 300 / 2000: training loss 1.366544 val loss: 1.510243\n",
      "iteration 350 / 2000: training loss 1.306645 val loss: 1.355795\n",
      "iteration 400 / 2000: training loss 1.476280 val loss: 1.366361\n",
      "iteration 450 / 2000: training loss 1.281061 val loss: 1.314362\n",
      "iteration 500 / 2000: training loss 1.379511 val loss: 1.303988\n",
      "iteration 550 / 2000: training loss 1.389428 val loss: 1.429294\n",
      "iteration 600 / 2000: training loss 1.385992 val loss: 1.334994\n",
      "hidden_size: 150 lr: 5.000000e-01 lr_decay: 9.800000e-01 reg: 4.000000e-02 train accuracy: 0.740000 val accuracy: 0.697000\n",
      "iteration 0 / 2000: training loss 2.302593 val loss: 2.302725\n",
      "iteration 50 / 2000: training loss 1.411481 val loss: 1.415483\n",
      "iteration 100 / 2000: training loss 1.317691 val loss: 1.314245\n",
      "iteration 150 / 2000: training loss 1.327131 val loss: 1.276992\n",
      "iteration 200 / 2000: training loss 1.256145 val loss: 1.251552\n",
      "iteration 250 / 2000: training loss 1.230447 val loss: 1.378466\n",
      "iteration 300 / 2000: training loss 1.207055 val loss: 1.356526\n",
      "iteration 350 / 2000: training loss 1.409412 val loss: 1.191096\n",
      "iteration 400 / 2000: training loss 1.261172 val loss: 1.144076\n",
      "iteration 450 / 2000: training loss 1.199229 val loss: 1.162735\n",
      "iteration 500 / 2000: training loss 1.097420 val loss: 1.144837\n",
      "iteration 550 / 2000: training loss 1.116343 val loss: 1.112170\n",
      "iteration 600 / 2000: training loss 1.124583 val loss: 1.151828\n",
      "iteration 650 / 2000: training loss 1.049345 val loss: 1.118997\n",
      "iteration 700 / 2000: training loss 1.158366 val loss: 1.140730\n",
      "iteration 750 / 2000: training loss 1.099599 val loss: 1.085268\n",
      "iteration 800 / 2000: training loss 1.078247 val loss: 1.097924\n",
      "iteration 850 / 2000: training loss 1.133067 val loss: 1.081058\n",
      "iteration 900 / 2000: training loss 1.132609 val loss: 1.090164\n",
      "iteration 950 / 2000: training loss 1.004997 val loss: 1.082089\n",
      "iteration 1000 / 2000: training loss 1.104186 val loss: 1.093237\n",
      "iteration 1050 / 2000: training loss 1.037601 val loss: 1.081575\n",
      "iteration 1100 / 2000: training loss 1.049289 val loss: 1.076062\n",
      "iteration 1150 / 2000: training loss 1.009362 val loss: 1.108190\n",
      "iteration 1200 / 2000: training loss 1.091061 val loss: 1.058906\n",
      "iteration 1250 / 2000: training loss 1.128593 val loss: 1.106366\n",
      "iteration 1300 / 2000: training loss 1.094062 val loss: 1.070742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1350 / 2000: training loss 1.109261 val loss: 1.078738\n",
      "iteration 1400 / 2000: training loss 1.051335 val loss: 1.049395\n",
      "iteration 1450 / 2000: training loss 1.104615 val loss: 1.082690\n",
      "iteration 1500 / 2000: training loss 1.063005 val loss: 1.040015\n",
      "iteration 1550 / 2000: training loss 1.083278 val loss: 1.058054\n",
      "iteration 1600 / 2000: training loss 1.072537 val loss: 1.056210\n",
      "iteration 1650 / 2000: training loss 1.022846 val loss: 1.039829\n",
      "iteration 1700 / 2000: training loss 0.986641 val loss: 1.051256\n",
      "iteration 1750 / 2000: training loss 0.938465 val loss: 1.036152\n",
      "iteration 1800 / 2000: training loss 1.103168 val loss: 1.055607\n",
      "iteration 1850 / 2000: training loss 1.040151 val loss: 1.043218\n",
      "iteration 1900 / 2000: training loss 0.952622 val loss: 1.031005\n",
      "iteration 1950 / 2000: training loss 1.017827 val loss: 1.036071\n",
      "hidden_size: 150 lr: 5.000000e-01 lr_decay: 9.000000e-01 reg: 4.000000e-02 train accuracy: 0.910000 val accuracy: 0.906000\n",
      "iteration 0 / 2000: training loss 2.302593 val loss: 2.302725\n",
      "iteration 50 / 2000: training loss 1.456788 val loss: 1.347483\n",
      "iteration 100 / 2000: training loss 1.228660 val loss: 1.278131\n",
      "iteration 150 / 2000: training loss 1.350111 val loss: 1.248308\n",
      "iteration 200 / 2000: training loss 1.234348 val loss: 1.186592\n",
      "iteration 250 / 2000: training loss 1.259189 val loss: 1.192423\n",
      "iteration 300 / 2000: training loss 1.083341 val loss: 1.249206\n",
      "iteration 350 / 2000: training loss 1.241643 val loss: 1.216459\n",
      "iteration 400 / 2000: training loss 1.175907 val loss: 1.127035\n",
      "iteration 450 / 2000: training loss 1.173481 val loss: 1.145250\n",
      "iteration 500 / 2000: training loss 1.090573 val loss: 1.116493\n",
      "iteration 550 / 2000: training loss 1.053886 val loss: 1.118584\n",
      "iteration 600 / 2000: training loss 1.064172 val loss: 1.138930\n",
      "iteration 650 / 2000: training loss 0.980811 val loss: 1.056057\n",
      "iteration 700 / 2000: training loss 1.081295 val loss: 1.104753\n",
      "iteration 750 / 2000: training loss 1.102639 val loss: 1.063937\n",
      "iteration 800 / 2000: training loss 1.078036 val loss: 1.061787\n",
      "hidden_size: 150 lr: 5.000000e-01 lr_decay: 8.500000e-01 reg: 4.000000e-02 train accuracy: 0.880000 val accuracy: 0.884000\n",
      "iteration 0 / 2000: training loss 2.302593 val loss: 2.302725\n",
      "iteration 50 / 2000: training loss 1.341246 val loss: 1.300713\n",
      "iteration 100 / 2000: training loss 1.166999 val loss: 1.293859\n",
      "iteration 150 / 2000: training loss 1.421618 val loss: 1.324294\n",
      "iteration 200 / 2000: training loss 1.136300 val loss: 1.134260\n",
      "iteration 250 / 2000: training loss 1.207394 val loss: 1.166702\n",
      "iteration 300 / 2000: training loss 1.055991 val loss: 1.167670\n",
      "iteration 350 / 2000: training loss 1.167399 val loss: 1.132871\n",
      "iteration 400 / 2000: training loss 1.132814 val loss: 1.117796\n",
      "iteration 450 / 2000: training loss 1.153483 val loss: 1.104786\n",
      "iteration 500 / 2000: training loss 1.040869 val loss: 1.069959\n",
      "hidden_size: 150 lr: 5.000000e-01 lr_decay: 8.000000e-01 reg: 4.000000e-02 train accuracy: 0.910000 val accuracy: 0.844000\n",
      "iteration 0 / 2000: training loss 2.302597 val loss: 2.302729\n",
      "iteration 50 / 2000: training loss 2.510830 val loss: 1.910100\n",
      "iteration 100 / 2000: training loss 1.831710 val loss: 1.649534\n",
      "iteration 150 / 2000: training loss 1.797167 val loss: 1.688905\n",
      "iteration 200 / 2000: training loss 1.515011 val loss: 1.557468\n",
      "iteration 250 / 2000: training loss 1.595737 val loss: 1.806300\n",
      "iteration 300 / 2000: training loss 1.913926 val loss: 1.753768\n",
      "iteration 350 / 2000: training loss 1.618669 val loss: 1.619226\n",
      "iteration 400 / 2000: training loss 1.752067 val loss: 1.802383\n",
      "iteration 450 / 2000: training loss 1.495093 val loss: 1.542065\n",
      "iteration 500 / 2000: training loss 1.761899 val loss: 1.599109\n",
      "iteration 550 / 2000: training loss 1.592007 val loss: 1.544387\n",
      "iteration 600 / 2000: training loss 1.635886 val loss: 1.545530\n",
      "hidden_size: 150 lr: 5.000000e-01 lr_decay: 9.800000e-01 reg: 6.000000e-02 train accuracy: 0.740000 val accuracy: 0.691000\n",
      "iteration 0 / 2000: training loss 2.302597 val loss: 2.302729\n",
      "iteration 50 / 2000: training loss 1.689052 val loss: 1.571591\n",
      "iteration 100 / 2000: training loss 1.535998 val loss: 1.534078\n",
      "iteration 150 / 2000: training loss 1.511923 val loss: 1.644367\n",
      "iteration 200 / 2000: training loss 1.480572 val loss: 1.503956\n",
      "iteration 250 / 2000: training loss 1.473258 val loss: 1.681969\n",
      "iteration 300 / 2000: training loss 1.453845 val loss: 1.534217\n",
      "iteration 350 / 2000: training loss 1.514584 val loss: 1.375731\n",
      "iteration 400 / 2000: training loss 1.479227 val loss: 1.445235\n",
      "iteration 450 / 2000: training loss 1.491732 val loss: 1.427718\n",
      "iteration 500 / 2000: training loss 1.324496 val loss: 1.386105\n",
      "iteration 550 / 2000: training loss 1.358431 val loss: 1.380448\n",
      "iteration 600 / 2000: training loss 1.361912 val loss: 1.414717\n",
      "iteration 650 / 2000: training loss 1.343877 val loss: 1.379988\n",
      "iteration 700 / 2000: training loss 1.415088 val loss: 1.436043\n",
      "iteration 750 / 2000: training loss 1.338213 val loss: 1.339300\n",
      "iteration 800 / 2000: training loss 1.313748 val loss: 1.335163\n",
      "hidden_size: 150 lr: 5.000000e-01 lr_decay: 9.000000e-01 reg: 6.000000e-02 train accuracy: 0.850000 val accuracy: 0.823000\n",
      "iteration 0 / 2000: training loss 2.302597 val loss: 2.302729\n",
      "iteration 50 / 2000: training loss 1.717030 val loss: 1.549350\n",
      "iteration 100 / 2000: training loss 1.483552 val loss: 1.563992\n",
      "iteration 150 / 2000: training loss 1.565720 val loss: 1.441139\n",
      "iteration 200 / 2000: training loss 1.433618 val loss: 1.432475\n",
      "iteration 250 / 2000: training loss 1.564223 val loss: 1.420614\n",
      "iteration 300 / 2000: training loss 1.353985 val loss: 1.498145\n",
      "iteration 350 / 2000: training loss 1.628363 val loss: 1.499219\n",
      "iteration 400 / 2000: training loss 1.448004 val loss: 1.391963\n",
      "iteration 450 / 2000: training loss 1.397037 val loss: 1.387616\n",
      "iteration 500 / 2000: training loss 1.294397 val loss: 1.341988\n",
      "iteration 550 / 2000: training loss 1.292691 val loss: 1.366001\n",
      "iteration 600 / 2000: training loss 1.335706 val loss: 1.373475\n",
      "iteration 650 / 2000: training loss 1.220405 val loss: 1.292725\n",
      "iteration 700 / 2000: training loss 1.332800 val loss: 1.374722\n",
      "iteration 750 / 2000: training loss 1.375749 val loss: 1.319108\n",
      "iteration 800 / 2000: training loss 1.307425 val loss: 1.305750\n",
      "iteration 850 / 2000: training loss 1.334474 val loss: 1.314894\n",
      "iteration 900 / 2000: training loss 1.321785 val loss: 1.315388\n",
      "iteration 950 / 2000: training loss 1.255902 val loss: 1.303900\n",
      "iteration 1000 / 2000: training loss 1.265347 val loss: 1.314320\n",
      "iteration 1050 / 2000: training loss 1.262879 val loss: 1.280913\n",
      "iteration 1100 / 2000: training loss 1.270465 val loss: 1.293120\n",
      "iteration 1150 / 2000: training loss 1.226288 val loss: 1.304153\n",
      "iteration 1200 / 2000: training loss 1.282169 val loss: 1.274060\n",
      "iteration 1250 / 2000: training loss 1.314898 val loss: 1.277197\n",
      "iteration 1300 / 2000: training loss 1.241934 val loss: 1.280251\n",
      "iteration 1350 / 2000: training loss 1.369869 val loss: 1.286854\n",
      "iteration 1400 / 2000: training loss 1.284595 val loss: 1.267736\n",
      "iteration 1450 / 2000: training loss 1.328341 val loss: 1.267150\n",
      "iteration 1500 / 2000: training loss 1.263586 val loss: 1.263168\n",
      "iteration 1550 / 2000: training loss 1.252978 val loss: 1.263758\n",
      "iteration 1600 / 2000: training loss 1.303828 val loss: 1.264940\n",
      "iteration 1650 / 2000: training loss 1.259295 val loss: 1.266252\n",
      "iteration 1700 / 2000: training loss 1.226036 val loss: 1.262338\n",
      "iteration 1750 / 2000: training loss 1.172248 val loss: 1.258630\n",
      "iteration 1800 / 2000: training loss 1.302071 val loss: 1.259651\n",
      "iteration 1850 / 2000: training loss 1.273941 val loss: 1.254713\n",
      "hidden_size: 150 lr: 5.000000e-01 lr_decay: 8.500000e-01 reg: 6.000000e-02 train accuracy: 0.900000 val accuracy: 0.893000\n",
      "iteration 0 / 2000: training loss 2.302597 val loss: 2.302729\n",
      "iteration 50 / 2000: training loss 1.616118 val loss: 1.589060\n",
      "iteration 100 / 2000: training loss 1.493777 val loss: 1.526622\n",
      "iteration 150 / 2000: training loss 1.635615 val loss: 1.554296\n",
      "iteration 200 / 2000: training loss 1.366353 val loss: 1.367261\n",
      "iteration 250 / 2000: training loss 1.488765 val loss: 1.426226\n",
      "iteration 300 / 2000: training loss 1.311333 val loss: 1.413755\n",
      "iteration 350 / 2000: training loss 1.445755 val loss: 1.440932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 400 / 2000: training loss 1.390401 val loss: 1.371895\n",
      "iteration 450 / 2000: training loss 1.414997 val loss: 1.361505\n",
      "iteration 500 / 2000: training loss 1.267888 val loss: 1.331937\n",
      "iteration 550 / 2000: training loss 1.248293 val loss: 1.323428\n",
      "iteration 600 / 2000: training loss 1.253170 val loss: 1.322157\n",
      "iteration 650 / 2000: training loss 1.228650 val loss: 1.302571\n",
      "iteration 700 / 2000: training loss 1.290715 val loss: 1.298276\n",
      "iteration 750 / 2000: training loss 1.286556 val loss: 1.272165\n",
      "iteration 800 / 2000: training loss 1.295430 val loss: 1.294142\n",
      "iteration 850 / 2000: training loss 1.314028 val loss: 1.275015\n",
      "iteration 900 / 2000: training loss 1.259688 val loss: 1.287799\n",
      "iteration 950 / 2000: training loss 1.245817 val loss: 1.281444\n",
      "iteration 1000 / 2000: training loss 1.263298 val loss: 1.270611\n",
      "iteration 1050 / 2000: training loss 1.260300 val loss: 1.267748\n",
      "iteration 1100 / 2000: training loss 1.251532 val loss: 1.266186\n",
      "iteration 1150 / 2000: training loss 1.214221 val loss: 1.268654\n",
      "iteration 1200 / 2000: training loss 1.312476 val loss: 1.271200\n",
      "iteration 1250 / 2000: training loss 1.312082 val loss: 1.264053\n",
      "iteration 1300 / 2000: training loss 1.261679 val loss: 1.261648\n",
      "iteration 1350 / 2000: training loss 1.339852 val loss: 1.273890\n",
      "hidden_size: 150 lr: 5.000000e-01 lr_decay: 8.000000e-01 reg: 6.000000e-02 train accuracy: 0.830000 val accuracy: 0.895000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302847\n",
      "iteration 50 / 2000: training loss 1.740053 val loss: 1.671494\n",
      "iteration 100 / 2000: training loss 1.087126 val loss: 1.322824\n",
      "iteration 150 / 2000: training loss 1.446897 val loss: 1.418326\n",
      "iteration 200 / 2000: training loss 1.199096 val loss: 1.180345\n",
      "iteration 250 / 2000: training loss 1.052524 val loss: 0.984715\n",
      "iteration 300 / 2000: training loss 1.020468 val loss: 1.553792\n",
      "iteration 350 / 2000: training loss 1.209304 val loss: 1.283258\n",
      "iteration 400 / 2000: training loss 0.955295 val loss: 0.996449\n",
      "iteration 450 / 2000: training loss 0.968093 val loss: 1.023594\n",
      "iteration 500 / 2000: training loss 1.231512 val loss: 1.086461\n",
      "hidden_size: 150 lr: 7.000000e-01 lr_decay: 9.800000e-01 reg: 1.000000e-02 train accuracy: 0.870000 val accuracy: 0.820000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302847\n",
      "iteration 50 / 2000: training loss 1.661672 val loss: 1.464761\n",
      "iteration 100 / 2000: training loss 0.947587 val loss: 1.059627\n",
      "iteration 150 / 2000: training loss 1.326447 val loss: 1.080166\n",
      "iteration 200 / 2000: training loss 0.975579 val loss: 0.976166\n",
      "iteration 250 / 2000: training loss 0.897954 val loss: 0.778910\n",
      "iteration 300 / 2000: training loss 0.723055 val loss: 0.977624\n",
      "iteration 350 / 2000: training loss 0.731732 val loss: 0.687444\n",
      "iteration 400 / 2000: training loss 0.640530 val loss: 0.696135\n",
      "iteration 450 / 2000: training loss 0.753062 val loss: 0.755770\n",
      "iteration 500 / 2000: training loss 0.642285 val loss: 0.666209\n",
      "iteration 550 / 2000: training loss 0.636391 val loss: 0.637723\n",
      "iteration 600 / 2000: training loss 0.566669 val loss: 0.660992\n",
      "iteration 650 / 2000: training loss 0.503090 val loss: 0.600175\n",
      "iteration 700 / 2000: training loss 0.720742 val loss: 0.647223\n",
      "iteration 750 / 2000: training loss 0.562053 val loss: 0.618458\n",
      "iteration 800 / 2000: training loss 0.583306 val loss: 0.616659\n",
      "iteration 850 / 2000: training loss 0.612130 val loss: 0.580655\n",
      "iteration 900 / 2000: training loss 0.568653 val loss: 0.578134\n",
      "iteration 950 / 2000: training loss 0.589693 val loss: 0.606659\n",
      "iteration 1000 / 2000: training loss 0.524636 val loss: 0.567682\n",
      "iteration 1050 / 2000: training loss 0.511525 val loss: 0.568737\n",
      "iteration 1100 / 2000: training loss 0.561131 val loss: 0.565052\n",
      "iteration 1150 / 2000: training loss 0.492595 val loss: 0.551493\n",
      "iteration 1200 / 2000: training loss 0.551344 val loss: 0.554558\n",
      "iteration 1250 / 2000: training loss 0.582111 val loss: 0.546790\n",
      "hidden_size: 150 lr: 7.000000e-01 lr_decay: 9.000000e-01 reg: 1.000000e-02 train accuracy: 0.970000 val accuracy: 0.927000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302847\n",
      "iteration 50 / 2000: training loss 1.544264 val loss: 1.483119\n",
      "iteration 100 / 2000: training loss 0.921533 val loss: 0.997186\n",
      "iteration 150 / 2000: training loss 1.097790 val loss: 0.854182\n",
      "iteration 200 / 2000: training loss 0.675330 val loss: 0.707220\n",
      "iteration 250 / 2000: training loss 0.807124 val loss: 0.777292\n",
      "iteration 300 / 2000: training loss 0.687391 val loss: 0.756853\n",
      "iteration 350 / 2000: training loss 0.693009 val loss: 0.706434\n",
      "iteration 400 / 2000: training loss 0.580171 val loss: 0.597853\n",
      "iteration 450 / 2000: training loss 0.649415 val loss: 0.606933\n",
      "iteration 500 / 2000: training loss 0.547633 val loss: 0.615258\n",
      "iteration 550 / 2000: training loss 0.562226 val loss: 0.604872\n",
      "iteration 600 / 2000: training loss 0.466946 val loss: 0.616321\n",
      "iteration 650 / 2000: training loss 0.467219 val loss: 0.551766\n",
      "iteration 700 / 2000: training loss 0.558773 val loss: 0.549728\n",
      "iteration 750 / 2000: training loss 0.535685 val loss: 0.533825\n",
      "iteration 800 / 2000: training loss 0.588199 val loss: 0.539835\n",
      "iteration 850 / 2000: training loss 0.539673 val loss: 0.566838\n",
      "iteration 900 / 2000: training loss 0.523887 val loss: 0.531075\n",
      "iteration 950 / 2000: training loss 0.500201 val loss: 0.525942\n",
      "iteration 1000 / 2000: training loss 0.490516 val loss: 0.518881\n",
      "iteration 1050 / 2000: training loss 0.490005 val loss: 0.545478\n",
      "hidden_size: 150 lr: 7.000000e-01 lr_decay: 8.500000e-01 reg: 1.000000e-02 train accuracy: 0.920000 val accuracy: 0.931000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.302847\n",
      "iteration 50 / 2000: training loss 1.257383 val loss: 1.251597\n",
      "iteration 100 / 2000: training loss 0.722960 val loss: 0.848676\n",
      "iteration 150 / 2000: training loss 1.382368 val loss: 0.953344\n",
      "iteration 200 / 2000: training loss 0.684186 val loss: 0.718509\n",
      "iteration 250 / 2000: training loss 0.701285 val loss: 0.688631\n",
      "iteration 300 / 2000: training loss 0.601250 val loss: 0.656913\n",
      "iteration 350 / 2000: training loss 0.583052 val loss: 0.662767\n",
      "iteration 400 / 2000: training loss 0.578174 val loss: 0.570324\n",
      "iteration 450 / 2000: training loss 0.622659 val loss: 0.598290\n",
      "iteration 500 / 2000: training loss 0.550839 val loss: 0.576104\n",
      "hidden_size: 150 lr: 7.000000e-01 lr_decay: 8.000000e-01 reg: 1.000000e-02 train accuracy: 0.970000 val accuracy: 0.894000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302849\n",
      "iteration 50 / 2000: training loss 2.071738 val loss: 1.987456\n",
      "iteration 100 / 2000: training loss 1.369504 val loss: 1.266110\n",
      "iteration 150 / 2000: training loss 1.113013 val loss: 1.360587\n",
      "iteration 200 / 2000: training loss 1.431614 val loss: 1.484686\n",
      "iteration 250 / 2000: training loss 1.492029 val loss: 1.700719\n",
      "iteration 300 / 2000: training loss 1.742241 val loss: 1.839669\n",
      "iteration 350 / 2000: training loss 1.676440 val loss: 1.371508\n",
      "iteration 400 / 2000: training loss 1.411307 val loss: 1.360370\n",
      "iteration 450 / 2000: training loss 1.451884 val loss: 1.143798\n",
      "iteration 500 / 2000: training loss 1.198172 val loss: 1.455497\n",
      "iteration 550 / 2000: training loss 1.463805 val loss: 1.331863\n",
      "iteration 600 / 2000: training loss 1.278277 val loss: 1.382702\n",
      "hidden_size: 150 lr: 7.000000e-01 lr_decay: 9.800000e-01 reg: 2.000000e-02 train accuracy: 0.640000 val accuracy: 0.639000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302849\n",
      "iteration 50 / 2000: training loss 2.443872 val loss: 2.122505\n",
      "iteration 100 / 2000: training loss 1.200173 val loss: 1.336829\n",
      "iteration 150 / 2000: training loss 1.042843 val loss: 1.045085\n",
      "iteration 200 / 2000: training loss 1.202102 val loss: 1.184166\n",
      "iteration 250 / 2000: training loss 1.081806 val loss: 1.235293\n",
      "iteration 300 / 2000: training loss 1.011337 val loss: 1.067917\n",
      "iteration 350 / 2000: training loss 1.058422 val loss: 0.906518\n",
      "iteration 400 / 2000: training loss 0.917236 val loss: 0.935882\n",
      "iteration 450 / 2000: training loss 1.076552 val loss: 0.934244\n",
      "iteration 500 / 2000: training loss 0.837509 val loss: 0.873151\n",
      "iteration 550 / 2000: training loss 0.848497 val loss: 0.847424\n",
      "iteration 600 / 2000: training loss 0.786324 val loss: 0.865557\n",
      "iteration 650 / 2000: training loss 0.794612 val loss: 0.839269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 700 / 2000: training loss 1.017864 val loss: 0.876375\n",
      "iteration 750 / 2000: training loss 0.804361 val loss: 0.799123\n",
      "iteration 800 / 2000: training loss 0.802706 val loss: 0.822762\n",
      "iteration 850 / 2000: training loss 0.852975 val loss: 0.819063\n",
      "iteration 900 / 2000: training loss 0.810638 val loss: 0.821109\n",
      "iteration 950 / 2000: training loss 0.717135 val loss: 0.800083\n",
      "iteration 1000 / 2000: training loss 0.811929 val loss: 0.812287\n",
      "iteration 1050 / 2000: training loss 0.759251 val loss: 0.807725\n",
      "iteration 1100 / 2000: training loss 0.771546 val loss: 0.784727\n",
      "iteration 1150 / 2000: training loss 0.713329 val loss: 0.786738\n",
      "iteration 1200 / 2000: training loss 0.781377 val loss: 0.767075\n",
      "iteration 1250 / 2000: training loss 0.817388 val loss: 0.790323\n",
      "iteration 1300 / 2000: training loss 0.790660 val loss: 0.772697\n",
      "iteration 1350 / 2000: training loss 0.805670 val loss: 0.773811\n",
      "iteration 1400 / 2000: training loss 0.718829 val loss: 0.755832\n",
      "iteration 1450 / 2000: training loss 0.797275 val loss: 0.780340\n",
      "iteration 1500 / 2000: training loss 0.741353 val loss: 0.729008\n",
      "iteration 1550 / 2000: training loss 0.780918 val loss: 0.757212\n",
      "iteration 1600 / 2000: training loss 0.741657 val loss: 0.764808\n",
      "iteration 1650 / 2000: training loss 0.722018 val loss: 0.727677\n",
      "iteration 1700 / 2000: training loss 0.674981 val loss: 0.750383\n",
      "iteration 1750 / 2000: training loss 0.634525 val loss: 0.735017\n",
      "iteration 1800 / 2000: training loss 0.776968 val loss: 0.748535\n",
      "iteration 1850 / 2000: training loss 0.725073 val loss: 0.738602\n",
      "iteration 1900 / 2000: training loss 0.631656 val loss: 0.722504\n",
      "iteration 1950 / 2000: training loss 0.710086 val loss: 0.734496\n",
      "hidden_size: 150 lr: 7.000000e-01 lr_decay: 9.000000e-01 reg: 2.000000e-02 train accuracy: 0.930000 val accuracy: 0.919000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302849\n",
      "iteration 50 / 2000: training loss 1.598137 val loss: 1.361525\n",
      "iteration 100 / 2000: training loss 1.227109 val loss: 1.146684\n",
      "iteration 150 / 2000: training loss 1.129415 val loss: 1.156726\n",
      "iteration 200 / 2000: training loss 0.953325 val loss: 1.047198\n",
      "iteration 250 / 2000: training loss 1.048641 val loss: 1.014024\n",
      "iteration 300 / 2000: training loss 0.901139 val loss: 0.997550\n",
      "iteration 350 / 2000: training loss 0.899441 val loss: 0.883924\n",
      "iteration 400 / 2000: training loss 0.808079 val loss: 0.811171\n",
      "iteration 450 / 2000: training loss 0.894841 val loss: 0.934159\n",
      "iteration 500 / 2000: training loss 0.774230 val loss: 0.858427\n",
      "iteration 550 / 2000: training loss 0.800935 val loss: 0.831275\n",
      "iteration 600 / 2000: training loss 0.772199 val loss: 0.876684\n",
      "iteration 650 / 2000: training loss 0.686318 val loss: 0.758315\n",
      "iteration 700 / 2000: training loss 0.827663 val loss: 0.826145\n",
      "iteration 750 / 2000: training loss 0.780376 val loss: 0.745387\n",
      "iteration 800 / 2000: training loss 0.778307 val loss: 0.774303\n",
      "iteration 850 / 2000: training loss 0.788634 val loss: 0.775787\n",
      "iteration 900 / 2000: training loss 0.753953 val loss: 0.751350\n",
      "iteration 950 / 2000: training loss 0.702040 val loss: 0.752278\n",
      "iteration 1000 / 2000: training loss 0.711333 val loss: 0.752301\n",
      "iteration 1050 / 2000: training loss 0.721536 val loss: 0.759101\n",
      "iteration 1100 / 2000: training loss 0.754435 val loss: 0.753927\n",
      "iteration 1150 / 2000: training loss 0.677980 val loss: 0.750501\n",
      "iteration 1200 / 2000: training loss 0.748767 val loss: 0.730247\n",
      "iteration 1250 / 2000: training loss 0.778683 val loss: 0.739439\n",
      "iteration 1300 / 2000: training loss 0.703185 val loss: 0.729080\n",
      "iteration 1350 / 2000: training loss 0.786268 val loss: 0.735259\n",
      "iteration 1400 / 2000: training loss 0.680780 val loss: 0.723238\n",
      "iteration 1450 / 2000: training loss 0.770657 val loss: 0.723903\n",
      "iteration 1500 / 2000: training loss 0.710026 val loss: 0.722065\n",
      "iteration 1550 / 2000: training loss 0.748825 val loss: 0.718516\n",
      "iteration 1600 / 2000: training loss 0.747565 val loss: 0.714623\n",
      "iteration 1650 / 2000: training loss 0.711561 val loss: 0.713918\n",
      "iteration 1700 / 2000: training loss 0.665155 val loss: 0.713930\n",
      "iteration 1750 / 2000: training loss 0.632299 val loss: 0.708984\n",
      "iteration 1800 / 2000: training loss 0.733949 val loss: 0.713420\n",
      "iteration 1850 / 2000: training loss 0.707905 val loss: 0.709373\n",
      "iteration 1900 / 2000: training loss 0.622061 val loss: 0.712241\n",
      "iteration 1950 / 2000: training loss 0.704025 val loss: 0.711694\n",
      "hidden_size: 150 lr: 7.000000e-01 lr_decay: 8.500000e-01 reg: 2.000000e-02 train accuracy: 0.930000 val accuracy: 0.936000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302849\n",
      "iteration 50 / 2000: training loss 1.340649 val loss: 1.354709\n",
      "iteration 100 / 2000: training loss 0.987835 val loss: 1.132543\n",
      "iteration 150 / 2000: training loss 1.227723 val loss: 1.041992\n",
      "iteration 200 / 2000: training loss 0.938709 val loss: 0.915227\n",
      "iteration 250 / 2000: training loss 0.932139 val loss: 1.017255\n",
      "iteration 300 / 2000: training loss 0.800119 val loss: 0.933803\n",
      "iteration 350 / 2000: training loss 0.939083 val loss: 0.889644\n",
      "iteration 400 / 2000: training loss 0.793059 val loss: 0.789628\n",
      "iteration 450 / 2000: training loss 0.832704 val loss: 0.796506\n",
      "iteration 500 / 2000: training loss 0.741360 val loss: 0.769277\n",
      "iteration 550 / 2000: training loss 0.726480 val loss: 0.773435\n",
      "iteration 600 / 2000: training loss 0.665154 val loss: 0.783659\n",
      "iteration 650 / 2000: training loss 0.662461 val loss: 0.755586\n",
      "iteration 700 / 2000: training loss 0.745450 val loss: 0.732373\n",
      "iteration 750 / 2000: training loss 0.730975 val loss: 0.731723\n",
      "iteration 800 / 2000: training loss 0.743762 val loss: 0.747244\n",
      "iteration 850 / 2000: training loss 0.723335 val loss: 0.741937\n",
      "iteration 900 / 2000: training loss 0.701773 val loss: 0.740988\n",
      "iteration 950 / 2000: training loss 0.693616 val loss: 0.728822\n",
      "iteration 1000 / 2000: training loss 0.700533 val loss: 0.722705\n",
      "iteration 1050 / 2000: training loss 0.696968 val loss: 0.722107\n",
      "iteration 1100 / 2000: training loss 0.720292 val loss: 0.722861\n",
      "iteration 1150 / 2000: training loss 0.665491 val loss: 0.721722\n",
      "iteration 1200 / 2000: training loss 0.751506 val loss: 0.724227\n",
      "iteration 1250 / 2000: training loss 0.761774 val loss: 0.721786\n",
      "iteration 1300 / 2000: training loss 0.709455 val loss: 0.714471\n",
      "iteration 1350 / 2000: training loss 0.762918 val loss: 0.722059\n",
      "iteration 1400 / 2000: training loss 0.674426 val loss: 0.712440\n",
      "iteration 1450 / 2000: training loss 0.745955 val loss: 0.717645\n",
      "iteration 1500 / 2000: training loss 0.688802 val loss: 0.711518\n",
      "iteration 1550 / 2000: training loss 0.739381 val loss: 0.712079\n",
      "iteration 1600 / 2000: training loss 0.725447 val loss: 0.707428\n",
      "iteration 1650 / 2000: training loss 0.709864 val loss: 0.706558\n",
      "iteration 1700 / 2000: training loss 0.666512 val loss: 0.708122\n",
      "iteration 1750 / 2000: training loss 0.635861 val loss: 0.708800\n",
      "iteration 1800 / 2000: training loss 0.738207 val loss: 0.706244\n",
      "iteration 1850 / 2000: training loss 0.702753 val loss: 0.706582\n",
      "hidden_size: 150 lr: 7.000000e-01 lr_decay: 8.000000e-01 reg: 2.000000e-02 train accuracy: 0.920000 val accuracy: 0.935000\n",
      "iteration 0 / 2000: training loss 2.302593 val loss: 2.302852\n",
      "iteration 50 / 2000: training loss 2.369500 val loss: 2.099123\n",
      "iteration 100 / 2000: training loss 2.534458 val loss: 2.769522\n",
      "iteration 150 / 2000: training loss 2.221526 val loss: 2.647326\n",
      "iteration 200 / 2000: training loss 2.188224 val loss: 2.079007\n",
      "iteration 250 / 2000: training loss 2.037033 val loss: 2.482580\n",
      "iteration 300 / 2000: training loss 2.518850 val loss: 2.221748\n",
      "iteration 350 / 2000: training loss 2.777797 val loss: 2.395350\n",
      "hidden_size: 150 lr: 7.000000e-01 lr_decay: 9.800000e-01 reg: 4.000000e-02 train accuracy: 0.500000 val accuracy: 0.460000\n",
      "iteration 0 / 2000: training loss 2.302593 val loss: 2.302852\n",
      "iteration 50 / 2000: training loss 2.705382 val loss: 2.399470\n",
      "iteration 100 / 2000: training loss 1.782762 val loss: 1.673517\n",
      "iteration 150 / 2000: training loss 1.396760 val loss: 1.408332\n",
      "iteration 200 / 2000: training loss 1.538305 val loss: 1.534594\n",
      "iteration 250 / 2000: training loss 1.442045 val loss: 1.456385\n",
      "iteration 300 / 2000: training loss 1.439005 val loss: 1.543244\n",
      "iteration 350 / 2000: training loss 1.345545 val loss: 1.249363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 400 / 2000: training loss 1.394919 val loss: 1.214633\n",
      "iteration 450 / 2000: training loss 1.295449 val loss: 1.234252\n",
      "iteration 500 / 2000: training loss 1.169266 val loss: 1.358757\n",
      "iteration 550 / 2000: training loss 1.188068 val loss: 1.313508\n",
      "iteration 600 / 2000: training loss 1.180770 val loss: 1.290033\n",
      "iteration 650 / 2000: training loss 1.065688 val loss: 1.178418\n",
      "iteration 700 / 2000: training loss 1.288988 val loss: 1.300614\n",
      "iteration 750 / 2000: training loss 1.193774 val loss: 1.197108\n",
      "iteration 800 / 2000: training loss 1.117820 val loss: 1.155942\n",
      "iteration 850 / 2000: training loss 1.195970 val loss: 1.166387\n",
      "iteration 900 / 2000: training loss 1.121097 val loss: 1.130104\n",
      "iteration 950 / 2000: training loss 1.042950 val loss: 1.161806\n",
      "iteration 1000 / 2000: training loss 1.105713 val loss: 1.232583\n",
      "iteration 1050 / 2000: training loss 1.128555 val loss: 1.110867\n",
      "iteration 1100 / 2000: training loss 1.050056 val loss: 1.112064\n",
      "iteration 1150 / 2000: training loss 1.029975 val loss: 1.121926\n",
      "iteration 1200 / 2000: training loss 1.115899 val loss: 1.078731\n",
      "iteration 1250 / 2000: training loss 1.108082 val loss: 1.082589\n",
      "iteration 1300 / 2000: training loss 1.171764 val loss: 1.110310\n",
      "iteration 1350 / 2000: training loss 1.111581 val loss: 1.073319\n",
      "iteration 1400 / 2000: training loss 1.097376 val loss: 1.078001\n",
      "iteration 1450 / 2000: training loss 1.139883 val loss: 1.103160\n",
      "iteration 1500 / 2000: training loss 1.071878 val loss: 1.048740\n",
      "iteration 1550 / 2000: training loss 1.072955 val loss: 1.047391\n",
      "iteration 1600 / 2000: training loss 1.151941 val loss: 1.100629\n",
      "iteration 1650 / 2000: training loss 1.035734 val loss: 1.060573\n",
      "iteration 1700 / 2000: training loss 1.009020 val loss: 1.062187\n",
      "iteration 1750 / 2000: training loss 0.956433 val loss: 1.065833\n",
      "iteration 1800 / 2000: training loss 1.091693 val loss: 1.071324\n",
      "iteration 1850 / 2000: training loss 1.035174 val loss: 1.050586\n",
      "iteration 1900 / 2000: training loss 0.978995 val loss: 1.039975\n",
      "iteration 1950 / 2000: training loss 1.023180 val loss: 1.053098\n",
      "hidden_size: 150 lr: 7.000000e-01 lr_decay: 9.000000e-01 reg: 4.000000e-02 train accuracy: 0.910000 val accuracy: 0.913000\n",
      "iteration 0 / 2000: training loss 2.302593 val loss: 2.302852\n",
      "iteration 50 / 2000: training loss 2.501882 val loss: 2.024270\n",
      "iteration 100 / 2000: training loss 1.586391 val loss: 1.532594\n",
      "iteration 150 / 2000: training loss 1.503658 val loss: 1.467281\n",
      "iteration 200 / 2000: training loss 1.308476 val loss: 1.361500\n",
      "iteration 250 / 2000: training loss 1.509036 val loss: 1.587001\n",
      "iteration 300 / 2000: training loss 1.259295 val loss: 1.473556\n",
      "iteration 350 / 2000: training loss 1.251527 val loss: 1.166330\n",
      "iteration 400 / 2000: training loss 1.237834 val loss: 1.181771\n",
      "iteration 450 / 2000: training loss 1.202437 val loss: 1.250024\n",
      "iteration 500 / 2000: training loss 1.082674 val loss: 1.125856\n",
      "iteration 550 / 2000: training loss 1.111074 val loss: 1.114491\n",
      "iteration 600 / 2000: training loss 1.096483 val loss: 1.147199\n",
      "iteration 650 / 2000: training loss 0.993557 val loss: 1.094061\n",
      "iteration 700 / 2000: training loss 1.139738 val loss: 1.118963\n",
      "iteration 750 / 2000: training loss 1.116670 val loss: 1.065197\n",
      "iteration 800 / 2000: training loss 1.069165 val loss: 1.092824\n",
      "hidden_size: 150 lr: 7.000000e-01 lr_decay: 8.500000e-01 reg: 4.000000e-02 train accuracy: 0.900000 val accuracy: 0.874000\n",
      "iteration 0 / 2000: training loss 2.302593 val loss: 2.302852\n",
      "iteration 50 / 2000: training loss 1.832481 val loss: 1.732500\n",
      "iteration 100 / 2000: training loss 1.491086 val loss: 1.466256\n",
      "iteration 150 / 2000: training loss 1.361346 val loss: 1.373050\n",
      "iteration 200 / 2000: training loss 1.183941 val loss: 1.242600\n",
      "iteration 250 / 2000: training loss 1.270468 val loss: 1.256713\n",
      "iteration 300 / 2000: training loss 1.151549 val loss: 1.310077\n",
      "iteration 350 / 2000: training loss 1.326591 val loss: 1.210995\n",
      "iteration 400 / 2000: training loss 1.171984 val loss: 1.125980\n",
      "iteration 450 / 2000: training loss 1.164149 val loss: 1.164247\n",
      "iteration 500 / 2000: training loss 1.080841 val loss: 1.113097\n",
      "iteration 550 / 2000: training loss 1.033639 val loss: 1.100639\n",
      "iteration 600 / 2000: training loss 1.042061 val loss: 1.128108\n",
      "iteration 650 / 2000: training loss 0.980389 val loss: 1.056045\n",
      "iteration 700 / 2000: training loss 1.061749 val loss: 1.081645\n",
      "iteration 750 / 2000: training loss 1.062980 val loss: 1.044870\n",
      "iteration 800 / 2000: training loss 1.080004 val loss: 1.058141\n",
      "iteration 850 / 2000: training loss 1.057709 val loss: 1.065348\n",
      "iteration 900 / 2000: training loss 1.037241 val loss: 1.054065\n",
      "iteration 950 / 2000: training loss 1.015478 val loss: 1.047637\n",
      "iteration 1000 / 2000: training loss 1.021030 val loss: 1.040156\n",
      "iteration 1050 / 2000: training loss 1.021157 val loss: 1.042457\n",
      "hidden_size: 150 lr: 7.000000e-01 lr_decay: 8.000000e-01 reg: 4.000000e-02 train accuracy: 0.880000 val accuracy: 0.912000\n",
      "iteration 0 / 2000: training loss 2.302597 val loss: 2.302856\n",
      "iteration 50 / 2000: training loss 3.146825 val loss: 3.416157\n",
      "iteration 100 / 2000: training loss 2.830382 val loss: 2.790542\n",
      "iteration 150 / 2000: training loss 3.134499 val loss: 2.896322\n",
      "iteration 200 / 2000: training loss 2.187196 val loss: 2.268359\n",
      "iteration 250 / 2000: training loss 3.035614 val loss: 3.125907\n",
      "hidden_size: 150 lr: 7.000000e-01 lr_decay: 9.800000e-01 reg: 6.000000e-02 train accuracy: 0.430000 val accuracy: 0.408000\n",
      "iteration 0 / 2000: training loss 2.302597 val loss: 2.302856\n",
      "iteration 50 / 2000: training loss 2.272874 val loss: 2.731510\n",
      "iteration 100 / 2000: training loss 2.220476 val loss: 1.840378\n",
      "iteration 150 / 2000: training loss 1.867906 val loss: 1.881589\n",
      "iteration 200 / 2000: training loss 1.821743 val loss: 1.784545\n",
      "iteration 250 / 2000: training loss 1.685271 val loss: 1.798393\n",
      "iteration 300 / 2000: training loss 1.815782 val loss: 1.750596\n",
      "iteration 350 / 2000: training loss 1.597717 val loss: 1.623138\n",
      "iteration 400 / 2000: training loss 1.571210 val loss: 1.511367\n",
      "iteration 450 / 2000: training loss 1.524647 val loss: 1.524359\n",
      "iteration 500 / 2000: training loss 1.462437 val loss: 1.529510\n",
      "iteration 550 / 2000: training loss 1.527322 val loss: 1.580665\n",
      "iteration 600 / 2000: training loss 1.563015 val loss: 1.690660\n",
      "iteration 650 / 2000: training loss 1.382142 val loss: 1.474078\n",
      "iteration 700 / 2000: training loss 1.492241 val loss: 1.517242\n",
      "iteration 750 / 2000: training loss 1.457195 val loss: 1.403458\n",
      "iteration 800 / 2000: training loss 1.383203 val loss: 1.429229\n",
      "iteration 850 / 2000: training loss 1.427142 val loss: 1.389235\n",
      "iteration 900 / 2000: training loss 1.406878 val loss: 1.413989\n",
      "iteration 950 / 2000: training loss 1.291390 val loss: 1.450910\n",
      "iteration 1000 / 2000: training loss 1.357093 val loss: 1.481727\n",
      "iteration 1050 / 2000: training loss 1.366703 val loss: 1.365182\n",
      "iteration 1100 / 2000: training loss 1.291549 val loss: 1.359172\n",
      "iteration 1150 / 2000: training loss 1.288358 val loss: 1.394966\n",
      "iteration 1200 / 2000: training loss 1.379720 val loss: 1.337214\n",
      "iteration 1250 / 2000: training loss 1.350607 val loss: 1.348275\n",
      "iteration 1300 / 2000: training loss 1.431533 val loss: 1.368123\n",
      "iteration 1350 / 2000: training loss 1.345260 val loss: 1.327391\n",
      "iteration 1400 / 2000: training loss 1.344717 val loss: 1.328576\n",
      "iteration 1450 / 2000: training loss 1.422545 val loss: 1.355643\n",
      "iteration 1500 / 2000: training loss 1.292974 val loss: 1.293577\n",
      "iteration 1550 / 2000: training loss 1.298297 val loss: 1.288342\n",
      "iteration 1600 / 2000: training loss 1.384684 val loss: 1.316680\n",
      "iteration 1650 / 2000: training loss 1.272514 val loss: 1.302379\n",
      "iteration 1700 / 2000: training loss 1.244228 val loss: 1.293168\n",
      "hidden_size: 150 lr: 7.000000e-01 lr_decay: 9.000000e-01 reg: 6.000000e-02 train accuracy: 0.830000 val accuracy: 0.858000\n",
      "iteration 0 / 2000: training loss 2.302597 val loss: 2.302856\n",
      "iteration 50 / 2000: training loss 2.566743 val loss: 2.376465\n",
      "iteration 100 / 2000: training loss 1.869799 val loss: 1.892549\n",
      "iteration 150 / 2000: training loss 1.788244 val loss: 1.686060\n",
      "iteration 200 / 2000: training loss 1.514906 val loss: 1.669549\n",
      "iteration 250 / 2000: training loss 1.518302 val loss: 1.763153\n",
      "iteration 300 / 2000: training loss 1.528633 val loss: 1.550827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 350 / 2000: training loss 1.553897 val loss: 1.447456\n",
      "iteration 400 / 2000: training loss 1.524585 val loss: 1.471951\n",
      "iteration 450 / 2000: training loss 1.471018 val loss: 1.404691\n",
      "iteration 500 / 2000: training loss 1.331769 val loss: 1.376414\n",
      "iteration 550 / 2000: training loss 1.338596 val loss: 1.379913\n",
      "iteration 600 / 2000: training loss 1.364502 val loss: 1.390776\n",
      "iteration 650 / 2000: training loss 1.278111 val loss: 1.345884\n",
      "iteration 700 / 2000: training loss 1.426861 val loss: 1.422890\n",
      "iteration 750 / 2000: training loss 1.333333 val loss: 1.310177\n",
      "iteration 800 / 2000: training loss 1.312220 val loss: 1.318080\n",
      "hidden_size: 150 lr: 7.000000e-01 lr_decay: 8.500000e-01 reg: 6.000000e-02 train accuracy: 0.870000 val accuracy: 0.818000\n",
      "iteration 0 / 2000: training loss 2.302597 val loss: 2.302856\n",
      "iteration 50 / 2000: training loss 2.228994 val loss: 1.981998\n",
      "iteration 100 / 2000: training loss 1.883680 val loss: 1.703990\n",
      "iteration 150 / 2000: training loss 1.602850 val loss: 1.568028\n",
      "iteration 200 / 2000: training loss 1.414054 val loss: 1.506141\n",
      "iteration 250 / 2000: training loss 1.545458 val loss: 1.565535\n",
      "iteration 300 / 2000: training loss 1.413142 val loss: 1.615213\n",
      "iteration 350 / 2000: training loss 1.580422 val loss: 1.432819\n",
      "iteration 400 / 2000: training loss 1.460222 val loss: 1.382550\n",
      "iteration 450 / 2000: training loss 1.404349 val loss: 1.411372\n",
      "iteration 500 / 2000: training loss 1.294625 val loss: 1.345337\n",
      "iteration 550 / 2000: training loss 1.275091 val loss: 1.368499\n",
      "iteration 600 / 2000: training loss 1.318739 val loss: 1.368889\n",
      "iteration 650 / 2000: training loss 1.220881 val loss: 1.298334\n",
      "iteration 700 / 2000: training loss 1.297787 val loss: 1.338632\n",
      "iteration 750 / 2000: training loss 1.311342 val loss: 1.297983\n",
      "iteration 800 / 2000: training loss 1.317561 val loss: 1.296557\n",
      "iteration 850 / 2000: training loss 1.300690 val loss: 1.302188\n",
      "iteration 900 / 2000: training loss 1.285110 val loss: 1.292714\n",
      "iteration 950 / 2000: training loss 1.260451 val loss: 1.290708\n",
      "iteration 1000 / 2000: training loss 1.262231 val loss: 1.286195\n",
      "iteration 1050 / 2000: training loss 1.272403 val loss: 1.285319\n",
      "hidden_size: 150 lr: 7.000000e-01 lr_decay: 8.000000e-01 reg: 6.000000e-02 train accuracy: 0.840000 val accuracy: 0.889000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.303017\n",
      "iteration 50 / 2000: training loss 2.711546 val loss: 2.870504\n",
      "iteration 100 / 2000: training loss 1.429311 val loss: 1.613555\n",
      "iteration 150 / 2000: training loss 1.988141 val loss: 1.796353\n",
      "iteration 200 / 2000: training loss 1.625419 val loss: 1.304515\n",
      "iteration 250 / 2000: training loss 1.820674 val loss: 2.095042\n",
      "hidden_size: 150 lr: 9.000000e-01 lr_decay: 9.800000e-01 reg: 1.000000e-02 train accuracy: 0.640000 val accuracy: 0.577000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.303017\n",
      "iteration 50 / 2000: training loss 1.922344 val loss: 1.534306\n",
      "iteration 100 / 2000: training loss 1.543400 val loss: 1.530235\n",
      "iteration 150 / 2000: training loss 1.317343 val loss: 1.335847\n",
      "iteration 200 / 2000: training loss 1.033483 val loss: 0.877329\n",
      "iteration 250 / 2000: training loss 1.016194 val loss: 1.164239\n",
      "iteration 300 / 2000: training loss 0.853001 val loss: 1.023793\n",
      "iteration 350 / 2000: training loss 1.132288 val loss: 0.880381\n",
      "iteration 400 / 2000: training loss 0.980091 val loss: 0.967825\n",
      "iteration 450 / 2000: training loss 0.769836 val loss: 0.775522\n",
      "iteration 500 / 2000: training loss 0.852698 val loss: 0.811370\n",
      "iteration 550 / 2000: training loss 0.658108 val loss: 0.746229\n",
      "iteration 600 / 2000: training loss 0.647621 val loss: 0.702752\n",
      "iteration 650 / 2000: training loss 0.633232 val loss: 0.724588\n",
      "iteration 700 / 2000: training loss 0.988727 val loss: 0.753274\n",
      "iteration 750 / 2000: training loss 0.635796 val loss: 0.652962\n",
      "iteration 800 / 2000: training loss 0.708595 val loss: 0.645733\n",
      "hidden_size: 150 lr: 9.000000e-01 lr_decay: 9.000000e-01 reg: 1.000000e-02 train accuracy: 0.940000 val accuracy: 0.901000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.303017\n",
      "iteration 50 / 2000: training loss 2.029260 val loss: 1.989476\n",
      "iteration 100 / 2000: training loss 1.125332 val loss: 1.203668\n",
      "iteration 150 / 2000: training loss 1.593846 val loss: 1.418291\n",
      "iteration 200 / 2000: training loss 1.016340 val loss: 1.032360\n",
      "iteration 250 / 2000: training loss 0.939320 val loss: 1.072261\n",
      "iteration 300 / 2000: training loss 0.727549 val loss: 0.953407\n",
      "iteration 350 / 2000: training loss 0.755452 val loss: 0.759124\n",
      "iteration 400 / 2000: training loss 0.590683 val loss: 0.661112\n",
      "iteration 450 / 2000: training loss 0.847882 val loss: 0.823765\n",
      "iteration 500 / 2000: training loss 0.603075 val loss: 0.686166\n",
      "iteration 550 / 2000: training loss 0.625570 val loss: 0.658572\n",
      "iteration 600 / 2000: training loss 0.572376 val loss: 0.662730\n",
      "iteration 650 / 2000: training loss 0.499078 val loss: 0.580880\n",
      "iteration 700 / 2000: training loss 0.666922 val loss: 0.633267\n",
      "iteration 750 / 2000: training loss 0.584325 val loss: 0.548329\n",
      "iteration 800 / 2000: training loss 0.564757 val loss: 0.594894\n",
      "iteration 850 / 2000: training loss 0.602957 val loss: 0.590522\n",
      "iteration 900 / 2000: training loss 0.531147 val loss: 0.564442\n",
      "iteration 950 / 2000: training loss 0.507006 val loss: 0.554598\n",
      "iteration 1000 / 2000: training loss 0.496357 val loss: 0.543505\n",
      "iteration 1050 / 2000: training loss 0.516159 val loss: 0.582951\n",
      "iteration 1100 / 2000: training loss 0.552034 val loss: 0.559153\n",
      "iteration 1150 / 2000: training loss 0.464022 val loss: 0.544848\n",
      "iteration 1200 / 2000: training loss 0.527265 val loss: 0.536428\n",
      "iteration 1250 / 2000: training loss 0.580994 val loss: 0.542018\n",
      "iteration 1300 / 2000: training loss 0.494742 val loss: 0.522768\n",
      "iteration 1350 / 2000: training loss 0.577105 val loss: 0.529826\n",
      "iteration 1400 / 2000: training loss 0.450443 val loss: 0.517555\n",
      "iteration 1450 / 2000: training loss 0.548640 val loss: 0.518655\n",
      "iteration 1500 / 2000: training loss 0.514715 val loss: 0.517926\n",
      "iteration 1550 / 2000: training loss 0.548978 val loss: 0.512737\n",
      "iteration 1600 / 2000: training loss 0.526308 val loss: 0.506125\n",
      "iteration 1650 / 2000: training loss 0.511652 val loss: 0.505252\n",
      "iteration 1700 / 2000: training loss 0.460934 val loss: 0.504880\n",
      "iteration 1750 / 2000: training loss 0.418588 val loss: 0.502544\n",
      "iteration 1800 / 2000: training loss 0.508631 val loss: 0.506663\n",
      "iteration 1850 / 2000: training loss 0.483771 val loss: 0.503911\n",
      "iteration 1900 / 2000: training loss 0.405942 val loss: 0.506350\n",
      "iteration 1950 / 2000: training loss 0.486742 val loss: 0.508352\n",
      "hidden_size: 150 lr: 9.000000e-01 lr_decay: 8.500000e-01 reg: 1.000000e-02 train accuracy: 0.950000 val accuracy: 0.946000\n",
      "iteration 0 / 2000: training loss 2.302587 val loss: 2.303017\n",
      "iteration 50 / 2000: training loss 2.360038 val loss: 2.087162\n",
      "iteration 100 / 2000: training loss 1.126450 val loss: 0.991899\n",
      "iteration 150 / 2000: training loss 1.401964 val loss: 1.005678\n",
      "iteration 200 / 2000: training loss 0.738075 val loss: 0.753235\n",
      "iteration 250 / 2000: training loss 0.863375 val loss: 0.869154\n",
      "iteration 300 / 2000: training loss 0.688969 val loss: 0.732724\n",
      "iteration 350 / 2000: training loss 0.668388 val loss: 0.737914\n",
      "iteration 400 / 2000: training loss 0.594461 val loss: 0.614151\n",
      "iteration 450 / 2000: training loss 0.610644 val loss: 0.623754\n",
      "iteration 500 / 2000: training loss 0.529105 val loss: 0.609418\n",
      "iteration 550 / 2000: training loss 0.537435 val loss: 0.580287\n",
      "iteration 600 / 2000: training loss 0.445805 val loss: 0.581694\n",
      "iteration 650 / 2000: training loss 0.461470 val loss: 0.557005\n",
      "iteration 700 / 2000: training loss 0.540273 val loss: 0.522117\n",
      "iteration 750 / 2000: training loss 0.497426 val loss: 0.539749\n",
      "iteration 800 / 2000: training loss 0.542121 val loss: 0.551673\n",
      "iteration 850 / 2000: training loss 0.499390 val loss: 0.539297\n",
      "iteration 900 / 2000: training loss 0.478655 val loss: 0.533805\n",
      "iteration 950 / 2000: training loss 0.487141 val loss: 0.516136\n",
      "iteration 1000 / 2000: training loss 0.483013 val loss: 0.510419\n",
      "iteration 1050 / 2000: training loss 0.476380 val loss: 0.514623\n",
      "iteration 1100 / 2000: training loss 0.510058 val loss: 0.517290\n",
      "iteration 1150 / 2000: training loss 0.451332 val loss: 0.513413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1200 / 2000: training loss 0.515114 val loss: 0.517216\n",
      "iteration 1250 / 2000: training loss 0.551055 val loss: 0.515887\n",
      "iteration 1300 / 2000: training loss 0.476791 val loss: 0.506845\n",
      "iteration 1350 / 2000: training loss 0.542524 val loss: 0.511811\n",
      "iteration 1400 / 2000: training loss 0.442510 val loss: 0.503867\n",
      "iteration 1450 / 2000: training loss 0.515962 val loss: 0.509228\n",
      "iteration 1500 / 2000: training loss 0.480730 val loss: 0.501462\n",
      "iteration 1550 / 2000: training loss 0.550594 val loss: 0.502704\n",
      "iteration 1600 / 2000: training loss 0.499180 val loss: 0.498444\n",
      "iteration 1650 / 2000: training loss 0.500852 val loss: 0.497948\n",
      "iteration 1700 / 2000: training loss 0.458498 val loss: 0.497774\n",
      "iteration 1750 / 2000: training loss 0.422647 val loss: 0.499135\n",
      "iteration 1800 / 2000: training loss 0.516992 val loss: 0.497939\n",
      "iteration 1850 / 2000: training loss 0.479532 val loss: 0.498151\n",
      "iteration 1900 / 2000: training loss 0.397997 val loss: 0.497827\n",
      "iteration 1950 / 2000: training loss 0.472891 val loss: 0.499043\n",
      "hidden_size: 150 lr: 9.000000e-01 lr_decay: 8.000000e-01 reg: 1.000000e-02 train accuracy: 0.950000 val accuracy: 0.946000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.303019\n",
      "iteration 50 / 2000: training loss 2.168870 val loss: 2.205648\n",
      "iteration 100 / 2000: training loss 1.512034 val loss: 1.947886\n",
      "iteration 150 / 2000: training loss 3.412061 val loss: 3.283199\n",
      "iteration 200 / 2000: training loss 1.765926 val loss: 2.168167\n",
      "iteration 250 / 2000: training loss 5.003964 val loss: 4.851658\n",
      "hidden_size: 150 lr: 9.000000e-01 lr_decay: 9.800000e-01 reg: 2.000000e-02 train accuracy: 0.540000 val accuracy: 0.493000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.303019\n",
      "iteration 50 / 2000: training loss 2.167246 val loss: 2.257783\n",
      "iteration 100 / 2000: training loss 1.358741 val loss: 1.388878\n",
      "iteration 150 / 2000: training loss 1.356252 val loss: 1.451144\n",
      "iteration 200 / 2000: training loss 1.291814 val loss: 1.251526\n",
      "iteration 250 / 2000: training loss 1.795272 val loss: 1.969505\n",
      "iteration 300 / 2000: training loss 1.172100 val loss: 1.319408\n",
      "iteration 350 / 2000: training loss 1.299063 val loss: 1.436411\n",
      "iteration 400 / 2000: training loss 1.272639 val loss: 1.121808\n",
      "iteration 450 / 2000: training loss 1.104927 val loss: 1.116866\n",
      "iteration 500 / 2000: training loss 1.112251 val loss: 1.171149\n",
      "iteration 550 / 2000: training loss 0.991002 val loss: 1.068321\n",
      "iteration 600 / 2000: training loss 0.990528 val loss: 1.066790\n",
      "iteration 650 / 2000: training loss 0.936586 val loss: 1.116717\n",
      "iteration 700 / 2000: training loss 1.001011 val loss: 1.042598\n",
      "iteration 750 / 2000: training loss 0.870190 val loss: 0.911873\n",
      "iteration 800 / 2000: training loss 0.836783 val loss: 0.850809\n",
      "iteration 850 / 2000: training loss 0.933730 val loss: 0.900968\n",
      "iteration 900 / 2000: training loss 0.868502 val loss: 0.880665\n",
      "iteration 950 / 2000: training loss 0.775590 val loss: 0.894940\n",
      "iteration 1000 / 2000: training loss 0.955733 val loss: 1.072222\n",
      "iteration 1050 / 2000: training loss 0.808510 val loss: 0.844659\n",
      "iteration 1100 / 2000: training loss 0.776798 val loss: 0.799000\n",
      "iteration 1150 / 2000: training loss 0.715702 val loss: 0.766376\n",
      "iteration 1200 / 2000: training loss 0.815932 val loss: 0.794001\n",
      "iteration 1250 / 2000: training loss 0.817761 val loss: 0.788408\n",
      "iteration 1300 / 2000: training loss 0.822367 val loss: 0.804326\n",
      "iteration 1350 / 2000: training loss 0.817778 val loss: 0.792796\n",
      "iteration 1400 / 2000: training loss 0.755409 val loss: 0.766571\n",
      "iteration 1450 / 2000: training loss 0.841888 val loss: 0.789064\n",
      "iteration 1500 / 2000: training loss 0.768812 val loss: 0.759058\n",
      "iteration 1550 / 2000: training loss 0.819008 val loss: 0.760252\n",
      "iteration 1600 / 2000: training loss 0.866531 val loss: 0.826064\n",
      "iteration 1650 / 2000: training loss 0.777595 val loss: 0.746066\n",
      "iteration 1700 / 2000: training loss 0.699401 val loss: 0.772990\n",
      "iteration 1750 / 2000: training loss 0.681726 val loss: 0.788895\n",
      "iteration 1800 / 2000: training loss 0.776767 val loss: 0.764836\n",
      "iteration 1850 / 2000: training loss 0.739237 val loss: 0.759422\n",
      "hidden_size: 150 lr: 9.000000e-01 lr_decay: 9.000000e-01 reg: 2.000000e-02 train accuracy: 0.940000 val accuracy: 0.913000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.303019\n",
      "iteration 50 / 2000: training loss 1.807847 val loss: 1.642109\n",
      "iteration 100 / 2000: training loss 1.658106 val loss: 1.662889\n",
      "iteration 150 / 2000: training loss 1.273967 val loss: 1.266932\n",
      "iteration 200 / 2000: training loss 1.136183 val loss: 1.177132\n",
      "iteration 250 / 2000: training loss 1.316993 val loss: 1.275568\n",
      "iteration 300 / 2000: training loss 1.063069 val loss: 1.307354\n",
      "iteration 350 / 2000: training loss 1.089154 val loss: 0.959825\n",
      "iteration 400 / 2000: training loss 0.880920 val loss: 0.842556\n",
      "iteration 450 / 2000: training loss 0.968860 val loss: 0.878538\n",
      "iteration 500 / 2000: training loss 0.805987 val loss: 0.832994\n",
      "iteration 550 / 2000: training loss 0.869590 val loss: 0.850202\n",
      "iteration 600 / 2000: training loss 0.776210 val loss: 0.841567\n",
      "iteration 650 / 2000: training loss 0.707294 val loss: 0.790043\n",
      "iteration 700 / 2000: training loss 0.886641 val loss: 0.822468\n",
      "iteration 750 / 2000: training loss 0.775987 val loss: 0.770492\n",
      "iteration 800 / 2000: training loss 0.773209 val loss: 0.815022\n",
      "iteration 850 / 2000: training loss 0.846118 val loss: 0.772128\n",
      "iteration 900 / 2000: training loss 0.782920 val loss: 0.769895\n",
      "iteration 950 / 2000: training loss 0.750206 val loss: 0.785508\n",
      "iteration 1000 / 2000: training loss 0.718936 val loss: 0.757686\n",
      "iteration 1050 / 2000: training loss 0.716607 val loss: 0.755972\n",
      "iteration 1100 / 2000: training loss 0.749438 val loss: 0.752108\n",
      "iteration 1150 / 2000: training loss 0.687703 val loss: 0.755810\n",
      "iteration 1200 / 2000: training loss 0.744809 val loss: 0.751022\n",
      "iteration 1250 / 2000: training loss 0.801947 val loss: 0.751540\n",
      "iteration 1300 / 2000: training loss 0.715668 val loss: 0.744023\n",
      "iteration 1350 / 2000: training loss 0.790246 val loss: 0.733328\n",
      "iteration 1400 / 2000: training loss 0.678833 val loss: 0.732832\n",
      "iteration 1450 / 2000: training loss 0.776500 val loss: 0.729968\n",
      "iteration 1500 / 2000: training loss 0.719148 val loss: 0.723563\n",
      "iteration 1550 / 2000: training loss 0.742746 val loss: 0.720705\n",
      "iteration 1600 / 2000: training loss 0.745008 val loss: 0.720894\n",
      "iteration 1650 / 2000: training loss 0.716598 val loss: 0.719761\n",
      "iteration 1700 / 2000: training loss 0.673217 val loss: 0.717232\n",
      "iteration 1750 / 2000: training loss 0.636982 val loss: 0.713310\n",
      "iteration 1800 / 2000: training loss 0.740606 val loss: 0.722261\n",
      "iteration 1850 / 2000: training loss 0.712952 val loss: 0.713911\n",
      "iteration 1900 / 2000: training loss 0.623717 val loss: 0.716292\n",
      "iteration 1950 / 2000: training loss 0.707175 val loss: 0.713880\n",
      "hidden_size: 150 lr: 9.000000e-01 lr_decay: 8.500000e-01 reg: 2.000000e-02 train accuracy: 0.930000 val accuracy: 0.933000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.303019\n",
      "iteration 50 / 2000: training loss 1.986834 val loss: 1.904134\n",
      "iteration 100 / 2000: training loss 1.425936 val loss: 1.353228\n",
      "iteration 150 / 2000: training loss 1.597410 val loss: 1.290797\n",
      "iteration 200 / 2000: training loss 1.013431 val loss: 0.977880\n",
      "iteration 250 / 2000: training loss 1.136636 val loss: 1.121339\n",
      "iteration 300 / 2000: training loss 0.866200 val loss: 0.927410\n",
      "iteration 350 / 2000: training loss 0.955919 val loss: 0.947037\n",
      "iteration 400 / 2000: training loss 0.879528 val loss: 0.841111\n",
      "iteration 450 / 2000: training loss 0.918182 val loss: 0.853354\n",
      "iteration 500 / 2000: training loss 0.831383 val loss: 0.862058\n",
      "iteration 550 / 2000: training loss 0.761144 val loss: 0.833075\n",
      "iteration 600 / 2000: training loss 0.720982 val loss: 0.836765\n",
      "iteration 650 / 2000: training loss 0.676673 val loss: 0.751174\n",
      "iteration 700 / 2000: training loss 0.772640 val loss: 0.771142\n",
      "iteration 750 / 2000: training loss 0.759379 val loss: 0.735757\n",
      "iteration 800 / 2000: training loss 0.795299 val loss: 0.748507\n",
      "iteration 850 / 2000: training loss 0.744773 val loss: 0.772046\n",
      "iteration 900 / 2000: training loss 0.726115 val loss: 0.745474\n",
      "iteration 950 / 2000: training loss 0.720530 val loss: 0.736420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1000 / 2000: training loss 0.702688 val loss: 0.730615\n",
      "iteration 1050 / 2000: training loss 0.698284 val loss: 0.737706\n",
      "hidden_size: 150 lr: 9.000000e-01 lr_decay: 8.000000e-01 reg: 2.000000e-02 train accuracy: 0.900000 val accuracy: 0.917000\n",
      "iteration 0 / 2000: training loss 2.302593 val loss: 2.303023\n",
      "iteration 50 / 2000: training loss 3.712425 val loss: 3.556062\n",
      "iteration 100 / 2000: training loss 14.038006 val loss: 13.553812\n",
      "iteration 150 / 2000: training loss nan val loss: nan\n",
      "iteration 200 / 2000: training loss nan val loss: nan\n",
      "iteration 250 / 2000: training loss nan val loss: nan\n",
      "iteration 300 / 2000: training loss nan val loss: nan\n",
      "iteration 350 / 2000: training loss nan val loss: nan\n",
      "iteration 400 / 2000: training loss nan val loss: nan\n",
      "iteration 450 / 2000: training loss nan val loss: nan\n",
      "iteration 500 / 2000: training loss nan val loss: nan\n",
      "iteration 550 / 2000: training loss nan val loss: nan\n",
      "iteration 600 / 2000: training loss nan val loss: nan\n",
      "iteration 650 / 2000: training loss nan val loss: nan\n",
      "iteration 700 / 2000: training loss nan val loss: nan\n",
      "iteration 750 / 2000: training loss nan val loss: nan\n",
      "iteration 800 / 2000: training loss nan val loss: nan\n",
      "iteration 850 / 2000: training loss nan val loss: nan\n",
      "iteration 900 / 2000: training loss nan val loss: nan\n",
      "iteration 950 / 2000: training loss nan val loss: nan\n",
      "iteration 1000 / 2000: training loss nan val loss: nan\n",
      "iteration 1050 / 2000: training loss nan val loss: nan\n",
      "iteration 1100 / 2000: training loss nan val loss: nan\n",
      "iteration 1150 / 2000: training loss nan val loss: nan\n",
      "iteration 1200 / 2000: training loss nan val loss: nan\n",
      "iteration 1250 / 2000: training loss nan val loss: nan\n",
      "iteration 1300 / 2000: training loss nan val loss: nan\n",
      "iteration 1350 / 2000: training loss nan val loss: nan\n",
      "iteration 1400 / 2000: training loss nan val loss: nan\n",
      "iteration 1450 / 2000: training loss nan val loss: nan\n",
      "iteration 1500 / 2000: training loss nan val loss: nan\n",
      "iteration 1550 / 2000: training loss nan val loss: nan\n",
      "iteration 1600 / 2000: training loss nan val loss: nan\n",
      "iteration 1650 / 2000: training loss nan val loss: nan\n",
      "iteration 1700 / 2000: training loss nan val loss: nan\n",
      "iteration 1750 / 2000: training loss nan val loss: nan\n",
      "iteration 1800 / 2000: training loss nan val loss: nan\n",
      "iteration 1850 / 2000: training loss nan val loss: nan\n",
      "iteration 1900 / 2000: training loss nan val loss: nan\n",
      "iteration 1950 / 2000: training loss nan val loss: nan\n",
      "hidden_size: 150 lr: 9.000000e-01 lr_decay: 9.800000e-01 reg: 4.000000e-02 train accuracy: 0.080000 val accuracy: 0.100000\n",
      "iteration 0 / 2000: training loss 2.302593 val loss: 2.303023\n",
      "iteration 50 / 2000: training loss 6.410197 val loss: 6.307585\n",
      "iteration 100 / 2000: training loss 3.328703 val loss: 3.394814\n",
      "iteration 150 / 2000: training loss 2.800134 val loss: 2.683464\n",
      "iteration 200 / 2000: training loss 2.424166 val loss: 2.270773\n",
      "iteration 250 / 2000: training loss 2.530022 val loss: 2.833857\n",
      "iteration 300 / 2000: training loss 1.631617 val loss: 1.617346\n",
      "iteration 350 / 2000: training loss 1.866484 val loss: 1.744556\n",
      "iteration 400 / 2000: training loss 1.465303 val loss: 1.620523\n",
      "iteration 450 / 2000: training loss 1.687079 val loss: 1.410333\n",
      "iteration 500 / 2000: training loss 1.594185 val loss: 1.688483\n",
      "iteration 550 / 2000: training loss 1.506592 val loss: 1.382854\n",
      "iteration 600 / 2000: training loss 1.472466 val loss: 1.420028\n",
      "iteration 650 / 2000: training loss 1.258072 val loss: 1.289674\n",
      "iteration 700 / 2000: training loss 1.504140 val loss: 1.571136\n",
      "iteration 750 / 2000: training loss 1.266000 val loss: 1.209489\n",
      "iteration 800 / 2000: training loss 1.286495 val loss: 1.335906\n",
      "iteration 850 / 2000: training loss 1.281476 val loss: 1.169634\n",
      "iteration 900 / 2000: training loss 1.259235 val loss: 1.352722\n",
      "iteration 950 / 2000: training loss 1.103495 val loss: 1.249922\n",
      "iteration 1000 / 2000: training loss 1.166157 val loss: 1.253156\n",
      "iteration 1050 / 2000: training loss 1.106275 val loss: 1.210030\n",
      "iteration 1100 / 2000: training loss 1.171466 val loss: 1.115311\n",
      "iteration 1150 / 2000: training loss 1.139104 val loss: 1.151297\n",
      "iteration 1200 / 2000: training loss 1.148479 val loss: 1.124742\n",
      "iteration 1250 / 2000: training loss 1.246133 val loss: 1.156520\n",
      "iteration 1300 / 2000: training loss 1.273401 val loss: 1.148318\n",
      "iteration 1350 / 2000: training loss 1.205173 val loss: 1.196433\n",
      "hidden_size: 150 lr: 9.000000e-01 lr_decay: 9.000000e-01 reg: 4.000000e-02 train accuracy: 0.840000 val accuracy: 0.809000\n",
      "iteration 0 / 2000: training loss 2.302593 val loss: 2.303023\n",
      "iteration 50 / 2000: training loss 2.924614 val loss: 2.842288\n",
      "iteration 100 / 2000: training loss 2.393016 val loss: 2.528571\n",
      "iteration 150 / 2000: training loss 2.148454 val loss: 2.258989\n",
      "iteration 200 / 2000: training loss 1.655896 val loss: 1.530412\n",
      "iteration 250 / 2000: training loss 1.700139 val loss: 1.740168\n",
      "iteration 300 / 2000: training loss 1.476767 val loss: 1.606717\n",
      "iteration 350 / 2000: training loss 1.357228 val loss: 1.289376\n",
      "hidden_size: 150 lr: 9.000000e-01 lr_decay: 8.500000e-01 reg: 4.000000e-02 train accuracy: 0.810000 val accuracy: 0.716000\n",
      "iteration 0 / 2000: training loss 2.302593 val loss: 2.303023\n",
      "iteration 50 / 2000: training loss 8.330533 val loss: 7.766315\n",
      "iteration 100 / 2000: training loss 2.762794 val loss: 2.683744\n",
      "iteration 150 / 2000: training loss 2.038898 val loss: 1.920413\n",
      "iteration 200 / 2000: training loss 1.346026 val loss: 1.430085\n",
      "iteration 250 / 2000: training loss 1.468991 val loss: 1.588610\n",
      "iteration 300 / 2000: training loss 1.258537 val loss: 1.452622\n",
      "iteration 350 / 2000: training loss 1.278739 val loss: 1.225989\n",
      "iteration 400 / 2000: training loss 1.240160 val loss: 1.151428\n",
      "iteration 450 / 2000: training loss 1.223763 val loss: 1.216047\n",
      "iteration 500 / 2000: training loss 1.096319 val loss: 1.119114\n",
      "iteration 550 / 2000: training loss 1.078481 val loss: 1.129099\n",
      "iteration 600 / 2000: training loss 1.089509 val loss: 1.139406\n",
      "iteration 650 / 2000: training loss 0.992726 val loss: 1.075623\n",
      "iteration 700 / 2000: training loss 1.113968 val loss: 1.134155\n",
      "iteration 750 / 2000: training loss 1.111368 val loss: 1.071213\n",
      "iteration 800 / 2000: training loss 1.079127 val loss: 1.070719\n",
      "iteration 850 / 2000: training loss 1.076865 val loss: 1.075377\n",
      "iteration 900 / 2000: training loss 1.076606 val loss: 1.055329\n",
      "iteration 950 / 2000: training loss 1.013473 val loss: 1.051306\n",
      "iteration 1000 / 2000: training loss 1.028491 val loss: 1.053389\n",
      "iteration 1050 / 2000: training loss 1.024476 val loss: 1.048784\n",
      "hidden_size: 150 lr: 9.000000e-01 lr_decay: 8.000000e-01 reg: 4.000000e-02 train accuracy: 0.870000 val accuracy: 0.902000\n",
      "iteration 0 / 2000: training loss 2.302597 val loss: 2.303026\n",
      "iteration 50 / 2000: training loss 15.481614 val loss: 13.880686\n",
      "iteration 100 / 2000: training loss nan val loss: nan\n",
      "iteration 150 / 2000: training loss nan val loss: nan\n",
      "iteration 200 / 2000: training loss nan val loss: nan\n",
      "iteration 250 / 2000: training loss nan val loss: nan\n",
      "iteration 300 / 2000: training loss nan val loss: nan\n",
      "iteration 350 / 2000: training loss nan val loss: nan\n",
      "iteration 400 / 2000: training loss nan val loss: nan\n",
      "iteration 450 / 2000: training loss nan val loss: nan\n",
      "iteration 500 / 2000: training loss nan val loss: nan\n",
      "iteration 550 / 2000: training loss nan val loss: nan\n",
      "iteration 600 / 2000: training loss nan val loss: nan\n",
      "iteration 650 / 2000: training loss nan val loss: nan\n",
      "iteration 700 / 2000: training loss nan val loss: nan\n",
      "iteration 750 / 2000: training loss nan val loss: nan\n",
      "iteration 800 / 2000: training loss nan val loss: nan\n",
      "iteration 850 / 2000: training loss nan val loss: nan\n",
      "iteration 900 / 2000: training loss nan val loss: nan\n",
      "iteration 950 / 2000: training loss nan val loss: nan\n",
      "iteration 1000 / 2000: training loss nan val loss: nan\n",
      "iteration 1050 / 2000: training loss nan val loss: nan\n",
      "iteration 1100 / 2000: training loss nan val loss: nan\n",
      "iteration 1150 / 2000: training loss nan val loss: nan\n",
      "iteration 1200 / 2000: training loss nan val loss: nan\n",
      "iteration 1250 / 2000: training loss nan val loss: nan\n",
      "iteration 1300 / 2000: training loss nan val loss: nan\n",
      "iteration 1350 / 2000: training loss nan val loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1400 / 2000: training loss nan val loss: nan\n",
      "iteration 1450 / 2000: training loss nan val loss: nan\n",
      "iteration 1500 / 2000: training loss nan val loss: nan\n",
      "iteration 1550 / 2000: training loss nan val loss: nan\n",
      "iteration 1600 / 2000: training loss nan val loss: nan\n",
      "iteration 1650 / 2000: training loss nan val loss: nan\n",
      "iteration 1700 / 2000: training loss nan val loss: nan\n",
      "iteration 1750 / 2000: training loss nan val loss: nan\n",
      "iteration 1800 / 2000: training loss nan val loss: nan\n",
      "iteration 1850 / 2000: training loss nan val loss: nan\n",
      "iteration 1900 / 2000: training loss nan val loss: nan\n",
      "iteration 1950 / 2000: training loss nan val loss: nan\n",
      "hidden_size: 150 lr: 9.000000e-01 lr_decay: 9.800000e-01 reg: 6.000000e-02 train accuracy: 0.080000 val accuracy: 0.100000\n",
      "iteration 0 / 2000: training loss 2.302597 val loss: 2.303026\n",
      "iteration 50 / 2000: training loss 4.091074 val loss: 4.431315\n",
      "iteration 100 / 2000: training loss 10.262413 val loss: 9.630138\n",
      "iteration 150 / 2000: training loss nan val loss: nan\n",
      "iteration 200 / 2000: training loss nan val loss: nan\n",
      "iteration 250 / 2000: training loss nan val loss: nan\n",
      "iteration 300 / 2000: training loss nan val loss: nan\n",
      "iteration 350 / 2000: training loss nan val loss: nan\n",
      "iteration 400 / 2000: training loss nan val loss: nan\n",
      "iteration 450 / 2000: training loss nan val loss: nan\n",
      "iteration 500 / 2000: training loss nan val loss: nan\n",
      "iteration 550 / 2000: training loss nan val loss: nan\n",
      "iteration 600 / 2000: training loss nan val loss: nan\n",
      "iteration 650 / 2000: training loss nan val loss: nan\n",
      "iteration 700 / 2000: training loss nan val loss: nan\n",
      "iteration 750 / 2000: training loss nan val loss: nan\n",
      "iteration 800 / 2000: training loss nan val loss: nan\n",
      "iteration 850 / 2000: training loss nan val loss: nan\n",
      "iteration 900 / 2000: training loss nan val loss: nan\n",
      "iteration 950 / 2000: training loss nan val loss: nan\n",
      "iteration 1000 / 2000: training loss nan val loss: nan\n",
      "iteration 1050 / 2000: training loss nan val loss: nan\n",
      "iteration 1100 / 2000: training loss nan val loss: nan\n",
      "iteration 1150 / 2000: training loss nan val loss: nan\n",
      "iteration 1200 / 2000: training loss nan val loss: nan\n",
      "iteration 1250 / 2000: training loss nan val loss: nan\n",
      "iteration 1300 / 2000: training loss nan val loss: nan\n",
      "iteration 1350 / 2000: training loss nan val loss: nan\n",
      "iteration 1400 / 2000: training loss nan val loss: nan\n",
      "iteration 1450 / 2000: training loss nan val loss: nan\n",
      "iteration 1500 / 2000: training loss nan val loss: nan\n",
      "iteration 1550 / 2000: training loss nan val loss: nan\n",
      "iteration 1600 / 2000: training loss nan val loss: nan\n",
      "iteration 1650 / 2000: training loss nan val loss: nan\n",
      "iteration 1700 / 2000: training loss nan val loss: nan\n",
      "iteration 1750 / 2000: training loss nan val loss: nan\n",
      "iteration 1800 / 2000: training loss nan val loss: nan\n",
      "iteration 1850 / 2000: training loss nan val loss: nan\n",
      "iteration 1900 / 2000: training loss nan val loss: nan\n",
      "iteration 1950 / 2000: training loss nan val loss: nan\n",
      "hidden_size: 150 lr: 9.000000e-01 lr_decay: 9.000000e-01 reg: 6.000000e-02 train accuracy: 0.080000 val accuracy: 0.100000\n",
      "iteration 0 / 2000: training loss 2.302597 val loss: 2.303026\n",
      "iteration 50 / 2000: training loss 4.130322 val loss: 4.213963\n",
      "iteration 100 / 2000: training loss 2.584081 val loss: 2.734590\n",
      "iteration 150 / 2000: training loss 2.689529 val loss: 3.037737\n",
      "iteration 200 / 2000: training loss 2.004241 val loss: 2.044306\n",
      "iteration 250 / 2000: training loss 2.071225 val loss: 1.895155\n",
      "iteration 300 / 2000: training loss 1.794513 val loss: 1.726428\n",
      "iteration 350 / 2000: training loss 1.581354 val loss: 1.729513\n",
      "iteration 400 / 2000: training loss 1.702552 val loss: 1.538884\n",
      "iteration 450 / 2000: training loss 1.579761 val loss: 1.583518\n",
      "iteration 500 / 2000: training loss 1.438158 val loss: 1.522790\n",
      "iteration 550 / 2000: training loss 1.401246 val loss: 1.484380\n",
      "iteration 600 / 2000: training loss 1.368767 val loss: 1.535756\n",
      "iteration 650 / 2000: training loss 1.398285 val loss: 1.388343\n",
      "iteration 700 / 2000: training loss 1.418890 val loss: 1.462831\n",
      "iteration 750 / 2000: training loss 1.334998 val loss: 1.367550\n",
      "iteration 800 / 2000: training loss 1.338346 val loss: 1.354260\n",
      "iteration 850 / 2000: training loss 1.391085 val loss: 1.344249\n",
      "iteration 900 / 2000: training loss 1.365832 val loss: 1.356879\n",
      "iteration 950 / 2000: training loss 1.243846 val loss: 1.359148\n",
      "iteration 1000 / 2000: training loss 1.336363 val loss: 1.339106\n",
      "iteration 1050 / 2000: training loss 1.286549 val loss: 1.311278\n",
      "iteration 1100 / 2000: training loss 1.297415 val loss: 1.309853\n",
      "iteration 1150 / 2000: training loss 1.245434 val loss: 1.334804\n",
      "iteration 1200 / 2000: training loss 1.316888 val loss: 1.292771\n",
      "iteration 1250 / 2000: training loss 1.336576 val loss: 1.322892\n",
      "iteration 1300 / 2000: training loss 1.297384 val loss: 1.298439\n",
      "iteration 1350 / 2000: training loss 1.346009 val loss: 1.291484\n",
      "iteration 1400 / 2000: training loss 1.291425 val loss: 1.283379\n",
      "iteration 1450 / 2000: training loss 1.349236 val loss: 1.280845\n",
      "iteration 1500 / 2000: training loss 1.285214 val loss: 1.278271\n",
      "iteration 1550 / 2000: training loss 1.284765 val loss: 1.290193\n",
      "iteration 1600 / 2000: training loss 1.307227 val loss: 1.285515\n",
      "iteration 1650 / 2000: training loss 1.274776 val loss: 1.285117\n",
      "iteration 1700 / 2000: training loss 1.235503 val loss: 1.272303\n",
      "iteration 1750 / 2000: training loss 1.177728 val loss: 1.262690\n",
      "iteration 1800 / 2000: training loss 1.316221 val loss: 1.282378\n",
      "hidden_size: 150 lr: 9.000000e-01 lr_decay: 8.500000e-01 reg: 6.000000e-02 train accuracy: 0.870000 val accuracy: 0.886000\n",
      "iteration 0 / 2000: training loss 2.302597 val loss: 2.303026\n",
      "iteration 50 / 2000: training loss 4.558087 val loss: 4.003354\n",
      "iteration 100 / 2000: training loss 4.790436 val loss: 4.772871\n",
      "iteration 150 / 2000: training loss 2.066088 val loss: 2.174557\n",
      "iteration 200 / 2000: training loss 1.750259 val loss: 1.986719\n",
      "iteration 250 / 2000: training loss 1.520287 val loss: 1.812862\n",
      "iteration 300 / 2000: training loss 1.584985 val loss: 1.666875\n",
      "iteration 350 / 2000: training loss 1.599763 val loss: 1.455345\n",
      "iteration 400 / 2000: training loss 1.507816 val loss: 1.477404\n",
      "iteration 450 / 2000: training loss 1.487148 val loss: 1.503059\n",
      "iteration 500 / 2000: training loss 1.342633 val loss: 1.365991\n",
      "iteration 550 / 2000: training loss 1.306619 val loss: 1.367005\n",
      "iteration 600 / 2000: training loss 1.351616 val loss: 1.374055\n",
      "iteration 650 / 2000: training loss 1.224401 val loss: 1.308555\n",
      "iteration 700 / 2000: training loss 1.345311 val loss: 1.403673\n",
      "iteration 750 / 2000: training loss 1.380290 val loss: 1.325083\n",
      "iteration 800 / 2000: training loss 1.310346 val loss: 1.305683\n",
      "iteration 850 / 2000: training loss 1.335782 val loss: 1.319167\n",
      "iteration 900 / 2000: training loss 1.301599 val loss: 1.306161\n",
      "iteration 950 / 2000: training loss 1.259175 val loss: 1.296273\n",
      "iteration 1000 / 2000: training loss 1.270951 val loss: 1.302465\n",
      "iteration 1050 / 2000: training loss 1.259901 val loss: 1.277500\n",
      "iteration 1100 / 2000: training loss 1.267215 val loss: 1.278014\n",
      "iteration 1150 / 2000: training loss 1.221812 val loss: 1.296580\n",
      "iteration 1200 / 2000: training loss 1.310001 val loss: 1.280149\n",
      "iteration 1250 / 2000: training loss 1.319294 val loss: 1.284166\n",
      "iteration 1300 / 2000: training loss 1.252654 val loss: 1.276607\n",
      "iteration 1350 / 2000: training loss 1.366729 val loss: 1.284149\n",
      "iteration 1400 / 2000: training loss 1.288836 val loss: 1.268495\n",
      "iteration 1450 / 2000: training loss 1.332560 val loss: 1.268256\n",
      "iteration 1500 / 2000: training loss 1.253060 val loss: 1.262301\n",
      "iteration 1550 / 2000: training loss 1.253760 val loss: 1.267526\n",
      "iteration 1600 / 2000: training loss 1.293718 val loss: 1.259696\n",
      "iteration 1650 / 2000: training loss 1.271225 val loss: 1.256323\n",
      "iteration 1700 / 2000: training loss 1.221315 val loss: 1.261090\n",
      "hidden_size: 150 lr: 9.000000e-01 lr_decay: 8.000000e-01 reg: 6.000000e-02 train accuracy: 0.890000 val accuracy: 0.906000\n",
      "iteration 0 / 2000: training loss 2.302588 val loss: 2.302593\n",
      "iteration 50 / 2000: training loss 1.915939 val loss: 1.860117\n",
      "iteration 100 / 2000: training loss 0.687376 val loss: 0.775758\n",
      "iteration 150 / 2000: training loss 0.470918 val loss: 0.578262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 200 / 2000: training loss 0.502311 val loss: 0.573682\n",
      "iteration 250 / 2000: training loss 0.473153 val loss: 0.558215\n",
      "iteration 300 / 2000: training loss 0.494592 val loss: 0.564264\n",
      "iteration 350 / 2000: training loss 0.529256 val loss: 0.566623\n",
      "iteration 400 / 2000: training loss 0.499987 val loss: 0.540072\n",
      "iteration 450 / 2000: training loss 0.537490 val loss: 0.559291\n",
      "iteration 500 / 2000: training loss 0.523550 val loss: 0.529911\n",
      "iteration 550 / 2000: training loss 0.517863 val loss: 0.529255\n",
      "iteration 600 / 2000: training loss 0.445129 val loss: 0.533574\n",
      "iteration 650 / 2000: training loss 0.423826 val loss: 0.523445\n",
      "iteration 700 / 2000: training loss 0.545399 val loss: 0.530258\n",
      "iteration 750 / 2000: training loss 0.507764 val loss: 0.538050\n",
      "iteration 800 / 2000: training loss 0.510853 val loss: 0.541434\n",
      "iteration 850 / 2000: training loss 0.508914 val loss: 0.544996\n",
      "iteration 900 / 2000: training loss 0.483670 val loss: 0.532327\n",
      "iteration 950 / 2000: training loss 0.492492 val loss: 0.520097\n",
      "iteration 1000 / 2000: training loss 0.488343 val loss: 0.531429\n",
      "iteration 1050 / 2000: training loss 0.492918 val loss: 0.537092\n",
      "hidden_size: 200 lr: 1.000000e-01 lr_decay: 9.800000e-01 reg: 1.000000e-02 train accuracy: 0.920000 val accuracy: 0.934000\n",
      "iteration 0 / 2000: training loss 2.302588 val loss: 2.302593\n",
      "iteration 50 / 2000: training loss 2.005197 val loss: 1.970567\n",
      "iteration 100 / 2000: training loss 0.670296 val loss: 0.754241\n",
      "iteration 150 / 2000: training loss 0.458521 val loss: 0.570374\n",
      "iteration 200 / 2000: training loss 0.501638 val loss: 0.569333\n",
      "iteration 250 / 2000: training loss 0.464625 val loss: 0.547078\n",
      "iteration 300 / 2000: training loss 0.488142 val loss: 0.566962\n",
      "iteration 350 / 2000: training loss 0.527624 val loss: 0.538557\n",
      "iteration 400 / 2000: training loss 0.509950 val loss: 0.526918\n",
      "iteration 450 / 2000: training loss 0.523788 val loss: 0.537329\n",
      "iteration 500 / 2000: training loss 0.492441 val loss: 0.524935\n",
      "iteration 550 / 2000: training loss 0.524034 val loss: 0.520703\n",
      "iteration 600 / 2000: training loss 0.434358 val loss: 0.525946\n",
      "hidden_size: 200 lr: 1.000000e-01 lr_decay: 9.000000e-01 reg: 1.000000e-02 train accuracy: 0.960000 val accuracy: 0.930000\n",
      "iteration 0 / 2000: training loss 2.302588 val loss: 2.302593\n",
      "iteration 50 / 2000: training loss 2.069577 val loss: 2.045264\n",
      "iteration 100 / 2000: training loss 0.656885 val loss: 0.725673\n",
      "iteration 150 / 2000: training loss 0.456111 val loss: 0.575102\n",
      "iteration 200 / 2000: training loss 0.504309 val loss: 0.561117\n",
      "iteration 250 / 2000: training loss 0.469838 val loss: 0.549596\n",
      "iteration 300 / 2000: training loss 0.474492 val loss: 0.562656\n",
      "iteration 350 / 2000: training loss 0.537381 val loss: 0.536985\n",
      "iteration 400 / 2000: training loss 0.526204 val loss: 0.534324\n",
      "iteration 450 / 2000: training loss 0.530346 val loss: 0.532867\n",
      "iteration 500 / 2000: training loss 0.488727 val loss: 0.528065\n",
      "iteration 550 / 2000: training loss 0.521380 val loss: 0.530469\n",
      "iteration 600 / 2000: training loss 0.428148 val loss: 0.520724\n",
      "hidden_size: 200 lr: 1.000000e-01 lr_decay: 8.500000e-01 reg: 1.000000e-02 train accuracy: 0.940000 val accuracy: 0.932000\n",
      "iteration 0 / 2000: training loss 2.302588 val loss: 2.302593\n",
      "iteration 50 / 2000: training loss 2.133992 val loss: 2.112858\n",
      "iteration 100 / 2000: training loss 0.656757 val loss: 0.716488\n",
      "iteration 150 / 2000: training loss 0.455985 val loss: 0.583985\n",
      "iteration 200 / 2000: training loss 0.506517 val loss: 0.563398\n",
      "iteration 250 / 2000: training loss 0.470946 val loss: 0.555130\n",
      "iteration 300 / 2000: training loss 0.463470 val loss: 0.556082\n",
      "iteration 350 / 2000: training loss 0.547920 val loss: 0.540475\n",
      "iteration 400 / 2000: training loss 0.531564 val loss: 0.539844\n",
      "iteration 450 / 2000: training loss 0.530094 val loss: 0.530346\n",
      "iteration 500 / 2000: training loss 0.492052 val loss: 0.525004\n",
      "iteration 550 / 2000: training loss 0.511927 val loss: 0.530995\n",
      "iteration 600 / 2000: training loss 0.431621 val loss: 0.519914\n",
      "iteration 650 / 2000: training loss 0.426945 val loss: 0.523735\n",
      "iteration 700 / 2000: training loss 0.522832 val loss: 0.526678\n",
      "iteration 750 / 2000: training loss 0.502147 val loss: 0.516765\n",
      "iteration 800 / 2000: training loss 0.547808 val loss: 0.526658\n",
      "iteration 850 / 2000: training loss 0.509492 val loss: 0.518679\n",
      "iteration 900 / 2000: training loss 0.482507 val loss: 0.517088\n",
      "iteration 950 / 2000: training loss 0.447719 val loss: 0.518163\n",
      "iteration 1000 / 2000: training loss 0.480748 val loss: 0.516632\n",
      "iteration 1050 / 2000: training loss 0.489760 val loss: 0.515644\n",
      "iteration 1100 / 2000: training loss 0.507878 val loss: 0.516321\n",
      "iteration 1150 / 2000: training loss 0.471876 val loss: 0.514816\n",
      "iteration 1200 / 2000: training loss 0.500898 val loss: 0.515467\n",
      "iteration 1250 / 2000: training loss 0.539264 val loss: 0.515160\n",
      "iteration 1300 / 2000: training loss 0.505498 val loss: 0.514583\n",
      "iteration 1350 / 2000: training loss 0.516074 val loss: 0.515315\n",
      "iteration 1400 / 2000: training loss 0.455228 val loss: 0.514057\n",
      "iteration 1450 / 2000: training loss 0.523433 val loss: 0.514748\n",
      "iteration 1500 / 2000: training loss 0.475650 val loss: 0.513814\n",
      "iteration 1550 / 2000: training loss 0.545765 val loss: 0.514525\n",
      "iteration 1600 / 2000: training loss 0.514703 val loss: 0.514517\n",
      "iteration 1650 / 2000: training loss 0.492884 val loss: 0.513132\n",
      "iteration 1700 / 2000: training loss 0.470852 val loss: 0.513498\n",
      "iteration 1750 / 2000: training loss 0.426833 val loss: 0.512845\n",
      "iteration 1800 / 2000: training loss 0.536899 val loss: 0.513983\n",
      "iteration 1850 / 2000: training loss 0.488318 val loss: 0.513086\n",
      "iteration 1900 / 2000: training loss 0.417713 val loss: 0.513529\n",
      "iteration 1950 / 2000: training loss 0.496136 val loss: 0.513111\n",
      "hidden_size: 200 lr: 1.000000e-01 lr_decay: 8.000000e-01 reg: 1.000000e-02 train accuracy: 0.950000 val accuracy: 0.940000\n",
      "iteration 0 / 2000: training loss 2.302590 val loss: 2.302596\n",
      "iteration 50 / 2000: training loss 1.958411 val loss: 1.913566\n",
      "iteration 100 / 2000: training loss 0.916094 val loss: 0.955593\n",
      "iteration 150 / 2000: training loss 0.705813 val loss: 0.763597\n",
      "iteration 200 / 2000: training loss 0.723586 val loss: 0.767865\n",
      "iteration 250 / 2000: training loss 0.701404 val loss: 0.778737\n",
      "iteration 300 / 2000: training loss 0.693395 val loss: 0.765021\n",
      "iteration 350 / 2000: training loss 0.755546 val loss: 0.787069\n",
      "iteration 400 / 2000: training loss 0.724132 val loss: 0.737334\n",
      "iteration 450 / 2000: training loss 0.810898 val loss: 0.778079\n",
      "iteration 500 / 2000: training loss 0.742810 val loss: 0.749349\n",
      "iteration 550 / 2000: training loss 0.727918 val loss: 0.751771\n",
      "iteration 600 / 2000: training loss 0.657006 val loss: 0.745365\n",
      "iteration 650 / 2000: training loss 0.658153 val loss: 0.758403\n",
      "iteration 700 / 2000: training loss 0.756163 val loss: 0.733826\n",
      "iteration 750 / 2000: training loss 0.733138 val loss: 0.732878\n",
      "iteration 800 / 2000: training loss 0.751139 val loss: 0.748150\n",
      "iteration 850 / 2000: training loss 0.756240 val loss: 0.771987\n",
      "iteration 900 / 2000: training loss 0.735747 val loss: 0.743842\n",
      "iteration 950 / 2000: training loss 0.701745 val loss: 0.743152\n",
      "iteration 1000 / 2000: training loss 0.712615 val loss: 0.753152\n",
      "iteration 1050 / 2000: training loss 0.723618 val loss: 0.748998\n",
      "iteration 1100 / 2000: training loss 0.750926 val loss: 0.745713\n",
      "iteration 1150 / 2000: training loss 0.687005 val loss: 0.746592\n",
      "iteration 1200 / 2000: training loss 0.735699 val loss: 0.744328\n",
      "iteration 1250 / 2000: training loss 0.812343 val loss: 0.749956\n",
      "iteration 1300 / 2000: training loss 0.757417 val loss: 0.750439\n",
      "iteration 1350 / 2000: training loss 0.789587 val loss: 0.735351\n",
      "iteration 1400 / 2000: training loss 0.687353 val loss: 0.734033\n",
      "iteration 1450 / 2000: training loss 0.774026 val loss: 0.758780\n",
      "iteration 1500 / 2000: training loss 0.720686 val loss: 0.724632\n",
      "iteration 1550 / 2000: training loss 0.759465 val loss: 0.744340\n",
      "iteration 1600 / 2000: training loss 0.747217 val loss: 0.745305\n",
      "iteration 1650 / 2000: training loss 0.724101 val loss: 0.724399\n",
      "iteration 1700 / 2000: training loss 0.665615 val loss: 0.728536\n",
      "hidden_size: 200 lr: 1.000000e-01 lr_decay: 9.800000e-01 reg: 2.000000e-02 train accuracy: 0.920000 val accuracy: 0.921000\n",
      "iteration 0 / 2000: training loss 2.302590 val loss: 2.302596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 50 / 2000: training loss 2.043531 val loss: 2.017194\n",
      "iteration 100 / 2000: training loss 0.883863 val loss: 0.937607\n",
      "iteration 150 / 2000: training loss 0.696212 val loss: 0.774532\n",
      "iteration 200 / 2000: training loss 0.719713 val loss: 0.770892\n",
      "iteration 250 / 2000: training loss 0.675010 val loss: 0.756768\n",
      "iteration 300 / 2000: training loss 0.698276 val loss: 0.770013\n",
      "iteration 350 / 2000: training loss 0.750691 val loss: 0.751932\n",
      "iteration 400 / 2000: training loss 0.729653 val loss: 0.741299\n",
      "iteration 450 / 2000: training loss 0.750854 val loss: 0.745082\n",
      "iteration 500 / 2000: training loss 0.718209 val loss: 0.728512\n",
      "iteration 550 / 2000: training loss 0.719754 val loss: 0.724003\n",
      "iteration 600 / 2000: training loss 0.648852 val loss: 0.732466\n",
      "iteration 650 / 2000: training loss 0.638600 val loss: 0.726622\n",
      "iteration 700 / 2000: training loss 0.741393 val loss: 0.738328\n",
      "iteration 750 / 2000: training loss 0.723413 val loss: 0.728128\n",
      "iteration 800 / 2000: training loss 0.720340 val loss: 0.740850\n",
      "iteration 850 / 2000: training loss 0.740418 val loss: 0.723256\n",
      "iteration 900 / 2000: training loss 0.693804 val loss: 0.726918\n",
      "iteration 950 / 2000: training loss 0.678468 val loss: 0.719113\n",
      "iteration 1000 / 2000: training loss 0.698466 val loss: 0.718331\n",
      "iteration 1050 / 2000: training loss 0.701738 val loss: 0.718594\n",
      "iteration 1100 / 2000: training loss 0.711313 val loss: 0.723844\n",
      "iteration 1150 / 2000: training loss 0.675760 val loss: 0.722391\n",
      "iteration 1200 / 2000: training loss 0.743895 val loss: 0.725116\n",
      "iteration 1250 / 2000: training loss 0.769726 val loss: 0.727647\n",
      "iteration 1300 / 2000: training loss 0.720575 val loss: 0.720488\n",
      "iteration 1350 / 2000: training loss 0.762755 val loss: 0.726400\n",
      "iteration 1400 / 2000: training loss 0.680619 val loss: 0.717121\n",
      "iteration 1450 / 2000: training loss 0.763327 val loss: 0.723042\n",
      "iteration 1500 / 2000: training loss 0.691936 val loss: 0.718455\n",
      "iteration 1550 / 2000: training loss 0.749006 val loss: 0.720186\n",
      "iteration 1600 / 2000: training loss 0.736638 val loss: 0.715535\n",
      "iteration 1650 / 2000: training loss 0.717904 val loss: 0.712422\n",
      "iteration 1700 / 2000: training loss 0.667800 val loss: 0.713166\n",
      "iteration 1750 / 2000: training loss 0.637714 val loss: 0.713840\n",
      "iteration 1800 / 2000: training loss 0.758108 val loss: 0.713972\n",
      "iteration 1850 / 2000: training loss 0.714430 val loss: 0.713231\n",
      "hidden_size: 200 lr: 1.000000e-01 lr_decay: 9.000000e-01 reg: 2.000000e-02 train accuracy: 0.920000 val accuracy: 0.931000\n",
      "iteration 0 / 2000: training loss 2.302590 val loss: 2.302596\n",
      "iteration 50 / 2000: training loss 2.102642 val loss: 2.082927\n",
      "iteration 100 / 2000: training loss 0.867082 val loss: 0.914827\n",
      "iteration 150 / 2000: training loss 0.689273 val loss: 0.778307\n",
      "iteration 200 / 2000: training loss 0.712817 val loss: 0.770483\n",
      "iteration 250 / 2000: training loss 0.673956 val loss: 0.750372\n",
      "iteration 300 / 2000: training loss 0.687308 val loss: 0.768257\n",
      "iteration 350 / 2000: training loss 0.743381 val loss: 0.743648\n",
      "iteration 400 / 2000: training loss 0.744302 val loss: 0.737815\n",
      "iteration 450 / 2000: training loss 0.755230 val loss: 0.741548\n",
      "iteration 500 / 2000: training loss 0.705188 val loss: 0.733795\n",
      "iteration 550 / 2000: training loss 0.728627 val loss: 0.731327\n",
      "iteration 600 / 2000: training loss 0.649175 val loss: 0.730767\n",
      "hidden_size: 200 lr: 1.000000e-01 lr_decay: 8.500000e-01 reg: 2.000000e-02 train accuracy: 0.940000 val accuracy: 0.922000\n",
      "iteration 0 / 2000: training loss 2.302590 val loss: 2.302596\n",
      "iteration 50 / 2000: training loss 2.161265 val loss: 2.142380\n",
      "iteration 100 / 2000: training loss 0.862639 val loss: 0.906063\n",
      "iteration 150 / 2000: training loss 0.683332 val loss: 0.784890\n",
      "iteration 200 / 2000: training loss 0.711552 val loss: 0.764772\n",
      "iteration 250 / 2000: training loss 0.678790 val loss: 0.753136\n",
      "iteration 300 / 2000: training loss 0.678700 val loss: 0.763488\n",
      "iteration 350 / 2000: training loss 0.753780 val loss: 0.742307\n",
      "iteration 400 / 2000: training loss 0.763531 val loss: 0.743017\n",
      "iteration 450 / 2000: training loss 0.758397 val loss: 0.736373\n",
      "iteration 500 / 2000: training loss 0.707996 val loss: 0.730037\n",
      "iteration 550 / 2000: training loss 0.720713 val loss: 0.735544\n",
      "iteration 600 / 2000: training loss 0.646759 val loss: 0.724323\n",
      "iteration 650 / 2000: training loss 0.640987 val loss: 0.728986\n",
      "iteration 700 / 2000: training loss 0.743533 val loss: 0.731128\n",
      "iteration 750 / 2000: training loss 0.724107 val loss: 0.722957\n",
      "iteration 800 / 2000: training loss 0.751451 val loss: 0.731130\n",
      "iteration 850 / 2000: training loss 0.736542 val loss: 0.723724\n",
      "iteration 900 / 2000: training loss 0.700138 val loss: 0.723109\n",
      "iteration 950 / 2000: training loss 0.664881 val loss: 0.722864\n",
      "iteration 1000 / 2000: training loss 0.700629 val loss: 0.721779\n",
      "iteration 1050 / 2000: training loss 0.702852 val loss: 0.721209\n",
      "iteration 1100 / 2000: training loss 0.708415 val loss: 0.721988\n",
      "iteration 1150 / 2000: training loss 0.681323 val loss: 0.720232\n",
      "iteration 1200 / 2000: training loss 0.726927 val loss: 0.720506\n",
      "iteration 1250 / 2000: training loss 0.751677 val loss: 0.720723\n",
      "iteration 1300 / 2000: training loss 0.726101 val loss: 0.720183\n",
      "iteration 1350 / 2000: training loss 0.738927 val loss: 0.720335\n",
      "iteration 1400 / 2000: training loss 0.684603 val loss: 0.719691\n",
      "iteration 1450 / 2000: training loss 0.743461 val loss: 0.719957\n",
      "iteration 1500 / 2000: training loss 0.690688 val loss: 0.719327\n",
      "hidden_size: 200 lr: 1.000000e-01 lr_decay: 8.000000e-01 reg: 2.000000e-02 train accuracy: 0.900000 val accuracy: 0.930000\n",
      "iteration 0 / 2000: training loss 2.302596 val loss: 2.302601\n",
      "iteration 50 / 2000: training loss 2.034032 val loss: 2.006808\n",
      "iteration 100 / 2000: training loss 1.211681 val loss: 1.209798\n",
      "iteration 150 / 2000: training loss 1.044932 val loss: 1.076307\n",
      "iteration 200 / 2000: training loss 1.037975 val loss: 1.097095\n",
      "iteration 250 / 2000: training loss 1.043309 val loss: 1.092101\n",
      "iteration 300 / 2000: training loss 1.005015 val loss: 1.074801\n",
      "iteration 350 / 2000: training loss 1.084450 val loss: 1.105498\n",
      "iteration 400 / 2000: training loss 1.094384 val loss: 1.049855\n",
      "iteration 450 / 2000: training loss 1.137987 val loss: 1.092231\n",
      "hidden_size: 200 lr: 1.000000e-01 lr_decay: 9.800000e-01 reg: 4.000000e-02 train accuracy: 0.910000 val accuracy: 0.890000\n",
      "iteration 0 / 2000: training loss 2.302596 val loss: 2.302601\n",
      "iteration 50 / 2000: training loss 2.109667 val loss: 2.094663\n",
      "iteration 100 / 2000: training loss 1.179386 val loss: 1.201788\n",
      "iteration 150 / 2000: training loss 1.024231 val loss: 1.080537\n",
      "iteration 200 / 2000: training loss 1.039997 val loss: 1.075504\n",
      "iteration 250 / 2000: training loss 0.992753 val loss: 1.070024\n",
      "iteration 300 / 2000: training loss 1.003246 val loss: 1.081175\n",
      "iteration 350 / 2000: training loss 1.081057 val loss: 1.070648\n",
      "iteration 400 / 2000: training loss 1.057031 val loss: 1.052285\n",
      "iteration 450 / 2000: training loss 1.092696 val loss: 1.052408\n",
      "iteration 500 / 2000: training loss 1.036030 val loss: 1.042253\n",
      "iteration 550 / 2000: training loss 1.015969 val loss: 1.036992\n",
      "iteration 600 / 2000: training loss 0.964396 val loss: 1.043947\n",
      "iteration 650 / 2000: training loss 0.950541 val loss: 1.036330\n",
      "iteration 700 / 2000: training loss 1.045956 val loss: 1.047747\n",
      "iteration 750 / 2000: training loss 1.036784 val loss: 1.038391\n",
      "iteration 800 / 2000: training loss 1.023991 val loss: 1.056619\n",
      "iteration 850 / 2000: training loss 1.067990 val loss: 1.031101\n",
      "iteration 900 / 2000: training loss 1.002444 val loss: 1.037195\n",
      "iteration 950 / 2000: training loss 0.988274 val loss: 1.029047\n",
      "iteration 1000 / 2000: training loss 1.015341 val loss: 1.029812\n",
      "iteration 1050 / 2000: training loss 1.016204 val loss: 1.024776\n",
      "iteration 1100 / 2000: training loss 1.012992 val loss: 1.030778\n",
      "iteration 1150 / 2000: training loss 0.986045 val loss: 1.033724\n",
      "iteration 1200 / 2000: training loss 1.069613 val loss: 1.037583\n",
      "iteration 1250 / 2000: training loss 1.080143 val loss: 1.031835\n",
      "iteration 1300 / 2000: training loss 1.035776 val loss: 1.030825\n",
      "iteration 1350 / 2000: training loss 1.112880 val loss: 1.044516\n",
      "hidden_size: 200 lr: 1.000000e-01 lr_decay: 9.000000e-01 reg: 4.000000e-02 train accuracy: 0.870000 val accuracy: 0.910000\n",
      "iteration 0 / 2000: training loss 2.302596 val loss: 2.302601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 50 / 2000: training loss 2.159806 val loss: 2.144892\n",
      "iteration 100 / 2000: training loss 1.167811 val loss: 1.192300\n",
      "iteration 150 / 2000: training loss 1.018202 val loss: 1.085472\n",
      "iteration 200 / 2000: training loss 1.042018 val loss: 1.077203\n",
      "iteration 250 / 2000: training loss 0.985548 val loss: 1.059787\n",
      "iteration 300 / 2000: training loss 0.995192 val loss: 1.076833\n",
      "iteration 350 / 2000: training loss 1.073127 val loss: 1.060942\n",
      "iteration 400 / 2000: training loss 1.070571 val loss: 1.050050\n",
      "iteration 450 / 2000: training loss 1.081352 val loss: 1.049175\n",
      "iteration 500 / 2000: training loss 1.020559 val loss: 1.041083\n",
      "iteration 550 / 2000: training loss 1.029642 val loss: 1.033030\n",
      "iteration 600 / 2000: training loss 0.967533 val loss: 1.038822\n",
      "iteration 650 / 2000: training loss 0.958054 val loss: 1.037629\n",
      "iteration 700 / 2000: training loss 1.045499 val loss: 1.039725\n",
      "iteration 750 / 2000: training loss 1.039978 val loss: 1.031463\n",
      "iteration 800 / 2000: training loss 1.036628 val loss: 1.044997\n",
      "iteration 850 / 2000: training loss 1.074286 val loss: 1.032612\n",
      "iteration 900 / 2000: training loss 1.010716 val loss: 1.035629\n",
      "iteration 950 / 2000: training loss 0.985965 val loss: 1.027638\n",
      "iteration 1000 / 2000: training loss 1.022752 val loss: 1.025934\n",
      "iteration 1050 / 2000: training loss 1.018372 val loss: 1.028014\n",
      "iteration 1100 / 2000: training loss 1.004407 val loss: 1.026982\n",
      "iteration 1150 / 2000: training loss 0.979565 val loss: 1.025060\n",
      "iteration 1200 / 2000: training loss 1.053442 val loss: 1.025070\n",
      "iteration 1250 / 2000: training loss 1.064180 val loss: 1.027984\n",
      "iteration 1300 / 2000: training loss 1.033038 val loss: 1.027282\n",
      "iteration 1350 / 2000: training loss 1.070639 val loss: 1.026272\n",
      "iteration 1400 / 2000: training loss 1.018855 val loss: 1.026369\n",
      "iteration 1450 / 2000: training loss 1.062190 val loss: 1.029654\n",
      "iteration 1500 / 2000: training loss 1.005021 val loss: 1.023319\n",
      "iteration 1550 / 2000: training loss 1.025445 val loss: 1.024288\n",
      "iteration 1600 / 2000: training loss 1.057429 val loss: 1.022008\n",
      "iteration 1650 / 2000: training loss 1.025432 val loss: 1.020518\n",
      "iteration 1700 / 2000: training loss 0.979353 val loss: 1.022889\n",
      "iteration 1750 / 2000: training loss 0.934053 val loss: 1.021349\n",
      "iteration 1800 / 2000: training loss 1.069536 val loss: 1.019908\n",
      "iteration 1850 / 2000: training loss 1.027006 val loss: 1.020712\n",
      "iteration 1900 / 2000: training loss 0.943415 val loss: 1.020592\n",
      "iteration 1950 / 2000: training loss 1.025978 val loss: 1.020181\n",
      "hidden_size: 200 lr: 1.000000e-01 lr_decay: 8.500000e-01 reg: 4.000000e-02 train accuracy: 0.910000 val accuracy: 0.924000\n",
      "iteration 0 / 2000: training loss 2.302596 val loss: 2.302601\n",
      "iteration 50 / 2000: training loss 2.206562 val loss: 2.191282\n",
      "iteration 100 / 2000: training loss 1.173360 val loss: 1.196144\n",
      "iteration 150 / 2000: training loss 1.014668 val loss: 1.089867\n",
      "iteration 200 / 2000: training loss 1.031916 val loss: 1.076433\n",
      "iteration 250 / 2000: training loss 0.989369 val loss: 1.059102\n",
      "iteration 300 / 2000: training loss 0.990450 val loss: 1.074761\n",
      "iteration 350 / 2000: training loss 1.065225 val loss: 1.053250\n",
      "iteration 400 / 2000: training loss 1.094582 val loss: 1.053356\n",
      "iteration 450 / 2000: training loss 1.085817 val loss: 1.044321\n",
      "iteration 500 / 2000: training loss 1.017041 val loss: 1.037743\n",
      "iteration 550 / 2000: training loss 1.029606 val loss: 1.040429\n",
      "iteration 600 / 2000: training loss 0.962683 val loss: 1.031296\n",
      "hidden_size: 200 lr: 1.000000e-01 lr_decay: 8.000000e-01 reg: 4.000000e-02 train accuracy: 0.940000 val accuracy: 0.906000\n",
      "iteration 0 / 2000: training loss 2.302601 val loss: 2.302606\n",
      "iteration 50 / 2000: training loss 2.098654 val loss: 2.085789\n",
      "iteration 100 / 2000: training loss 1.420023 val loss: 1.407942\n",
      "iteration 150 / 2000: training loss 1.279106 val loss: 1.315137\n",
      "iteration 200 / 2000: training loss 1.274196 val loss: 1.337341\n",
      "iteration 250 / 2000: training loss 1.277889 val loss: 1.327143\n",
      "iteration 300 / 2000: training loss 1.239719 val loss: 1.311933\n",
      "iteration 350 / 2000: training loss 1.338941 val loss: 1.343716\n",
      "iteration 400 / 2000: training loss 1.358888 val loss: 1.289966\n",
      "iteration 450 / 2000: training loss 1.385585 val loss: 1.332438\n",
      "hidden_size: 200 lr: 1.000000e-01 lr_decay: 9.800000e-01 reg: 6.000000e-02 train accuracy: 0.890000 val accuracy: 0.866000\n",
      "iteration 0 / 2000: training loss 2.302601 val loss: 2.302606\n",
      "iteration 50 / 2000: training loss 2.164326 val loss: 2.153721\n",
      "iteration 100 / 2000: training loss 1.403562 val loss: 1.412626\n",
      "iteration 150 / 2000: training loss 1.271412 val loss: 1.326210\n",
      "iteration 200 / 2000: training loss 1.276776 val loss: 1.319679\n",
      "iteration 250 / 2000: training loss 1.240053 val loss: 1.304507\n",
      "iteration 300 / 2000: training loss 1.243983 val loss: 1.321447\n",
      "iteration 350 / 2000: training loss 1.327694 val loss: 1.311705\n",
      "iteration 400 / 2000: training loss 1.302863 val loss: 1.285456\n",
      "iteration 450 / 2000: training loss 1.345855 val loss: 1.291264\n",
      "iteration 500 / 2000: training loss 1.267070 val loss: 1.283609\n",
      "iteration 550 / 2000: training loss 1.251529 val loss: 1.275718\n",
      "iteration 600 / 2000: training loss 1.209097 val loss: 1.285651\n",
      "iteration 650 / 2000: training loss 1.197194 val loss: 1.280257\n",
      "iteration 700 / 2000: training loss 1.273409 val loss: 1.283473\n",
      "iteration 750 / 2000: training loss 1.272790 val loss: 1.277178\n",
      "iteration 800 / 2000: training loss 1.257085 val loss: 1.294827\n",
      "iteration 850 / 2000: training loss 1.316183 val loss: 1.267391\n",
      "iteration 900 / 2000: training loss 1.234732 val loss: 1.275967\n",
      "iteration 950 / 2000: training loss 1.228832 val loss: 1.267919\n",
      "iteration 1000 / 2000: training loss 1.259223 val loss: 1.268799\n",
      "iteration 1050 / 2000: training loss 1.257497 val loss: 1.263051\n",
      "iteration 1100 / 2000: training loss 1.249769 val loss: 1.266294\n",
      "iteration 1150 / 2000: training loss 1.219258 val loss: 1.271179\n",
      "iteration 1200 / 2000: training loss 1.311460 val loss: 1.275854\n",
      "iteration 1250 / 2000: training loss 1.314206 val loss: 1.266127\n",
      "iteration 1300 / 2000: training loss 1.265132 val loss: 1.267872\n",
      "iteration 1350 / 2000: training loss 1.365052 val loss: 1.284956\n",
      "hidden_size: 200 lr: 1.000000e-01 lr_decay: 9.000000e-01 reg: 6.000000e-02 train accuracy: 0.830000 val accuracy: 0.891000\n",
      "iteration 0 / 2000: training loss 2.302601 val loss: 2.302606\n",
      "iteration 50 / 2000: training loss 2.205435 val loss: 2.192919\n",
      "iteration 100 / 2000: training loss 1.404829 val loss: 1.413297\n",
      "iteration 150 / 2000: training loss 1.263966 val loss: 1.324728\n",
      "iteration 200 / 2000: training loss 1.284724 val loss: 1.314639\n",
      "iteration 250 / 2000: training loss 1.229726 val loss: 1.298754\n",
      "iteration 300 / 2000: training loss 1.228863 val loss: 1.314945\n",
      "iteration 350 / 2000: training loss 1.328008 val loss: 1.302608\n",
      "iteration 400 / 2000: training loss 1.316693 val loss: 1.293703\n",
      "iteration 450 / 2000: training loss 1.325384 val loss: 1.285123\n",
      "iteration 500 / 2000: training loss 1.258987 val loss: 1.278426\n",
      "iteration 550 / 2000: training loss 1.261024 val loss: 1.267906\n",
      "iteration 600 / 2000: training loss 1.208025 val loss: 1.275798\n",
      "iteration 650 / 2000: training loss 1.198852 val loss: 1.274197\n",
      "iteration 700 / 2000: training loss 1.276997 val loss: 1.277106\n",
      "iteration 750 / 2000: training loss 1.278015 val loss: 1.267106\n",
      "iteration 800 / 2000: training loss 1.266764 val loss: 1.284203\n",
      "iteration 850 / 2000: training loss 1.318654 val loss: 1.269994\n",
      "iteration 900 / 2000: training loss 1.240020 val loss: 1.273247\n",
      "iteration 950 / 2000: training loss 1.223144 val loss: 1.263336\n",
      "iteration 1000 / 2000: training loss 1.262082 val loss: 1.261585\n",
      "iteration 1050 / 2000: training loss 1.261891 val loss: 1.264453\n",
      "iteration 1100 / 2000: training loss 1.238227 val loss: 1.262335\n",
      "iteration 1150 / 2000: training loss 1.211630 val loss: 1.261085\n",
      "hidden_size: 200 lr: 1.000000e-01 lr_decay: 8.500000e-01 reg: 6.000000e-02 train accuracy: 0.890000 val accuracy: 0.904000\n",
      "iteration 0 / 2000: training loss 2.302601 val loss: 2.302606\n",
      "iteration 50 / 2000: training loss 2.240417 val loss: 2.228154\n",
      "iteration 100 / 2000: training loss 1.438212 val loss: 1.439417\n",
      "iteration 150 / 2000: training loss 1.259191 val loss: 1.325400\n",
      "iteration 200 / 2000: training loss 1.281347 val loss: 1.316316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 250 / 2000: training loss 1.230665 val loss: 1.298253\n",
      "iteration 300 / 2000: training loss 1.224215 val loss: 1.311636\n",
      "iteration 350 / 2000: training loss 1.314387 val loss: 1.293241\n",
      "hidden_size: 200 lr: 1.000000e-01 lr_decay: 8.000000e-01 reg: 6.000000e-02 train accuracy: 0.890000 val accuracy: 0.881000\n",
      "iteration 0 / 2000: training loss 2.302588 val loss: 2.302635\n",
      "iteration 50 / 2000: training loss 0.895521 val loss: 0.869651\n",
      "iteration 100 / 2000: training loss 0.615897 val loss: 0.646527\n",
      "iteration 150 / 2000: training loss 0.769160 val loss: 0.652951\n",
      "iteration 200 / 2000: training loss 0.613614 val loss: 0.627667\n",
      "iteration 250 / 2000: training loss 0.643531 val loss: 0.651941\n",
      "iteration 300 / 2000: training loss 0.629069 val loss: 0.663399\n",
      "iteration 350 / 2000: training loss 0.588930 val loss: 0.662378\n",
      "iteration 400 / 2000: training loss 0.557077 val loss: 0.571229\n",
      "iteration 450 / 2000: training loss 0.641853 val loss: 0.609261\n",
      "iteration 500 / 2000: training loss 0.632598 val loss: 0.641692\n",
      "hidden_size: 200 lr: 3.000000e-01 lr_decay: 9.800000e-01 reg: 1.000000e-02 train accuracy: 0.930000 val accuracy: 0.864000\n",
      "iteration 0 / 2000: training loss 2.302588 val loss: 2.302635\n",
      "iteration 50 / 2000: training loss 0.801684 val loss: 0.813672\n",
      "iteration 100 / 2000: training loss 0.598584 val loss: 0.605863\n",
      "iteration 150 / 2000: training loss 0.641412 val loss: 0.670540\n",
      "iteration 200 / 2000: training loss 0.573083 val loss: 0.639403\n",
      "iteration 250 / 2000: training loss 0.545484 val loss: 0.643057\n",
      "iteration 300 / 2000: training loss 0.557454 val loss: 0.624113\n",
      "iteration 350 / 2000: training loss 0.553370 val loss: 0.588314\n",
      "iteration 400 / 2000: training loss 0.552724 val loss: 0.544694\n",
      "iteration 450 / 2000: training loss 0.604995 val loss: 0.590647\n",
      "iteration 500 / 2000: training loss 0.561347 val loss: 0.562444\n",
      "iteration 550 / 2000: training loss 0.513978 val loss: 0.526031\n",
      "iteration 600 / 2000: training loss 0.452855 val loss: 0.558434\n",
      "iteration 650 / 2000: training loss 0.456878 val loss: 0.555372\n",
      "iteration 700 / 2000: training loss 0.536254 val loss: 0.520576\n",
      "iteration 750 / 2000: training loss 0.497370 val loss: 0.528142\n",
      "iteration 800 / 2000: training loss 0.544437 val loss: 0.538485\n",
      "iteration 850 / 2000: training loss 0.519619 val loss: 0.556922\n",
      "iteration 900 / 2000: training loss 0.504642 val loss: 0.532889\n",
      "iteration 950 / 2000: training loss 0.494116 val loss: 0.519516\n",
      "iteration 1000 / 2000: training loss 0.484788 val loss: 0.521966\n",
      "iteration 1050 / 2000: training loss 0.485363 val loss: 0.535073\n",
      "hidden_size: 200 lr: 3.000000e-01 lr_decay: 9.000000e-01 reg: 1.000000e-02 train accuracy: 0.920000 val accuracy: 0.934000\n",
      "iteration 0 / 2000: training loss 2.302588 val loss: 2.302635\n",
      "iteration 50 / 2000: training loss 0.768018 val loss: 0.803916\n",
      "iteration 100 / 2000: training loss 0.621981 val loss: 0.597224\n",
      "iteration 150 / 2000: training loss 0.575492 val loss: 0.614764\n",
      "iteration 200 / 2000: training loss 0.536442 val loss: 0.622988\n",
      "iteration 250 / 2000: training loss 0.523169 val loss: 0.602921\n",
      "iteration 300 / 2000: training loss 0.505940 val loss: 0.575985\n",
      "iteration 350 / 2000: training loss 0.570523 val loss: 0.579897\n",
      "iteration 400 / 2000: training loss 0.511044 val loss: 0.536981\n",
      "iteration 450 / 2000: training loss 0.589543 val loss: 0.584527\n",
      "hidden_size: 200 lr: 3.000000e-01 lr_decay: 8.500000e-01 reg: 1.000000e-02 train accuracy: 0.920000 val accuracy: 0.920000\n",
      "iteration 0 / 2000: training loss 2.302588 val loss: 2.302635\n",
      "iteration 50 / 2000: training loss 0.763536 val loss: 0.855262\n",
      "iteration 100 / 2000: training loss 0.621978 val loss: 0.601780\n",
      "iteration 150 / 2000: training loss 0.577095 val loss: 0.605569\n",
      "iteration 200 / 2000: training loss 0.512685 val loss: 0.610713\n",
      "iteration 250 / 2000: training loss 0.539212 val loss: 0.580841\n",
      "iteration 300 / 2000: training loss 0.498699 val loss: 0.557558\n",
      "iteration 350 / 2000: training loss 0.539669 val loss: 0.578932\n",
      "iteration 400 / 2000: training loss 0.497224 val loss: 0.533846\n",
      "iteration 450 / 2000: training loss 0.519741 val loss: 0.548753\n",
      "iteration 500 / 2000: training loss 0.494340 val loss: 0.513039\n",
      "iteration 550 / 2000: training loss 0.515632 val loss: 0.509517\n",
      "iteration 600 / 2000: training loss 0.433519 val loss: 0.516200\n",
      "iteration 650 / 2000: training loss 0.426393 val loss: 0.511188\n",
      "iteration 700 / 2000: training loss 0.515374 val loss: 0.516420\n",
      "iteration 750 / 2000: training loss 0.484942 val loss: 0.509466\n",
      "iteration 800 / 2000: training loss 0.517628 val loss: 0.517807\n",
      "iteration 850 / 2000: training loss 0.511592 val loss: 0.509095\n",
      "iteration 900 / 2000: training loss 0.473280 val loss: 0.512604\n",
      "iteration 950 / 2000: training loss 0.453274 val loss: 0.505324\n",
      "iteration 1000 / 2000: training loss 0.474680 val loss: 0.503306\n",
      "iteration 1050 / 2000: training loss 0.476928 val loss: 0.502239\n",
      "iteration 1100 / 2000: training loss 0.495741 val loss: 0.501109\n",
      "iteration 1150 / 2000: training loss 0.449502 val loss: 0.503070\n",
      "iteration 1200 / 2000: training loss 0.500041 val loss: 0.501901\n",
      "iteration 1250 / 2000: training loss 0.528806 val loss: 0.504189\n",
      "iteration 1300 / 2000: training loss 0.475823 val loss: 0.502268\n",
      "iteration 1350 / 2000: training loss 0.511343 val loss: 0.503612\n",
      "iteration 1400 / 2000: training loss 0.440230 val loss: 0.502564\n",
      "iteration 1450 / 2000: training loss 0.509418 val loss: 0.504198\n",
      "iteration 1500 / 2000: training loss 0.465154 val loss: 0.499610\n",
      "iteration 1550 / 2000: training loss 0.535228 val loss: 0.501850\n",
      "iteration 1600 / 2000: training loss 0.503201 val loss: 0.499850\n",
      "iteration 1650 / 2000: training loss 0.490262 val loss: 0.498328\n",
      "iteration 1700 / 2000: training loss 0.460490 val loss: 0.499213\n",
      "iteration 1750 / 2000: training loss 0.416749 val loss: 0.497805\n",
      "iteration 1800 / 2000: training loss 0.519501 val loss: 0.498795\n",
      "iteration 1850 / 2000: training loss 0.478655 val loss: 0.498199\n",
      "iteration 1900 / 2000: training loss 0.408592 val loss: 0.498683\n",
      "iteration 1950 / 2000: training loss 0.472979 val loss: 0.498345\n",
      "hidden_size: 200 lr: 3.000000e-01 lr_decay: 8.000000e-01 reg: 1.000000e-02 train accuracy: 0.950000 val accuracy: 0.947000\n",
      "iteration 0 / 2000: training loss 2.302590 val loss: 2.302638\n",
      "iteration 50 / 2000: training loss 1.076656 val loss: 1.091703\n",
      "iteration 100 / 2000: training loss 0.893282 val loss: 0.883157\n",
      "iteration 150 / 2000: training loss 0.974433 val loss: 0.935237\n",
      "iteration 200 / 2000: training loss 0.800647 val loss: 0.826773\n",
      "iteration 250 / 2000: training loss 0.797731 val loss: 0.879758\n",
      "iteration 300 / 2000: training loss 0.822928 val loss: 0.982471\n",
      "iteration 350 / 2000: training loss 0.884910 val loss: 0.878670\n",
      "iteration 400 / 2000: training loss 0.847392 val loss: 0.853577\n",
      "iteration 450 / 2000: training loss 0.854288 val loss: 0.858762\n",
      "iteration 500 / 2000: training loss 0.756211 val loss: 0.802756\n",
      "hidden_size: 200 lr: 3.000000e-01 lr_decay: 9.800000e-01 reg: 2.000000e-02 train accuracy: 0.920000 val accuracy: 0.857000\n",
      "iteration 0 / 2000: training loss 2.302590 val loss: 2.302638\n",
      "iteration 50 / 2000: training loss 0.988356 val loss: 1.048184\n",
      "iteration 100 / 2000: training loss 0.850408 val loss: 0.836458\n",
      "iteration 150 / 2000: training loss 0.966955 val loss: 0.887777\n",
      "iteration 200 / 2000: training loss 0.798599 val loss: 0.852000\n",
      "iteration 250 / 2000: training loss 0.860689 val loss: 0.825859\n",
      "iteration 300 / 2000: training loss 0.780988 val loss: 0.828233\n",
      "iteration 350 / 2000: training loss 0.799541 val loss: 0.854585\n",
      "iteration 400 / 2000: training loss 0.802097 val loss: 0.779633\n",
      "iteration 450 / 2000: training loss 0.846927 val loss: 0.790075\n",
      "iteration 500 / 2000: training loss 0.737907 val loss: 0.764163\n",
      "iteration 550 / 2000: training loss 0.732513 val loss: 0.781956\n",
      "iteration 600 / 2000: training loss 0.678659 val loss: 0.808238\n",
      "iteration 650 / 2000: training loss 0.681711 val loss: 0.756633\n",
      "iteration 700 / 2000: training loss 0.755454 val loss: 0.753712\n",
      "iteration 750 / 2000: training loss 0.771415 val loss: 0.732767\n",
      "iteration 800 / 2000: training loss 0.789166 val loss: 0.741429\n",
      "iteration 850 / 2000: training loss 0.757135 val loss: 0.759609\n",
      "iteration 900 / 2000: training loss 0.759420 val loss: 0.736246\n",
      "iteration 950 / 2000: training loss 0.702288 val loss: 0.743834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1000 / 2000: training loss 0.710170 val loss: 0.751561\n",
      "iteration 1050 / 2000: training loss 0.719828 val loss: 0.750964\n",
      "iteration 1100 / 2000: training loss 0.751018 val loss: 0.747857\n",
      "iteration 1150 / 2000: training loss 0.677913 val loss: 0.745664\n",
      "iteration 1200 / 2000: training loss 0.740254 val loss: 0.728539\n",
      "iteration 1250 / 2000: training loss 0.790871 val loss: 0.744630\n",
      "iteration 1300 / 2000: training loss 0.711885 val loss: 0.727054\n",
      "iteration 1350 / 2000: training loss 0.787372 val loss: 0.731062\n",
      "iteration 1400 / 2000: training loss 0.682213 val loss: 0.728222\n",
      "iteration 1450 / 2000: training loss 0.762417 val loss: 0.730721\n",
      "iteration 1500 / 2000: training loss 0.716475 val loss: 0.723475\n",
      "iteration 1550 / 2000: training loss 0.749129 val loss: 0.721887\n",
      "iteration 1600 / 2000: training loss 0.748004 val loss: 0.720003\n",
      "iteration 1650 / 2000: training loss 0.715136 val loss: 0.722267\n",
      "iteration 1700 / 2000: training loss 0.681088 val loss: 0.719633\n",
      "hidden_size: 200 lr: 3.000000e-01 lr_decay: 9.000000e-01 reg: 2.000000e-02 train accuracy: 0.890000 val accuracy: 0.926000\n",
      "iteration 0 / 2000: training loss 2.302590 val loss: 2.302638\n",
      "iteration 50 / 2000: training loss 0.966501 val loss: 1.046973\n",
      "iteration 100 / 2000: training loss 0.820703 val loss: 0.822909\n",
      "iteration 150 / 2000: training loss 0.937992 val loss: 0.867480\n",
      "iteration 200 / 2000: training loss 0.782382 val loss: 0.847593\n",
      "iteration 250 / 2000: training loss 0.773290 val loss: 0.849662\n",
      "iteration 300 / 2000: training loss 0.739339 val loss: 0.794053\n",
      "iteration 350 / 2000: training loss 0.760971 val loss: 0.804308\n",
      "iteration 400 / 2000: training loss 0.782500 val loss: 0.754192\n",
      "iteration 450 / 2000: training loss 0.834218 val loss: 0.798488\n",
      "iteration 500 / 2000: training loss 0.773915 val loss: 0.763887\n",
      "hidden_size: 200 lr: 3.000000e-01 lr_decay: 8.500000e-01 reg: 2.000000e-02 train accuracy: 0.950000 val accuracy: 0.889000\n",
      "iteration 0 / 2000: training loss 2.302590 val loss: 2.302638\n",
      "iteration 50 / 2000: training loss 1.005183 val loss: 1.061830\n",
      "iteration 100 / 2000: training loss 0.803286 val loss: 0.796435\n",
      "iteration 150 / 2000: training loss 0.853829 val loss: 0.849923\n",
      "iteration 200 / 2000: training loss 0.763925 val loss: 0.823852\n",
      "iteration 250 / 2000: training loss 0.733632 val loss: 0.823144\n",
      "iteration 300 / 2000: training loss 0.693145 val loss: 0.760977\n",
      "iteration 350 / 2000: training loss 0.769718 val loss: 0.790990\n",
      "iteration 400 / 2000: training loss 0.722923 val loss: 0.728865\n",
      "iteration 450 / 2000: training loss 0.809351 val loss: 0.777510\n",
      "hidden_size: 200 lr: 3.000000e-01 lr_decay: 8.000000e-01 reg: 2.000000e-02 train accuracy: 0.900000 val accuracy: 0.914000\n",
      "iteration 0 / 2000: training loss 2.302596 val loss: 2.302643\n",
      "iteration 50 / 2000: training loss 1.235031 val loss: 1.407854\n",
      "iteration 100 / 2000: training loss 1.186397 val loss: 1.282490\n",
      "iteration 150 / 2000: training loss 1.346052 val loss: 1.359247\n",
      "iteration 200 / 2000: training loss 1.114236 val loss: 1.141970\n",
      "iteration 250 / 2000: training loss 1.267324 val loss: 1.208750\n",
      "iteration 300 / 2000: training loss 1.083061 val loss: 1.281619\n",
      "iteration 350 / 2000: training loss 1.278467 val loss: 1.226956\n",
      "iteration 400 / 2000: training loss 1.247263 val loss: 1.149465\n",
      "iteration 450 / 2000: training loss 1.215965 val loss: 1.164862\n",
      "iteration 500 / 2000: training loss 1.117569 val loss: 1.190951\n",
      "iteration 550 / 2000: training loss 1.087581 val loss: 1.132577\n",
      "iteration 600 / 2000: training loss 1.107221 val loss: 1.185046\n",
      "iteration 650 / 2000: training loss 1.156359 val loss: 1.146540\n",
      "iteration 700 / 2000: training loss 1.271286 val loss: 1.211995\n",
      "iteration 750 / 2000: training loss 1.154070 val loss: 1.151142\n",
      "iteration 800 / 2000: training loss 1.087856 val loss: 1.141066\n",
      "iteration 850 / 2000: training loss 1.184320 val loss: 1.149129\n",
      "iteration 900 / 2000: training loss 1.112618 val loss: 1.164089\n",
      "iteration 950 / 2000: training loss 1.069068 val loss: 1.214253\n",
      "iteration 1000 / 2000: training loss 1.198103 val loss: 1.342057\n",
      "iteration 1050 / 2000: training loss 1.120881 val loss: 1.149776\n",
      "iteration 1100 / 2000: training loss 1.064245 val loss: 1.109590\n",
      "iteration 1150 / 2000: training loss 1.127829 val loss: 1.160079\n",
      "iteration 1200 / 2000: training loss 1.175608 val loss: 1.127310\n",
      "iteration 1250 / 2000: training loss 1.175902 val loss: 1.098356\n",
      "iteration 1300 / 2000: training loss 1.229519 val loss: 1.102514\n",
      "iteration 1350 / 2000: training loss 1.198860 val loss: 1.189622\n",
      "iteration 1400 / 2000: training loss 1.124163 val loss: 1.181671\n",
      "iteration 1450 / 2000: training loss 1.262851 val loss: 1.154436\n",
      "iteration 1500 / 2000: training loss 1.118418 val loss: 1.164739\n",
      "iteration 1550 / 2000: training loss 1.248002 val loss: 1.106397\n",
      "iteration 1600 / 2000: training loss 1.198595 val loss: 1.161364\n",
      "iteration 1650 / 2000: training loss 1.108764 val loss: 1.106610\n",
      "iteration 1700 / 2000: training loss 1.017448 val loss: 1.116408\n",
      "iteration 1750 / 2000: training loss 1.121981 val loss: 1.145670\n",
      "iteration 1800 / 2000: training loss 1.148630 val loss: 1.107083\n",
      "iteration 1850 / 2000: training loss 1.108230 val loss: 1.118492\n",
      "hidden_size: 200 lr: 3.000000e-01 lr_decay: 9.800000e-01 reg: 4.000000e-02 train accuracy: 0.920000 val accuracy: 0.874000\n",
      "iteration 0 / 2000: training loss 2.302596 val loss: 2.302643\n",
      "iteration 50 / 2000: training loss 1.215136 val loss: 1.338290\n",
      "iteration 100 / 2000: training loss 1.180334 val loss: 1.247195\n",
      "iteration 150 / 2000: training loss 1.258943 val loss: 1.241388\n",
      "iteration 200 / 2000: training loss 1.096891 val loss: 1.116848\n",
      "iteration 250 / 2000: training loss 1.177730 val loss: 1.137604\n",
      "iteration 300 / 2000: training loss 1.055030 val loss: 1.146248\n",
      "iteration 350 / 2000: training loss 1.149029 val loss: 1.120640\n",
      "iteration 400 / 2000: training loss 1.151575 val loss: 1.105130\n",
      "iteration 450 / 2000: training loss 1.145022 val loss: 1.115062\n",
      "iteration 500 / 2000: training loss 1.077154 val loss: 1.108093\n",
      "hidden_size: 200 lr: 3.000000e-01 lr_decay: 9.000000e-01 reg: 4.000000e-02 train accuracy: 0.890000 val accuracy: 0.838000\n",
      "iteration 0 / 2000: training loss 2.302596 val loss: 2.302643\n",
      "iteration 50 / 2000: training loss 1.272807 val loss: 1.302640\n",
      "iteration 100 / 2000: training loss 1.185804 val loss: 1.180412\n",
      "iteration 150 / 2000: training loss 1.221899 val loss: 1.231719\n",
      "iteration 200 / 2000: training loss 1.110540 val loss: 1.150692\n",
      "iteration 250 / 2000: training loss 1.143983 val loss: 1.100240\n",
      "iteration 300 / 2000: training loss 1.047890 val loss: 1.100618\n",
      "iteration 350 / 2000: training loss 1.114798 val loss: 1.178294\n",
      "iteration 400 / 2000: training loss 1.120387 val loss: 1.083432\n",
      "iteration 450 / 2000: training loss 1.164726 val loss: 1.101824\n",
      "iteration 500 / 2000: training loss 1.054796 val loss: 1.065557\n",
      "hidden_size: 200 lr: 3.000000e-01 lr_decay: 8.500000e-01 reg: 4.000000e-02 train accuracy: 0.930000 val accuracy: 0.849000\n",
      "iteration 0 / 2000: training loss 2.302596 val loss: 2.302643\n",
      "iteration 50 / 2000: training loss 1.358171 val loss: 1.340691\n",
      "iteration 100 / 2000: training loss 1.147647 val loss: 1.134417\n",
      "iteration 150 / 2000: training loss 1.171884 val loss: 1.192276\n",
      "iteration 200 / 2000: training loss 1.087035 val loss: 1.148904\n",
      "iteration 250 / 2000: training loss 1.094146 val loss: 1.111208\n",
      "iteration 300 / 2000: training loss 1.032383 val loss: 1.083538\n",
      "iteration 350 / 2000: training loss 1.056534 val loss: 1.116854\n",
      "iteration 400 / 2000: training loss 1.103554 val loss: 1.052621\n",
      "iteration 450 / 2000: training loss 1.147199 val loss: 1.098011\n",
      "iteration 500 / 2000: training loss 1.043004 val loss: 1.047795\n",
      "iteration 550 / 2000: training loss 1.017263 val loss: 1.056688\n",
      "iteration 600 / 2000: training loss 0.970883 val loss: 1.052023\n",
      "iteration 650 / 2000: training loss 0.948319 val loss: 1.040166\n",
      "iteration 700 / 2000: training loss 1.042549 val loss: 1.037828\n",
      "iteration 750 / 2000: training loss 1.033154 val loss: 1.034772\n",
      "iteration 800 / 2000: training loss 1.021493 val loss: 1.054370\n",
      "iteration 850 / 2000: training loss 1.064839 val loss: 1.026398\n",
      "iteration 900 / 2000: training loss 0.999430 val loss: 1.032941\n",
      "iteration 950 / 2000: training loss 0.985441 val loss: 1.022268\n",
      "iteration 1000 / 2000: training loss 1.011960 val loss: 1.019820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1050 / 2000: training loss 1.012885 val loss: 1.021014\n",
      "iteration 1100 / 2000: training loss 1.002576 val loss: 1.025260\n",
      "iteration 1150 / 2000: training loss 0.973413 val loss: 1.020206\n",
      "iteration 1200 / 2000: training loss 1.053756 val loss: 1.021511\n",
      "iteration 1250 / 2000: training loss 1.064227 val loss: 1.024934\n",
      "iteration 1300 / 2000: training loss 1.024742 val loss: 1.022964\n",
      "iteration 1350 / 2000: training loss 1.070551 val loss: 1.022474\n",
      "iteration 1400 / 2000: training loss 1.013509 val loss: 1.021046\n",
      "iteration 1450 / 2000: training loss 1.057825 val loss: 1.025159\n",
      "iteration 1500 / 2000: training loss 0.999572 val loss: 1.018146\n",
      "iteration 1550 / 2000: training loss 1.019386 val loss: 1.019144\n",
      "iteration 1600 / 2000: training loss 1.051656 val loss: 1.016840\n",
      "iteration 1650 / 2000: training loss 1.020399 val loss: 1.015028\n",
      "iteration 1700 / 2000: training loss 0.974797 val loss: 1.017344\n",
      "iteration 1750 / 2000: training loss 0.931133 val loss: 1.015667\n",
      "iteration 1800 / 2000: training loss 1.060476 val loss: 1.014676\n",
      "iteration 1850 / 2000: training loss 1.021672 val loss: 1.015539\n",
      "iteration 1900 / 2000: training loss 0.939068 val loss: 1.015402\n",
      "iteration 1950 / 2000: training loss 1.018059 val loss: 1.014912\n",
      "hidden_size: 200 lr: 3.000000e-01 lr_decay: 8.000000e-01 reg: 4.000000e-02 train accuracy: 0.910000 val accuracy: 0.923000\n",
      "iteration 0 / 2000: training loss 2.302601 val loss: 2.302648\n",
      "iteration 50 / 2000: training loss 1.437558 val loss: 1.568055\n",
      "iteration 100 / 2000: training loss 1.484703 val loss: 1.598387\n",
      "iteration 150 / 2000: training loss 1.599378 val loss: 1.543087\n",
      "iteration 200 / 2000: training loss 1.406525 val loss: 1.411875\n",
      "iteration 250 / 2000: training loss 1.553222 val loss: 1.412356\n",
      "hidden_size: 200 lr: 3.000000e-01 lr_decay: 9.800000e-01 reg: 6.000000e-02 train accuracy: 0.740000 val accuracy: 0.766000\n",
      "iteration 0 / 2000: training loss 2.302601 val loss: 2.302648\n",
      "iteration 50 / 2000: training loss 1.473135 val loss: 1.501792\n",
      "iteration 100 / 2000: training loss 1.402371 val loss: 1.491751\n",
      "iteration 150 / 2000: training loss 1.503900 val loss: 1.502816\n",
      "iteration 200 / 2000: training loss 1.353847 val loss: 1.342126\n",
      "iteration 250 / 2000: training loss 1.434289 val loss: 1.380245\n",
      "iteration 300 / 2000: training loss 1.347535 val loss: 1.448313\n",
      "iteration 350 / 2000: training loss 1.420460 val loss: 1.440918\n",
      "iteration 400 / 2000: training loss 1.436074 val loss: 1.358558\n",
      "iteration 450 / 2000: training loss 1.415159 val loss: 1.364224\n",
      "iteration 500 / 2000: training loss 1.305246 val loss: 1.342410\n",
      "iteration 550 / 2000: training loss 1.288157 val loss: 1.351444\n",
      "iteration 600 / 2000: training loss 1.319816 val loss: 1.368161\n",
      "iteration 650 / 2000: training loss 1.223761 val loss: 1.290432\n",
      "iteration 700 / 2000: training loss 1.336879 val loss: 1.367212\n",
      "iteration 750 / 2000: training loss 1.367117 val loss: 1.309291\n",
      "iteration 800 / 2000: training loss 1.303953 val loss: 1.304631\n",
      "hidden_size: 200 lr: 3.000000e-01 lr_decay: 9.000000e-01 reg: 6.000000e-02 train accuracy: 0.850000 val accuracy: 0.853000\n",
      "iteration 0 / 2000: training loss 2.302601 val loss: 2.302648\n",
      "iteration 50 / 2000: training loss 1.553038 val loss: 1.522281\n",
      "iteration 100 / 2000: training loss 1.405421 val loss: 1.428624\n",
      "iteration 150 / 2000: training loss 1.461607 val loss: 1.490501\n",
      "iteration 200 / 2000: training loss 1.348183 val loss: 1.388516\n",
      "iteration 250 / 2000: training loss 1.442227 val loss: 1.362909\n",
      "iteration 300 / 2000: training loss 1.286244 val loss: 1.360787\n",
      "iteration 350 / 2000: training loss 1.383160 val loss: 1.409386\n",
      "iteration 400 / 2000: training loss 1.357236 val loss: 1.329783\n",
      "iteration 450 / 2000: training loss 1.406653 val loss: 1.335278\n",
      "iteration 500 / 2000: training loss 1.272295 val loss: 1.313677\n",
      "iteration 550 / 2000: training loss 1.248654 val loss: 1.307829\n",
      "iteration 600 / 2000: training loss 1.255931 val loss: 1.319168\n",
      "iteration 650 / 2000: training loss 1.232394 val loss: 1.300163\n",
      "iteration 700 / 2000: training loss 1.289243 val loss: 1.303790\n",
      "iteration 750 / 2000: training loss 1.291398 val loss: 1.272811\n",
      "iteration 800 / 2000: training loss 1.311052 val loss: 1.291235\n",
      "iteration 850 / 2000: training loss 1.300105 val loss: 1.290143\n",
      "iteration 900 / 2000: training loss 1.261519 val loss: 1.285608\n",
      "iteration 950 / 2000: training loss 1.254740 val loss: 1.283655\n",
      "iteration 1000 / 2000: training loss 1.261529 val loss: 1.279931\n",
      "iteration 1050 / 2000: training loss 1.265449 val loss: 1.279822\n",
      "hidden_size: 200 lr: 3.000000e-01 lr_decay: 8.500000e-01 reg: 6.000000e-02 train accuracy: 0.840000 val accuracy: 0.886000\n",
      "iteration 0 / 2000: training loss 2.302601 val loss: 2.302648\n",
      "iteration 50 / 2000: training loss 1.577080 val loss: 1.577900\n",
      "iteration 100 / 2000: training loss 1.403844 val loss: 1.393447\n",
      "iteration 150 / 2000: training loss 1.418677 val loss: 1.468309\n",
      "iteration 200 / 2000: training loss 1.337576 val loss: 1.403588\n",
      "iteration 250 / 2000: training loss 1.327456 val loss: 1.315865\n",
      "iteration 300 / 2000: training loss 1.276148 val loss: 1.335394\n",
      "iteration 350 / 2000: training loss 1.295419 val loss: 1.366863\n",
      "iteration 400 / 2000: training loss 1.376182 val loss: 1.288749\n",
      "iteration 450 / 2000: training loss 1.389288 val loss: 1.332534\n",
      "iteration 500 / 2000: training loss 1.283340 val loss: 1.290976\n",
      "iteration 550 / 2000: training loss 1.261030 val loss: 1.298581\n",
      "iteration 600 / 2000: training loss 1.229276 val loss: 1.295216\n",
      "iteration 650 / 2000: training loss 1.203568 val loss: 1.292107\n",
      "iteration 700 / 2000: training loss 1.269419 val loss: 1.273875\n",
      "iteration 750 / 2000: training loss 1.271616 val loss: 1.274672\n",
      "iteration 800 / 2000: training loss 1.255317 val loss: 1.292194\n",
      "iteration 850 / 2000: training loss 1.314662 val loss: 1.264992\n",
      "iteration 900 / 2000: training loss 1.230548 val loss: 1.273175\n",
      "iteration 950 / 2000: training loss 1.224737 val loss: 1.260118\n",
      "iteration 1000 / 2000: training loss 1.251225 val loss: 1.258322\n",
      "iteration 1050 / 2000: training loss 1.255262 val loss: 1.257685\n",
      "iteration 1100 / 2000: training loss 1.239164 val loss: 1.262272\n",
      "iteration 1150 / 2000: training loss 1.209960 val loss: 1.259020\n",
      "iteration 1200 / 2000: training loss 1.295186 val loss: 1.260242\n",
      "iteration 1250 / 2000: training loss 1.303185 val loss: 1.262491\n",
      "iteration 1300 / 2000: training loss 1.260502 val loss: 1.261072\n",
      "iteration 1350 / 2000: training loss 1.314336 val loss: 1.259784\n",
      "iteration 1400 / 2000: training loss 1.270409 val loss: 1.258164\n",
      "iteration 1450 / 2000: training loss 1.299654 val loss: 1.261389\n",
      "iteration 1500 / 2000: training loss 1.240456 val loss: 1.255383\n",
      "iteration 1550 / 2000: training loss 1.238308 val loss: 1.255416\n",
      "iteration 1600 / 2000: training loss 1.290253 val loss: 1.253512\n",
      "iteration 1650 / 2000: training loss 1.257287 val loss: 1.251941\n",
      "iteration 1700 / 2000: training loss 1.216725 val loss: 1.253701\n",
      "iteration 1750 / 2000: training loss 1.163183 val loss: 1.252313\n",
      "iteration 1800 / 2000: training loss 1.296656 val loss: 1.251421\n",
      "iteration 1850 / 2000: training loss 1.265170 val loss: 1.252158\n",
      "iteration 1900 / 2000: training loss 1.181124 val loss: 1.252151\n",
      "iteration 1950 / 2000: training loss 1.252843 val loss: 1.251416\n",
      "hidden_size: 200 lr: 3.000000e-01 lr_decay: 8.000000e-01 reg: 6.000000e-02 train accuracy: 0.890000 val accuracy: 0.912000\n",
      "iteration 0 / 2000: training loss 2.302588 val loss: 2.302720\n",
      "iteration 50 / 2000: training loss 1.306259 val loss: 1.121835\n",
      "iteration 100 / 2000: training loss 0.888250 val loss: 0.969044\n",
      "iteration 150 / 2000: training loss 1.044374 val loss: 0.899319\n",
      "iteration 200 / 2000: training loss 0.816605 val loss: 0.862410\n",
      "iteration 250 / 2000: training loss 0.743489 val loss: 0.787079\n",
      "iteration 300 / 2000: training loss 0.773882 val loss: 0.860886\n",
      "iteration 350 / 2000: training loss 0.733968 val loss: 0.740761\n",
      "iteration 400 / 2000: training loss 0.683126 val loss: 0.834735\n",
      "iteration 450 / 2000: training loss 0.753195 val loss: 0.738026\n",
      "iteration 500 / 2000: training loss 0.709782 val loss: 0.704860\n",
      "iteration 550 / 2000: training loss 0.799637 val loss: 0.709494\n",
      "iteration 600 / 2000: training loss 0.652756 val loss: 0.714647\n",
      "iteration 650 / 2000: training loss 0.634923 val loss: 0.817931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 700 / 2000: training loss 0.814948 val loss: 0.701984\n",
      "iteration 750 / 2000: training loss 0.680186 val loss: 0.679272\n",
      "iteration 800 / 2000: training loss 0.739876 val loss: 0.644578\n",
      "iteration 850 / 2000: training loss 0.765199 val loss: 0.688706\n",
      "iteration 900 / 2000: training loss 0.827886 val loss: 0.773010\n",
      "iteration 950 / 2000: training loss 0.586324 val loss: 0.714860\n",
      "iteration 1000 / 2000: training loss 0.628742 val loss: 0.874299\n",
      "iteration 1050 / 2000: training loss 0.613415 val loss: 0.760838\n",
      "iteration 1100 / 2000: training loss 0.661277 val loss: 0.685748\n",
      "iteration 1150 / 2000: training loss 0.574531 val loss: 0.614583\n",
      "iteration 1200 / 2000: training loss 0.627670 val loss: 0.731701\n",
      "iteration 1250 / 2000: training loss 0.699816 val loss: 0.692168\n",
      "hidden_size: 200 lr: 5.000000e-01 lr_decay: 9.800000e-01 reg: 1.000000e-02 train accuracy: 0.940000 val accuracy: 0.891000\n",
      "iteration 0 / 2000: training loss 2.302588 val loss: 2.302720\n",
      "iteration 50 / 2000: training loss 1.215192 val loss: 1.098453\n",
      "iteration 100 / 2000: training loss 0.744779 val loss: 0.840251\n",
      "iteration 150 / 2000: training loss 1.126341 val loss: 0.908788\n",
      "iteration 200 / 2000: training loss 0.610121 val loss: 0.671065\n",
      "iteration 250 / 2000: training loss 0.733006 val loss: 0.708801\n",
      "iteration 300 / 2000: training loss 0.611358 val loss: 0.755026\n",
      "iteration 350 / 2000: training loss 0.666646 val loss: 0.714597\n",
      "iteration 400 / 2000: training loss 0.570019 val loss: 0.601549\n",
      "iteration 450 / 2000: training loss 0.592256 val loss: 0.591924\n",
      "iteration 500 / 2000: training loss 0.577134 val loss: 0.615472\n",
      "iteration 550 / 2000: training loss 0.589671 val loss: 0.637071\n",
      "iteration 600 / 2000: training loss 0.486937 val loss: 0.612872\n",
      "iteration 650 / 2000: training loss 0.477586 val loss: 0.547103\n",
      "iteration 700 / 2000: training loss 0.604340 val loss: 0.579046\n",
      "iteration 750 / 2000: training loss 0.580751 val loss: 0.535614\n",
      "iteration 800 / 2000: training loss 0.597776 val loss: 0.560014\n",
      "iteration 850 / 2000: training loss 0.574321 val loss: 0.568385\n",
      "iteration 900 / 2000: training loss 0.523208 val loss: 0.543902\n",
      "iteration 950 / 2000: training loss 0.503519 val loss: 0.557700\n",
      "iteration 1000 / 2000: training loss 0.498499 val loss: 0.544130\n",
      "iteration 1050 / 2000: training loss 0.521471 val loss: 0.573005\n",
      "hidden_size: 200 lr: 5.000000e-01 lr_decay: 9.000000e-01 reg: 1.000000e-02 train accuracy: 0.940000 val accuracy: 0.927000\n",
      "iteration 0 / 2000: training loss 2.302588 val loss: 2.302720\n",
      "iteration 50 / 2000: training loss 1.080891 val loss: 1.119802\n",
      "iteration 100 / 2000: training loss 0.740894 val loss: 0.749791\n",
      "iteration 150 / 2000: training loss 0.849674 val loss: 0.726848\n",
      "iteration 200 / 2000: training loss 0.608164 val loss: 0.643771\n",
      "iteration 250 / 2000: training loss 0.644832 val loss: 0.642507\n",
      "iteration 300 / 2000: training loss 0.648909 val loss: 0.647232\n",
      "iteration 350 / 2000: training loss 0.566417 val loss: 0.637614\n",
      "iteration 400 / 2000: training loss 0.579220 val loss: 0.563969\n",
      "iteration 450 / 2000: training loss 0.620695 val loss: 0.595410\n",
      "iteration 500 / 2000: training loss 0.560509 val loss: 0.566709\n",
      "hidden_size: 200 lr: 5.000000e-01 lr_decay: 8.500000e-01 reg: 1.000000e-02 train accuracy: 0.940000 val accuracy: 0.890000\n",
      "iteration 0 / 2000: training loss 2.302588 val loss: 2.302720\n",
      "iteration 50 / 2000: training loss 1.086367 val loss: 1.131614\n",
      "iteration 100 / 2000: training loss 0.656941 val loss: 0.690092\n",
      "iteration 150 / 2000: training loss 0.776504 val loss: 0.686605\n",
      "iteration 200 / 2000: training loss 0.592989 val loss: 0.636678\n",
      "iteration 250 / 2000: training loss 0.600877 val loss: 0.646019\n",
      "iteration 300 / 2000: training loss 0.542502 val loss: 0.605047\n",
      "iteration 350 / 2000: training loss 0.560828 val loss: 0.580137\n",
      "iteration 400 / 2000: training loss 0.513333 val loss: 0.525379\n",
      "iteration 450 / 2000: training loss 0.590963 val loss: 0.590747\n",
      "iteration 500 / 2000: training loss 0.534359 val loss: 0.540263\n",
      "iteration 550 / 2000: training loss 0.512572 val loss: 0.531202\n",
      "iteration 600 / 2000: training loss 0.448676 val loss: 0.524827\n",
      "iteration 650 / 2000: training loss 0.426710 val loss: 0.511600\n",
      "iteration 700 / 2000: training loss 0.537142 val loss: 0.521807\n",
      "iteration 750 / 2000: training loss 0.497475 val loss: 0.515399\n",
      "iteration 800 / 2000: training loss 0.510104 val loss: 0.530609\n",
      "iteration 850 / 2000: training loss 0.498769 val loss: 0.508656\n",
      "iteration 900 / 2000: training loss 0.465870 val loss: 0.512200\n",
      "iteration 950 / 2000: training loss 0.462629 val loss: 0.503007\n",
      "iteration 1000 / 2000: training loss 0.474688 val loss: 0.501005\n",
      "iteration 1050 / 2000: training loss 0.475224 val loss: 0.502565\n",
      "iteration 1100 / 2000: training loss 0.491900 val loss: 0.500844\n",
      "iteration 1150 / 2000: training loss 0.444519 val loss: 0.502637\n",
      "iteration 1200 / 2000: training loss 0.503990 val loss: 0.502575\n",
      "iteration 1250 / 2000: training loss 0.533917 val loss: 0.505843\n",
      "iteration 1300 / 2000: training loss 0.468477 val loss: 0.501824\n",
      "iteration 1350 / 2000: training loss 0.522309 val loss: 0.504460\n",
      "iteration 1400 / 2000: training loss 0.438323 val loss: 0.499976\n",
      "iteration 1450 / 2000: training loss 0.503011 val loss: 0.504306\n",
      "iteration 1500 / 2000: training loss 0.473087 val loss: 0.496745\n",
      "iteration 1550 / 2000: training loss 0.536384 val loss: 0.499455\n",
      "iteration 1600 / 2000: training loss 0.498148 val loss: 0.495940\n",
      "iteration 1650 / 2000: training loss 0.492265 val loss: 0.494859\n",
      "iteration 1700 / 2000: training loss 0.461287 val loss: 0.495937\n",
      "hidden_size: 200 lr: 5.000000e-01 lr_decay: 8.000000e-01 reg: 1.000000e-02 train accuracy: 0.950000 val accuracy: 0.946000\n",
      "iteration 0 / 2000: training loss 2.302590 val loss: 2.302723\n",
      "iteration 50 / 2000: training loss 1.333135 val loss: 1.268210\n",
      "iteration 100 / 2000: training loss 1.052474 val loss: 1.071176\n",
      "iteration 150 / 2000: training loss 1.268476 val loss: 1.124657\n",
      "iteration 200 / 2000: training loss 0.986994 val loss: 1.013922\n",
      "iteration 250 / 2000: training loss 1.188693 val loss: 1.225801\n",
      "iteration 300 / 2000: training loss 1.132846 val loss: 1.307889\n",
      "iteration 350 / 2000: training loss 1.034961 val loss: 0.975056\n",
      "iteration 400 / 2000: training loss 1.053940 val loss: 0.958700\n",
      "iteration 450 / 2000: training loss 1.009052 val loss: 0.906267\n",
      "iteration 500 / 2000: training loss 1.403855 val loss: 1.205274\n",
      "iteration 550 / 2000: training loss 1.021378 val loss: 1.135310\n",
      "iteration 600 / 2000: training loss 0.896749 val loss: 0.993589\n",
      "iteration 650 / 2000: training loss 0.829961 val loss: 1.026125\n",
      "iteration 700 / 2000: training loss 1.140209 val loss: 1.029877\n",
      "iteration 750 / 2000: training loss 1.008565 val loss: 0.934318\n",
      "iteration 800 / 2000: training loss 0.930642 val loss: 0.967816\n",
      "iteration 850 / 2000: training loss 0.984645 val loss: 0.914143\n",
      "iteration 900 / 2000: training loss 1.025077 val loss: 1.053987\n",
      "iteration 950 / 2000: training loss 0.861286 val loss: 0.991570\n",
      "iteration 1000 / 2000: training loss 1.039188 val loss: 1.042039\n",
      "iteration 1050 / 2000: training loss 0.850569 val loss: 1.019886\n",
      "iteration 1100 / 2000: training loss 0.973046 val loss: 0.944272\n",
      "iteration 1150 / 2000: training loss 0.973962 val loss: 0.975498\n",
      "iteration 1200 / 2000: training loss 0.873537 val loss: 0.928183\n",
      "iteration 1250 / 2000: training loss 1.011042 val loss: 0.893033\n",
      "iteration 1300 / 2000: training loss 0.890798 val loss: 0.988684\n",
      "iteration 1350 / 2000: training loss 0.953746 val loss: 0.968651\n",
      "iteration 1400 / 2000: training loss 0.869326 val loss: 0.966669\n",
      "iteration 1450 / 2000: training loss 0.968042 val loss: 0.906390\n",
      "iteration 1500 / 2000: training loss 0.862943 val loss: 0.927639\n",
      "iteration 1550 / 2000: training loss 1.038613 val loss: 0.854626\n",
      "iteration 1600 / 2000: training loss 0.947096 val loss: 0.813055\n",
      "iteration 1650 / 2000: training loss 0.987122 val loss: 0.974231\n",
      "iteration 1700 / 2000: training loss 0.760558 val loss: 0.859134\n",
      "iteration 1750 / 2000: training loss 0.958156 val loss: 0.834742\n",
      "iteration 1800 / 2000: training loss 0.871206 val loss: 0.820330\n",
      "iteration 1850 / 2000: training loss 0.850480 val loss: 0.871171\n",
      "iteration 1900 / 2000: training loss 0.701912 val loss: 0.887620\n",
      "iteration 1950 / 2000: training loss 0.859804 val loss: 0.830458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_size: 200 lr: 5.000000e-01 lr_decay: 9.800000e-01 reg: 2.000000e-02 train accuracy: 0.930000 val accuracy: 0.883000\n",
      "iteration 0 / 2000: training loss 2.302590 val loss: 2.302723\n",
      "iteration 50 / 2000: training loss 1.180442 val loss: 1.116632\n",
      "iteration 100 / 2000: training loss 0.932442 val loss: 1.113612\n",
      "iteration 150 / 2000: training loss 1.257006 val loss: 1.177725\n",
      "iteration 200 / 2000: training loss 0.964011 val loss: 0.957225\n",
      "iteration 250 / 2000: training loss 0.935700 val loss: 0.988652\n",
      "iteration 300 / 2000: training loss 0.782695 val loss: 0.981381\n",
      "iteration 350 / 2000: training loss 0.886257 val loss: 0.909716\n",
      "iteration 400 / 2000: training loss 0.863287 val loss: 0.850886\n",
      "iteration 450 / 2000: training loss 0.892652 val loss: 0.897042\n",
      "iteration 500 / 2000: training loss 0.798767 val loss: 0.877175\n",
      "iteration 550 / 2000: training loss 0.811907 val loss: 0.835246\n",
      "iteration 600 / 2000: training loss 0.787803 val loss: 0.884028\n",
      "iteration 650 / 2000: training loss 0.682902 val loss: 0.790401\n",
      "iteration 700 / 2000: training loss 0.878043 val loss: 0.836896\n",
      "iteration 750 / 2000: training loss 0.770062 val loss: 0.774886\n",
      "iteration 800 / 2000: training loss 0.777300 val loss: 0.790372\n",
      "iteration 850 / 2000: training loss 0.837430 val loss: 0.769041\n",
      "iteration 900 / 2000: training loss 0.791712 val loss: 0.770855\n",
      "iteration 950 / 2000: training loss 0.748935 val loss: 0.783786\n",
      "iteration 1000 / 2000: training loss 0.738079 val loss: 0.763432\n",
      "iteration 1050 / 2000: training loss 0.713634 val loss: 0.756492\n",
      "iteration 1100 / 2000: training loss 0.746192 val loss: 0.760854\n",
      "iteration 1150 / 2000: training loss 0.692871 val loss: 0.762432\n",
      "iteration 1200 / 2000: training loss 0.751321 val loss: 0.745006\n",
      "iteration 1250 / 2000: training loss 0.796569 val loss: 0.757487\n",
      "iteration 1300 / 2000: training loss 0.768665 val loss: 0.755185\n",
      "iteration 1350 / 2000: training loss 0.782402 val loss: 0.744216\n",
      "iteration 1400 / 2000: training loss 0.689072 val loss: 0.737771\n",
      "iteration 1450 / 2000: training loss 0.780666 val loss: 0.755083\n",
      "iteration 1500 / 2000: training loss 0.725826 val loss: 0.728269\n",
      "iteration 1550 / 2000: training loss 0.764841 val loss: 0.737198\n",
      "iteration 1600 / 2000: training loss 0.747697 val loss: 0.739069\n",
      "iteration 1650 / 2000: training loss 0.721719 val loss: 0.724514\n",
      "iteration 1700 / 2000: training loss 0.671549 val loss: 0.725873\n",
      "hidden_size: 200 lr: 5.000000e-01 lr_decay: 9.000000e-01 reg: 2.000000e-02 train accuracy: 0.920000 val accuracy: 0.921000\n",
      "iteration 0 / 2000: training loss 2.302590 val loss: 2.302723\n",
      "iteration 50 / 2000: training loss 1.247572 val loss: 1.234042\n",
      "iteration 100 / 2000: training loss 0.912927 val loss: 0.972645\n",
      "iteration 150 / 2000: training loss 1.064016 val loss: 1.046274\n",
      "iteration 200 / 2000: training loss 0.825821 val loss: 0.846198\n",
      "iteration 250 / 2000: training loss 0.850194 val loss: 0.916225\n",
      "iteration 300 / 2000: training loss 0.753325 val loss: 0.876262\n",
      "iteration 350 / 2000: training loss 0.879869 val loss: 0.854903\n",
      "iteration 400 / 2000: training loss 0.791519 val loss: 0.788955\n",
      "iteration 450 / 2000: training loss 0.834972 val loss: 0.799306\n",
      "iteration 500 / 2000: training loss 0.749885 val loss: 0.780140\n",
      "iteration 550 / 2000: training loss 0.729332 val loss: 0.797408\n",
      "iteration 600 / 2000: training loss 0.681782 val loss: 0.815024\n",
      "iteration 650 / 2000: training loss 0.675641 val loss: 0.754098\n",
      "iteration 700 / 2000: training loss 0.755823 val loss: 0.754019\n",
      "iteration 750 / 2000: training loss 0.756452 val loss: 0.735215\n",
      "iteration 800 / 2000: training loss 0.783722 val loss: 0.742667\n",
      "iteration 850 / 2000: training loss 0.748362 val loss: 0.776043\n",
      "iteration 900 / 2000: training loss 0.738133 val loss: 0.744850\n",
      "iteration 950 / 2000: training loss 0.711215 val loss: 0.739738\n",
      "iteration 1000 / 2000: training loss 0.699732 val loss: 0.732062\n",
      "iteration 1050 / 2000: training loss 0.702321 val loss: 0.742386\n",
      "hidden_size: 200 lr: 5.000000e-01 lr_decay: 8.500000e-01 reg: 2.000000e-02 train accuracy: 0.890000 val accuracy: 0.921000\n",
      "iteration 0 / 2000: training loss 2.302590 val loss: 2.302723\n",
      "iteration 50 / 2000: training loss 1.159149 val loss: 1.161130\n",
      "iteration 100 / 2000: training loss 0.925736 val loss: 0.991346\n",
      "iteration 150 / 2000: training loss 0.971426 val loss: 0.953849\n",
      "iteration 200 / 2000: training loss 0.793052 val loss: 0.832260\n",
      "iteration 250 / 2000: training loss 0.851731 val loss: 0.832466\n",
      "iteration 300 / 2000: training loss 0.763027 val loss: 0.810559\n",
      "iteration 350 / 2000: training loss 0.794014 val loss: 0.870829\n",
      "iteration 400 / 2000: training loss 0.794253 val loss: 0.762359\n",
      "iteration 450 / 2000: training loss 0.838912 val loss: 0.794587\n",
      "iteration 500 / 2000: training loss 0.767388 val loss: 0.761007\n",
      "iteration 550 / 2000: training loss 0.733955 val loss: 0.745211\n",
      "iteration 600 / 2000: training loss 0.659179 val loss: 0.744583\n",
      "iteration 650 / 2000: training loss 0.644997 val loss: 0.746675\n",
      "iteration 700 / 2000: training loss 0.754907 val loss: 0.728903\n",
      "iteration 750 / 2000: training loss 0.722672 val loss: 0.732643\n",
      "iteration 800 / 2000: training loss 0.718190 val loss: 0.751178\n",
      "iteration 850 / 2000: training loss 0.728495 val loss: 0.723961\n",
      "iteration 900 / 2000: training loss 0.694097 val loss: 0.728595\n",
      "iteration 950 / 2000: training loss 0.674981 val loss: 0.716593\n",
      "iteration 1000 / 2000: training loss 0.692917 val loss: 0.715230\n",
      "iteration 1050 / 2000: training loss 0.695997 val loss: 0.713530\n",
      "iteration 1100 / 2000: training loss 0.709613 val loss: 0.719369\n",
      "iteration 1150 / 2000: training loss 0.666146 val loss: 0.716351\n",
      "iteration 1200 / 2000: training loss 0.741998 val loss: 0.716668\n",
      "iteration 1250 / 2000: training loss 0.757149 val loss: 0.720378\n",
      "hidden_size: 200 lr: 5.000000e-01 lr_decay: 8.000000e-01 reg: 2.000000e-02 train accuracy: 0.950000 val accuracy: 0.930000\n",
      "iteration 0 / 2000: training loss 2.302596 val loss: 2.302728\n",
      "iteration 50 / 2000: training loss 2.164612 val loss: 1.955524\n",
      "iteration 100 / 2000: training loss 1.891683 val loss: 1.733248\n",
      "iteration 150 / 2000: training loss 1.519404 val loss: 1.430870\n",
      "iteration 200 / 2000: training loss 1.356432 val loss: 1.495323\n",
      "iteration 250 / 2000: training loss 1.413964 val loss: 1.641348\n",
      "iteration 300 / 2000: training loss 1.503618 val loss: 1.596895\n",
      "iteration 350 / 2000: training loss 1.412886 val loss: 1.300005\n",
      "iteration 400 / 2000: training loss 1.347016 val loss: 1.330372\n",
      "iteration 450 / 2000: training loss 1.416532 val loss: 1.300537\n",
      "iteration 500 / 2000: training loss 1.560618 val loss: 1.366560\n",
      "hidden_size: 200 lr: 5.000000e-01 lr_decay: 9.800000e-01 reg: 4.000000e-02 train accuracy: 0.830000 val accuracy: 0.821000\n",
      "iteration 0 / 2000: training loss 2.302596 val loss: 2.302728\n",
      "iteration 50 / 2000: training loss 1.388455 val loss: 1.453927\n",
      "iteration 100 / 2000: training loss 1.238276 val loss: 1.256257\n",
      "iteration 150 / 2000: training loss 1.426107 val loss: 1.290712\n",
      "iteration 200 / 2000: training loss 1.211992 val loss: 1.247552\n",
      "iteration 250 / 2000: training loss 1.229515 val loss: 1.440968\n",
      "iteration 300 / 2000: training loss 1.234614 val loss: 1.383981\n",
      "iteration 350 / 2000: training loss 1.354136 val loss: 1.195646\n",
      "iteration 400 / 2000: training loss 1.281426 val loss: 1.147464\n",
      "iteration 450 / 2000: training loss 1.201177 val loss: 1.173972\n",
      "iteration 500 / 2000: training loss 1.098689 val loss: 1.142420\n",
      "iteration 550 / 2000: training loss 1.118000 val loss: 1.112745\n",
      "iteration 600 / 2000: training loss 1.126380 val loss: 1.155312\n",
      "iteration 650 / 2000: training loss 1.045115 val loss: 1.117605\n",
      "iteration 700 / 2000: training loss 1.151112 val loss: 1.142205\n",
      "iteration 750 / 2000: training loss 1.100817 val loss: 1.083447\n",
      "iteration 800 / 2000: training loss 1.079898 val loss: 1.098296\n",
      "iteration 850 / 2000: training loss 1.131337 val loss: 1.080904\n",
      "iteration 900 / 2000: training loss 1.134519 val loss: 1.088623\n",
      "iteration 950 / 2000: training loss 1.004365 val loss: 1.081883\n",
      "iteration 1000 / 2000: training loss 1.100708 val loss: 1.092804\n",
      "iteration 1050 / 2000: training loss 1.040532 val loss: 1.081056\n",
      "iteration 1100 / 2000: training loss 1.049988 val loss: 1.074231\n",
      "iteration 1150 / 2000: training loss 1.010249 val loss: 1.108399\n",
      "iteration 1200 / 2000: training loss 1.093675 val loss: 1.058506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1250 / 2000: training loss 1.127845 val loss: 1.105452\n",
      "iteration 1300 / 2000: training loss 1.095621 val loss: 1.070757\n",
      "iteration 1350 / 2000: training loss 1.108470 val loss: 1.077882\n",
      "iteration 1400 / 2000: training loss 1.052129 val loss: 1.049112\n",
      "hidden_size: 200 lr: 5.000000e-01 lr_decay: 9.000000e-01 reg: 4.000000e-02 train accuracy: 0.940000 val accuracy: 0.898000\n",
      "iteration 0 / 2000: training loss 2.302596 val loss: 2.302728\n",
      "iteration 50 / 2000: training loss 1.408087 val loss: 1.335881\n",
      "iteration 100 / 2000: training loss 1.228512 val loss: 1.290263\n",
      "iteration 150 / 2000: training loss 1.391895 val loss: 1.275211\n",
      "iteration 200 / 2000: training loss 1.201206 val loss: 1.171918\n",
      "iteration 250 / 2000: training loss 1.252069 val loss: 1.191028\n",
      "iteration 300 / 2000: training loss 1.077684 val loss: 1.230387\n",
      "iteration 350 / 2000: training loss 1.246185 val loss: 1.219802\n",
      "iteration 400 / 2000: training loss 1.178271 val loss: 1.125183\n",
      "iteration 450 / 2000: training loss 1.179701 val loss: 1.146121\n",
      "iteration 500 / 2000: training loss 1.093037 val loss: 1.118322\n",
      "iteration 550 / 2000: training loss 1.054848 val loss: 1.118349\n",
      "iteration 600 / 2000: training loss 1.061998 val loss: 1.139853\n",
      "iteration 650 / 2000: training loss 0.981072 val loss: 1.055728\n",
      "iteration 700 / 2000: training loss 1.081112 val loss: 1.106300\n",
      "iteration 750 / 2000: training loss 1.102236 val loss: 1.064099\n",
      "iteration 800 / 2000: training loss 1.079237 val loss: 1.061193\n",
      "hidden_size: 200 lr: 5.000000e-01 lr_decay: 8.500000e-01 reg: 4.000000e-02 train accuracy: 0.870000 val accuracy: 0.882000\n",
      "iteration 0 / 2000: training loss 2.302596 val loss: 2.302728\n",
      "iteration 50 / 2000: training loss 1.342710 val loss: 1.297120\n",
      "iteration 100 / 2000: training loss 1.204643 val loss: 1.299242\n",
      "iteration 150 / 2000: training loss 1.395638 val loss: 1.338417\n",
      "iteration 200 / 2000: training loss 1.131049 val loss: 1.129901\n",
      "iteration 250 / 2000: training loss 1.189539 val loss: 1.166206\n",
      "iteration 300 / 2000: training loss 1.056864 val loss: 1.169892\n",
      "iteration 350 / 2000: training loss 1.168938 val loss: 1.135545\n",
      "iteration 400 / 2000: training loss 1.130877 val loss: 1.113766\n",
      "iteration 450 / 2000: training loss 1.156125 val loss: 1.104148\n",
      "iteration 500 / 2000: training loss 1.039101 val loss: 1.070175\n",
      "hidden_size: 200 lr: 5.000000e-01 lr_decay: 8.000000e-01 reg: 4.000000e-02 train accuracy: 0.900000 val accuracy: 0.844000\n",
      "iteration 0 / 2000: training loss 2.302601 val loss: 2.302732\n",
      "iteration 50 / 2000: training loss 2.404218 val loss: 1.869779\n",
      "iteration 100 / 2000: training loss 1.645262 val loss: 1.559446\n",
      "iteration 150 / 2000: training loss 1.662621 val loss: 1.667539\n",
      "iteration 200 / 2000: training loss 1.449052 val loss: 1.555266\n",
      "iteration 250 / 2000: training loss 1.607676 val loss: 1.825683\n",
      "iteration 300 / 2000: training loss 2.009778 val loss: 1.810934\n",
      "iteration 350 / 2000: training loss 1.659722 val loss: 1.630001\n",
      "iteration 400 / 2000: training loss 1.835314 val loss: 1.770250\n",
      "iteration 450 / 2000: training loss 1.492628 val loss: 1.637108\n",
      "iteration 500 / 2000: training loss 1.698528 val loss: 1.520187\n",
      "iteration 550 / 2000: training loss 1.651047 val loss: 1.586529\n",
      "iteration 600 / 2000: training loss 1.916432 val loss: 1.608810\n",
      "hidden_size: 200 lr: 5.000000e-01 lr_decay: 9.800000e-01 reg: 6.000000e-02 train accuracy: 0.720000 val accuracy: 0.679000\n",
      "iteration 0 / 2000: training loss 2.302601 val loss: 2.302732\n",
      "iteration 50 / 2000: training loss 1.625511 val loss: 1.541311\n",
      "iteration 100 / 2000: training loss 1.721776 val loss: 1.602198\n",
      "iteration 150 / 2000: training loss 1.604165 val loss: 1.505767\n",
      "iteration 200 / 2000: training loss 1.542106 val loss: 1.577253\n",
      "iteration 250 / 2000: training loss 1.509810 val loss: 1.583900\n",
      "iteration 300 / 2000: training loss 1.478044 val loss: 1.549684\n",
      "iteration 350 / 2000: training loss 1.512631 val loss: 1.390054\n",
      "iteration 400 / 2000: training loss 1.492530 val loss: 1.446883\n",
      "iteration 450 / 2000: training loss 1.473732 val loss: 1.452718\n",
      "iteration 500 / 2000: training loss 1.327886 val loss: 1.388716\n",
      "iteration 550 / 2000: training loss 1.351349 val loss: 1.378459\n",
      "iteration 600 / 2000: training loss 1.370618 val loss: 1.417518\n",
      "iteration 650 / 2000: training loss 1.344987 val loss: 1.384445\n",
      "iteration 700 / 2000: training loss 1.420707 val loss: 1.431240\n",
      "iteration 750 / 2000: training loss 1.336464 val loss: 1.341034\n",
      "iteration 800 / 2000: training loss 1.311066 val loss: 1.336738\n",
      "hidden_size: 200 lr: 5.000000e-01 lr_decay: 9.000000e-01 reg: 6.000000e-02 train accuracy: 0.860000 val accuracy: 0.824000\n",
      "iteration 0 / 2000: training loss 2.302601 val loss: 2.302732\n",
      "iteration 50 / 2000: training loss 1.654270 val loss: 1.507494\n",
      "iteration 100 / 2000: training loss 1.470211 val loss: 1.519681\n",
      "iteration 150 / 2000: training loss 1.602466 val loss: 1.412207\n",
      "iteration 200 / 2000: training loss 1.424529 val loss: 1.432196\n",
      "iteration 250 / 2000: training loss 1.518584 val loss: 1.422205\n",
      "iteration 300 / 2000: training loss 1.355747 val loss: 1.512130\n",
      "iteration 350 / 2000: training loss 1.615635 val loss: 1.484852\n",
      "iteration 400 / 2000: training loss 1.443612 val loss: 1.389923\n",
      "iteration 450 / 2000: training loss 1.394954 val loss: 1.381500\n",
      "iteration 500 / 2000: training loss 1.297795 val loss: 1.344866\n",
      "iteration 550 / 2000: training loss 1.296163 val loss: 1.365419\n",
      "iteration 600 / 2000: training loss 1.334221 val loss: 1.372751\n",
      "iteration 650 / 2000: training loss 1.219921 val loss: 1.292359\n",
      "iteration 700 / 2000: training loss 1.332919 val loss: 1.375590\n",
      "iteration 750 / 2000: training loss 1.374875 val loss: 1.317522\n",
      "iteration 800 / 2000: training loss 1.307041 val loss: 1.305643\n",
      "iteration 850 / 2000: training loss 1.335086 val loss: 1.315682\n",
      "iteration 900 / 2000: training loss 1.322770 val loss: 1.316889\n",
      "iteration 950 / 2000: training loss 1.257222 val loss: 1.302696\n",
      "iteration 1000 / 2000: training loss 1.265297 val loss: 1.314200\n",
      "iteration 1050 / 2000: training loss 1.263442 val loss: 1.281205\n",
      "iteration 1100 / 2000: training loss 1.270329 val loss: 1.292983\n",
      "iteration 1150 / 2000: training loss 1.226098 val loss: 1.303551\n",
      "iteration 1200 / 2000: training loss 1.282366 val loss: 1.273848\n",
      "iteration 1250 / 2000: training loss 1.314977 val loss: 1.276888\n",
      "iteration 1300 / 2000: training loss 1.241667 val loss: 1.279888\n",
      "iteration 1350 / 2000: training loss 1.368720 val loss: 1.286247\n",
      "iteration 1400 / 2000: training loss 1.284865 val loss: 1.267763\n",
      "iteration 1450 / 2000: training loss 1.328402 val loss: 1.267200\n",
      "iteration 1500 / 2000: training loss 1.263349 val loss: 1.263012\n",
      "iteration 1550 / 2000: training loss 1.253006 val loss: 1.263456\n",
      "iteration 1600 / 2000: training loss 1.303884 val loss: 1.264914\n",
      "iteration 1650 / 2000: training loss 1.259315 val loss: 1.266252\n",
      "iteration 1700 / 2000: training loss 1.226311 val loss: 1.262318\n",
      "iteration 1750 / 2000: training loss 1.172168 val loss: 1.258476\n",
      "iteration 1800 / 2000: training loss 1.302182 val loss: 1.259582\n",
      "iteration 1850 / 2000: training loss 1.272841 val loss: 1.254684\n",
      "hidden_size: 200 lr: 5.000000e-01 lr_decay: 8.500000e-01 reg: 6.000000e-02 train accuracy: 0.900000 val accuracy: 0.893000\n",
      "iteration 0 / 2000: training loss 2.302601 val loss: 2.302732\n",
      "iteration 50 / 2000: training loss 1.603921 val loss: 1.609206\n",
      "iteration 100 / 2000: training loss 1.494007 val loss: 1.529470\n",
      "iteration 150 / 2000: training loss 1.686571 val loss: 1.506502\n",
      "iteration 200 / 2000: training loss 1.420924 val loss: 1.371259\n",
      "iteration 250 / 2000: training loss 1.483780 val loss: 1.415934\n",
      "iteration 300 / 2000: training loss 1.305573 val loss: 1.411796\n",
      "iteration 350 / 2000: training loss 1.439305 val loss: 1.433262\n",
      "iteration 400 / 2000: training loss 1.391239 val loss: 1.369234\n",
      "iteration 450 / 2000: training loss 1.419513 val loss: 1.360536\n",
      "iteration 500 / 2000: training loss 1.269308 val loss: 1.333058\n",
      "iteration 550 / 2000: training loss 1.248035 val loss: 1.326635\n",
      "iteration 600 / 2000: training loss 1.254745 val loss: 1.322202\n",
      "iteration 650 / 2000: training loss 1.230013 val loss: 1.303231\n",
      "iteration 700 / 2000: training loss 1.291650 val loss: 1.298122\n",
      "iteration 750 / 2000: training loss 1.288915 val loss: 1.271667\n",
      "iteration 800 / 2000: training loss 1.295015 val loss: 1.294233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 850 / 2000: training loss 1.314427 val loss: 1.275791\n",
      "iteration 900 / 2000: training loss 1.258360 val loss: 1.287714\n",
      "iteration 950 / 2000: training loss 1.246065 val loss: 1.281496\n",
      "iteration 1000 / 2000: training loss 1.262792 val loss: 1.270785\n",
      "iteration 1050 / 2000: training loss 1.260257 val loss: 1.267561\n",
      "iteration 1100 / 2000: training loss 1.251066 val loss: 1.266056\n",
      "iteration 1150 / 2000: training loss 1.214189 val loss: 1.268630\n",
      "iteration 1200 / 2000: training loss 1.311934 val loss: 1.271218\n",
      "iteration 1250 / 2000: training loss 1.312005 val loss: 1.263916\n",
      "iteration 1300 / 2000: training loss 1.262779 val loss: 1.261513\n",
      "iteration 1350 / 2000: training loss 1.340332 val loss: 1.274218\n",
      "hidden_size: 200 lr: 5.000000e-01 lr_decay: 8.000000e-01 reg: 6.000000e-02 train accuracy: 0.820000 val accuracy: 0.894000\n",
      "iteration 0 / 2000: training loss 2.302588 val loss: 2.302847\n",
      "iteration 50 / 2000: training loss 1.758987 val loss: 1.717929\n",
      "iteration 100 / 2000: training loss 1.065586 val loss: 1.326136\n",
      "iteration 150 / 2000: training loss 1.274595 val loss: 1.003287\n",
      "iteration 200 / 2000: training loss 0.888197 val loss: 0.820644\n",
      "iteration 250 / 2000: training loss 1.099397 val loss: 1.138049\n",
      "iteration 300 / 2000: training loss 0.973112 val loss: 1.095607\n",
      "iteration 350 / 2000: training loss 1.356662 val loss: 1.306226\n",
      "iteration 400 / 2000: training loss 0.939511 val loss: 0.822990\n",
      "iteration 450 / 2000: training loss 0.853038 val loss: 0.868160\n",
      "iteration 500 / 2000: training loss 1.360400 val loss: 1.359844\n",
      "iteration 550 / 2000: training loss 1.099676 val loss: 1.347876\n",
      "iteration 600 / 2000: training loss 0.924260 val loss: 0.908904\n",
      "iteration 650 / 2000: training loss 0.776247 val loss: 1.236336\n",
      "iteration 700 / 2000: training loss 1.081414 val loss: 1.107348\n",
      "iteration 750 / 2000: training loss 1.469560 val loss: 1.169166\n",
      "iteration 800 / 2000: training loss 0.886798 val loss: 0.898148\n",
      "iteration 850 / 2000: training loss 0.932227 val loss: 0.861396\n",
      "iteration 900 / 2000: training loss 0.720955 val loss: 0.966973\n",
      "hidden_size: 200 lr: 7.000000e-01 lr_decay: 9.800000e-01 reg: 1.000000e-02 train accuracy: 0.920000 val accuracy: 0.838000\n",
      "iteration 0 / 2000: training loss 2.302588 val loss: 2.302847\n",
      "iteration 50 / 2000: training loss 2.005942 val loss: 1.857309\n",
      "iteration 100 / 2000: training loss 1.084052 val loss: 1.124589\n",
      "iteration 150 / 2000: training loss 1.292669 val loss: 0.944829\n",
      "iteration 200 / 2000: training loss 0.760135 val loss: 0.897952\n",
      "iteration 250 / 2000: training loss 0.816560 val loss: 0.996315\n",
      "iteration 300 / 2000: training loss 0.764375 val loss: 0.796122\n",
      "iteration 350 / 2000: training loss 0.719229 val loss: 0.660044\n",
      "iteration 400 / 2000: training loss 0.674714 val loss: 0.701455\n",
      "iteration 450 / 2000: training loss 0.715092 val loss: 0.751962\n",
      "iteration 500 / 2000: training loss 0.582893 val loss: 0.655304\n",
      "iteration 550 / 2000: training loss 0.691784 val loss: 0.691093\n",
      "iteration 600 / 2000: training loss 0.557229 val loss: 0.672675\n",
      "iteration 650 / 2000: training loss 0.538093 val loss: 0.611617\n",
      "iteration 700 / 2000: training loss 0.768198 val loss: 0.699896\n",
      "iteration 750 / 2000: training loss 0.565968 val loss: 0.607370\n",
      "iteration 800 / 2000: training loss 0.603338 val loss: 0.627886\n",
      "hidden_size: 200 lr: 7.000000e-01 lr_decay: 9.000000e-01 reg: 1.000000e-02 train accuracy: 0.950000 val accuracy: 0.902000\n",
      "iteration 0 / 2000: training loss 2.302588 val loss: 2.302847\n",
      "iteration 50 / 2000: training loss 1.557826 val loss: 1.573633\n",
      "iteration 100 / 2000: training loss 0.748750 val loss: 0.889616\n",
      "iteration 150 / 2000: training loss 1.121840 val loss: 0.943167\n",
      "iteration 200 / 2000: training loss 0.739810 val loss: 0.740766\n",
      "iteration 250 / 2000: training loss 0.808248 val loss: 0.731311\n",
      "iteration 300 / 2000: training loss 0.693754 val loss: 0.825237\n",
      "iteration 350 / 2000: training loss 0.676202 val loss: 0.703318\n",
      "iteration 400 / 2000: training loss 0.567862 val loss: 0.608780\n",
      "iteration 450 / 2000: training loss 0.634983 val loss: 0.607201\n",
      "iteration 500 / 2000: training loss 0.576676 val loss: 0.608970\n",
      "iteration 550 / 2000: training loss 0.561288 val loss: 0.605102\n",
      "iteration 600 / 2000: training loss 0.467978 val loss: 0.610943\n",
      "iteration 650 / 2000: training loss 0.466989 val loss: 0.551783\n",
      "iteration 700 / 2000: training loss 0.568335 val loss: 0.546355\n",
      "iteration 750 / 2000: training loss 0.546733 val loss: 0.536816\n",
      "iteration 800 / 2000: training loss 0.595020 val loss: 0.541492\n",
      "iteration 850 / 2000: training loss 0.535680 val loss: 0.567418\n",
      "iteration 900 / 2000: training loss 0.522879 val loss: 0.533569\n",
      "iteration 950 / 2000: training loss 0.497672 val loss: 0.527384\n",
      "iteration 1000 / 2000: training loss 0.488076 val loss: 0.518117\n",
      "iteration 1050 / 2000: training loss 0.489979 val loss: 0.542720\n",
      "iteration 1100 / 2000: training loss 0.535887 val loss: 0.531936\n",
      "iteration 1150 / 2000: training loss 0.443937 val loss: 0.525836\n",
      "iteration 1200 / 2000: training loss 0.531376 val loss: 0.525775\n",
      "iteration 1250 / 2000: training loss 0.557392 val loss: 0.529411\n",
      "hidden_size: 200 lr: 7.000000e-01 lr_decay: 8.500000e-01 reg: 1.000000e-02 train accuracy: 0.970000 val accuracy: 0.935000\n",
      "iteration 0 / 2000: training loss 2.302588 val loss: 2.302847\n",
      "iteration 50 / 2000: training loss 1.362714 val loss: 1.287152\n",
      "iteration 100 / 2000: training loss 1.098567 val loss: 0.994102\n",
      "iteration 150 / 2000: training loss 1.015595 val loss: 0.864508\n",
      "iteration 200 / 2000: training loss 0.643137 val loss: 0.690705\n",
      "iteration 250 / 2000: training loss 0.741669 val loss: 0.699958\n",
      "iteration 300 / 2000: training loss 0.623878 val loss: 0.674713\n",
      "iteration 350 / 2000: training loss 0.582177 val loss: 0.690155\n",
      "iteration 400 / 2000: training loss 0.579571 val loss: 0.567246\n",
      "iteration 450 / 2000: training loss 0.625358 val loss: 0.597562\n",
      "iteration 500 / 2000: training loss 0.544306 val loss: 0.568199\n",
      "iteration 550 / 2000: training loss 0.531299 val loss: 0.533226\n",
      "iteration 600 / 2000: training loss 0.449569 val loss: 0.537722\n",
      "iteration 650 / 2000: training loss 0.435019 val loss: 0.528067\n",
      "iteration 700 / 2000: training loss 0.537574 val loss: 0.518481\n",
      "iteration 750 / 2000: training loss 0.508118 val loss: 0.524654\n",
      "iteration 800 / 2000: training loss 0.514575 val loss: 0.537261\n",
      "iteration 850 / 2000: training loss 0.500915 val loss: 0.525255\n",
      "iteration 900 / 2000: training loss 0.472843 val loss: 0.517846\n",
      "iteration 950 / 2000: training loss 0.468896 val loss: 0.505903\n",
      "iteration 1000 / 2000: training loss 0.478105 val loss: 0.503265\n",
      "iteration 1050 / 2000: training loss 0.476961 val loss: 0.502841\n",
      "iteration 1100 / 2000: training loss 0.502081 val loss: 0.507550\n",
      "iteration 1150 / 2000: training loss 0.445905 val loss: 0.506957\n",
      "iteration 1200 / 2000: training loss 0.509913 val loss: 0.507015\n",
      "iteration 1250 / 2000: training loss 0.539216 val loss: 0.510716\n",
      "iteration 1300 / 2000: training loss 0.477335 val loss: 0.501738\n",
      "iteration 1350 / 2000: training loss 0.526303 val loss: 0.506092\n",
      "iteration 1400 / 2000: training loss 0.438948 val loss: 0.500657\n",
      "iteration 1450 / 2000: training loss 0.506941 val loss: 0.506573\n",
      "iteration 1500 / 2000: training loss 0.474560 val loss: 0.497433\n",
      "iteration 1550 / 2000: training loss 0.543925 val loss: 0.499243\n",
      "iteration 1600 / 2000: training loss 0.496926 val loss: 0.495258\n",
      "iteration 1650 / 2000: training loss 0.492541 val loss: 0.494696\n",
      "iteration 1700 / 2000: training loss 0.460952 val loss: 0.495213\n",
      "iteration 1750 / 2000: training loss 0.420150 val loss: 0.495633\n",
      "iteration 1800 / 2000: training loss 0.509443 val loss: 0.494606\n",
      "iteration 1850 / 2000: training loss 0.478580 val loss: 0.494954\n",
      "iteration 1900 / 2000: training loss 0.401236 val loss: 0.494900\n",
      "iteration 1950 / 2000: training loss 0.468966 val loss: 0.495457\n",
      "hidden_size: 200 lr: 7.000000e-01 lr_decay: 8.000000e-01 reg: 1.000000e-02 train accuracy: 0.950000 val accuracy: 0.949000\n",
      "iteration 0 / 2000: training loss 2.302590 val loss: 2.302850\n",
      "iteration 50 / 2000: training loss 2.384032 val loss: 2.111607\n",
      "iteration 100 / 2000: training loss 1.353994 val loss: 1.620483\n",
      "iteration 150 / 2000: training loss 1.301976 val loss: 1.364864\n",
      "iteration 200 / 2000: training loss 1.392184 val loss: 1.197530\n",
      "iteration 250 / 2000: training loss 1.440827 val loss: 1.643490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 300 / 2000: training loss 1.297822 val loss: 1.571701\n",
      "iteration 350 / 2000: training loss 1.115931 val loss: 1.168189\n",
      "iteration 400 / 2000: training loss 1.428542 val loss: 1.562009\n",
      "iteration 450 / 2000: training loss 1.442326 val loss: 1.466097\n",
      "iteration 500 / 2000: training loss 1.416217 val loss: 1.519222\n",
      "iteration 550 / 2000: training loss 1.149942 val loss: 1.116533\n",
      "iteration 600 / 2000: training loss 1.236630 val loss: 1.411216\n",
      "iteration 650 / 2000: training loss 1.222583 val loss: 1.410545\n",
      "iteration 700 / 2000: training loss 1.631080 val loss: 1.396390\n",
      "iteration 750 / 2000: training loss 1.457986 val loss: 1.526093\n",
      "iteration 800 / 2000: training loss 1.238670 val loss: 1.456884\n",
      "iteration 850 / 2000: training loss 1.271316 val loss: 1.141560\n",
      "iteration 900 / 2000: training loss 1.744823 val loss: 1.512295\n",
      "iteration 950 / 2000: training loss 0.973934 val loss: 1.445940\n",
      "iteration 1000 / 2000: training loss 1.414053 val loss: 1.503834\n",
      "iteration 1050 / 2000: training loss 0.899847 val loss: 1.052856\n",
      "iteration 1100 / 2000: training loss 1.146974 val loss: 1.080446\n",
      "iteration 1150 / 2000: training loss 0.938006 val loss: 1.080545\n",
      "iteration 1200 / 2000: training loss 1.016457 val loss: 1.141024\n",
      "iteration 1250 / 2000: training loss 1.044534 val loss: 1.019232\n",
      "iteration 1300 / 2000: training loss 1.118716 val loss: 1.089693\n",
      "iteration 1350 / 2000: training loss 1.171423 val loss: 1.262273\n",
      "iteration 1400 / 2000: training loss 1.016453 val loss: 1.088025\n",
      "iteration 1450 / 2000: training loss 1.205856 val loss: 1.090728\n",
      "iteration 1500 / 2000: training loss 1.068568 val loss: 1.085515\n",
      "iteration 1550 / 2000: training loss 1.335365 val loss: 1.086905\n",
      "iteration 1600 / 2000: training loss 1.243056 val loss: 1.158444\n",
      "hidden_size: 200 lr: 7.000000e-01 lr_decay: 9.800000e-01 reg: 2.000000e-02 train accuracy: 0.830000 val accuracy: 0.751000\n",
      "iteration 0 / 2000: training loss 2.302590 val loss: 2.302850\n",
      "iteration 50 / 2000: training loss 2.231320 val loss: 2.150787\n",
      "iteration 100 / 2000: training loss 1.216187 val loss: 1.235804\n",
      "iteration 150 / 2000: training loss 1.168508 val loss: 1.435173\n",
      "iteration 200 / 2000: training loss 1.074109 val loss: 1.136039\n",
      "iteration 250 / 2000: training loss 1.056810 val loss: 1.137554\n",
      "iteration 300 / 2000: training loss 0.940170 val loss: 1.103840\n",
      "iteration 350 / 2000: training loss 0.957139 val loss: 0.965411\n",
      "iteration 400 / 2000: training loss 0.939457 val loss: 0.845773\n",
      "iteration 450 / 2000: training loss 1.001938 val loss: 0.920126\n",
      "iteration 500 / 2000: training loss 0.801828 val loss: 0.862137\n",
      "iteration 550 / 2000: training loss 0.851922 val loss: 0.873359\n",
      "iteration 600 / 2000: training loss 0.766802 val loss: 0.872484\n",
      "iteration 650 / 2000: training loss 0.769140 val loss: 0.847697\n",
      "iteration 700 / 2000: training loss 0.980428 val loss: 0.848870\n",
      "iteration 750 / 2000: training loss 0.786839 val loss: 0.808715\n",
      "iteration 800 / 2000: training loss 0.792963 val loss: 0.836940\n",
      "iteration 850 / 2000: training loss 0.851808 val loss: 0.826524\n",
      "iteration 900 / 2000: training loss 0.816069 val loss: 0.820639\n",
      "iteration 950 / 2000: training loss 0.718263 val loss: 0.803648\n",
      "iteration 1000 / 2000: training loss 0.827583 val loss: 0.832714\n",
      "iteration 1050 / 2000: training loss 0.759325 val loss: 0.804604\n",
      "iteration 1100 / 2000: training loss 0.765279 val loss: 0.779622\n",
      "iteration 1150 / 2000: training loss 0.712561 val loss: 0.781674\n",
      "hidden_size: 200 lr: 7.000000e-01 lr_decay: 9.000000e-01 reg: 2.000000e-02 train accuracy: 0.930000 val accuracy: 0.908000\n",
      "iteration 0 / 2000: training loss 2.302590 val loss: 2.302850\n",
      "iteration 50 / 2000: training loss 1.713867 val loss: 1.772917\n",
      "iteration 100 / 2000: training loss 1.072931 val loss: 1.256522\n",
      "iteration 150 / 2000: training loss 1.304760 val loss: 1.074824\n",
      "iteration 200 / 2000: training loss 1.000252 val loss: 1.053413\n",
      "iteration 250 / 2000: training loss 1.065644 val loss: 1.042071\n",
      "iteration 300 / 2000: training loss 0.976904 val loss: 1.046406\n",
      "iteration 350 / 2000: training loss 0.932414 val loss: 0.916261\n",
      "iteration 400 / 2000: training loss 0.852470 val loss: 0.823735\n",
      "iteration 450 / 2000: training loss 0.876687 val loss: 0.922779\n",
      "iteration 500 / 2000: training loss 0.785850 val loss: 0.853002\n",
      "iteration 550 / 2000: training loss 0.795147 val loss: 0.834129\n",
      "iteration 600 / 2000: training loss 0.780412 val loss: 0.863361\n",
      "iteration 650 / 2000: training loss 0.690064 val loss: 0.764113\n",
      "iteration 700 / 2000: training loss 0.818254 val loss: 0.830658\n",
      "iteration 750 / 2000: training loss 0.776941 val loss: 0.743083\n",
      "iteration 800 / 2000: training loss 0.777267 val loss: 0.770992\n",
      "iteration 850 / 2000: training loss 0.790176 val loss: 0.773296\n",
      "iteration 900 / 2000: training loss 0.755984 val loss: 0.751571\n",
      "iteration 950 / 2000: training loss 0.705544 val loss: 0.753390\n",
      "iteration 1000 / 2000: training loss 0.709467 val loss: 0.753552\n",
      "iteration 1050 / 2000: training loss 0.718299 val loss: 0.760886\n",
      "iteration 1100 / 2000: training loss 0.755897 val loss: 0.755816\n",
      "iteration 1150 / 2000: training loss 0.680489 val loss: 0.750317\n",
      "hidden_size: 200 lr: 7.000000e-01 lr_decay: 8.500000e-01 reg: 2.000000e-02 train accuracy: 0.930000 val accuracy: 0.915000\n",
      "iteration 0 / 2000: training loss 2.302590 val loss: 2.302850\n",
      "iteration 50 / 2000: training loss 1.338299 val loss: 1.261471\n",
      "iteration 100 / 2000: training loss 0.984517 val loss: 1.159158\n",
      "iteration 150 / 2000: training loss 1.320925 val loss: 1.072942\n",
      "iteration 200 / 2000: training loss 0.872572 val loss: 0.925288\n",
      "iteration 250 / 2000: training loss 0.931072 val loss: 1.054136\n",
      "iteration 300 / 2000: training loss 0.803822 val loss: 0.946879\n",
      "iteration 350 / 2000: training loss 0.931939 val loss: 0.873952\n",
      "iteration 400 / 2000: training loss 0.800913 val loss: 0.793256\n",
      "iteration 450 / 2000: training loss 0.831058 val loss: 0.798910\n",
      "iteration 500 / 2000: training loss 0.738650 val loss: 0.765875\n",
      "iteration 550 / 2000: training loss 0.719469 val loss: 0.776157\n",
      "iteration 600 / 2000: training loss 0.664545 val loss: 0.784984\n",
      "iteration 650 / 2000: training loss 0.663880 val loss: 0.755082\n",
      "iteration 700 / 2000: training loss 0.748565 val loss: 0.732923\n",
      "iteration 750 / 2000: training loss 0.727190 val loss: 0.731126\n",
      "iteration 800 / 2000: training loss 0.747692 val loss: 0.746813\n",
      "iteration 850 / 2000: training loss 0.726394 val loss: 0.741908\n",
      "iteration 900 / 2000: training loss 0.701371 val loss: 0.740543\n",
      "iteration 950 / 2000: training loss 0.691183 val loss: 0.728774\n",
      "iteration 1000 / 2000: training loss 0.699152 val loss: 0.722042\n",
      "iteration 1050 / 2000: training loss 0.696572 val loss: 0.721614\n",
      "iteration 1100 / 2000: training loss 0.718879 val loss: 0.722648\n",
      "iteration 1150 / 2000: training loss 0.664783 val loss: 0.721441\n",
      "iteration 1200 / 2000: training loss 0.749093 val loss: 0.724254\n",
      "iteration 1250 / 2000: training loss 0.762674 val loss: 0.721461\n",
      "iteration 1300 / 2000: training loss 0.706974 val loss: 0.714397\n",
      "iteration 1350 / 2000: training loss 0.762972 val loss: 0.721734\n",
      "iteration 1400 / 2000: training loss 0.674865 val loss: 0.712413\n",
      "iteration 1450 / 2000: training loss 0.745369 val loss: 0.717477\n",
      "iteration 1500 / 2000: training loss 0.687488 val loss: 0.711247\n",
      "iteration 1550 / 2000: training loss 0.739099 val loss: 0.711870\n",
      "iteration 1600 / 2000: training loss 0.725970 val loss: 0.707261\n",
      "iteration 1650 / 2000: training loss 0.710500 val loss: 0.706266\n",
      "iteration 1700 / 2000: training loss 0.666956 val loss: 0.707990\n",
      "iteration 1750 / 2000: training loss 0.636948 val loss: 0.708714\n",
      "iteration 1800 / 2000: training loss 0.736659 val loss: 0.706168\n",
      "iteration 1850 / 2000: training loss 0.704467 val loss: 0.706381\n",
      "iteration 1900 / 2000: training loss 0.618833 val loss: 0.706334\n",
      "iteration 1950 / 2000: training loss 0.700334 val loss: 0.706640\n",
      "hidden_size: 200 lr: 7.000000e-01 lr_decay: 8.000000e-01 reg: 2.000000e-02 train accuracy: 0.940000 val accuracy: 0.934000\n",
      "iteration 0 / 2000: training loss 2.302596 val loss: 2.302855\n",
      "iteration 50 / 2000: training loss 2.358903 val loss: 2.356866\n",
      "iteration 100 / 2000: training loss 1.756770 val loss: 1.873908\n",
      "iteration 150 / 2000: training loss 2.138018 val loss: 2.304589\n",
      "iteration 200 / 2000: training loss 2.890586 val loss: 2.790536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 250 / 2000: training loss 2.256809 val loss: 2.499569\n",
      "iteration 300 / 2000: training loss 1.852594 val loss: 1.860264\n",
      "iteration 350 / 2000: training loss 1.669225 val loss: 1.691708\n",
      "iteration 400 / 2000: training loss 1.597000 val loss: 1.648354\n",
      "iteration 450 / 2000: training loss 1.996662 val loss: 2.054837\n",
      "iteration 500 / 2000: training loss 1.949428 val loss: 2.027025\n",
      "iteration 550 / 2000: training loss 2.479291 val loss: 2.913841\n",
      "iteration 600 / 2000: training loss 2.115315 val loss: 1.989516\n",
      "iteration 650 / 2000: training loss 1.470422 val loss: 1.568618\n",
      "iteration 700 / 2000: training loss 2.502733 val loss: 2.101337\n",
      "iteration 750 / 2000: training loss 2.536919 val loss: 2.450861\n",
      "iteration 800 / 2000: training loss 1.940883 val loss: 1.815099\n",
      "iteration 850 / 2000: training loss 2.015020 val loss: 2.097015\n",
      "iteration 900 / 2000: training loss 1.820590 val loss: 1.982120\n",
      "iteration 950 / 2000: training loss 1.858013 val loss: 1.854608\n",
      "iteration 1000 / 2000: training loss 1.685546 val loss: 1.797573\n",
      "iteration 1050 / 2000: training loss 2.840804 val loss: 3.317188\n",
      "iteration 1100 / 2000: training loss 2.236105 val loss: 1.732024\n",
      "iteration 1150 / 2000: training loss 1.740177 val loss: 1.857199\n",
      "iteration 1200 / 2000: training loss 1.655434 val loss: 1.831354\n",
      "iteration 1250 / 2000: training loss 1.954462 val loss: 1.646930\n",
      "hidden_size: 200 lr: 7.000000e-01 lr_decay: 9.800000e-01 reg: 4.000000e-02 train accuracy: 0.680000 val accuracy: 0.637000\n",
      "iteration 0 / 2000: training loss 2.302596 val loss: 2.302855\n",
      "iteration 50 / 2000: training loss 2.099022 val loss: 1.959599\n",
      "iteration 100 / 2000: training loss 1.904699 val loss: 1.702994\n",
      "iteration 150 / 2000: training loss 1.782748 val loss: 1.581522\n",
      "iteration 200 / 2000: training loss 1.755091 val loss: 1.620078\n",
      "iteration 250 / 2000: training loss 1.382331 val loss: 1.586406\n",
      "iteration 300 / 2000: training loss 1.613977 val loss: 1.521487\n",
      "iteration 350 / 2000: training loss 1.293164 val loss: 1.367858\n",
      "iteration 400 / 2000: training loss 1.326578 val loss: 1.267859\n",
      "iteration 450 / 2000: training loss 1.310244 val loss: 1.222908\n",
      "iteration 500 / 2000: training loss 1.248221 val loss: 1.325268\n",
      "iteration 550 / 2000: training loss 1.184306 val loss: 1.257272\n",
      "iteration 600 / 2000: training loss 1.184122 val loss: 1.331902\n",
      "iteration 650 / 2000: training loss 1.055454 val loss: 1.158722\n",
      "iteration 700 / 2000: training loss 1.268057 val loss: 1.324992\n",
      "iteration 750 / 2000: training loss 1.206440 val loss: 1.185274\n",
      "iteration 800 / 2000: training loss 1.101355 val loss: 1.137445\n",
      "iteration 850 / 2000: training loss 1.168876 val loss: 1.152472\n",
      "iteration 900 / 2000: training loss 1.120625 val loss: 1.134735\n",
      "iteration 950 / 2000: training loss 1.048160 val loss: 1.160029\n",
      "iteration 1000 / 2000: training loss 1.113644 val loss: 1.242528\n",
      "iteration 1050 / 2000: training loss 1.132767 val loss: 1.118033\n",
      "iteration 1100 / 2000: training loss 1.047909 val loss: 1.104608\n",
      "iteration 1150 / 2000: training loss 1.037972 val loss: 1.135063\n",
      "iteration 1200 / 2000: training loss 1.117721 val loss: 1.076928\n",
      "iteration 1250 / 2000: training loss 1.105536 val loss: 1.081343\n",
      "iteration 1300 / 2000: training loss 1.174454 val loss: 1.110093\n",
      "iteration 1350 / 2000: training loss 1.113539 val loss: 1.072948\n",
      "iteration 1400 / 2000: training loss 1.093364 val loss: 1.080200\n",
      "iteration 1450 / 2000: training loss 1.138530 val loss: 1.099129\n",
      "iteration 1500 / 2000: training loss 1.073845 val loss: 1.047678\n",
      "iteration 1550 / 2000: training loss 1.073244 val loss: 1.047926\n",
      "iteration 1600 / 2000: training loss 1.150280 val loss: 1.098740\n",
      "iteration 1650 / 2000: training loss 1.040182 val loss: 1.060269\n",
      "iteration 1700 / 2000: training loss 1.009508 val loss: 1.063262\n",
      "hidden_size: 200 lr: 7.000000e-01 lr_decay: 9.000000e-01 reg: 4.000000e-02 train accuracy: 0.870000 val accuracy: 0.896000\n",
      "iteration 0 / 2000: training loss 2.302596 val loss: 2.302855\n",
      "iteration 50 / 2000: training loss 2.296895 val loss: 2.107997\n",
      "iteration 100 / 2000: training loss 1.648748 val loss: 1.640480\n",
      "iteration 150 / 2000: training loss 1.510774 val loss: 1.483880\n",
      "iteration 200 / 2000: training loss 1.321398 val loss: 1.359947\n",
      "iteration 250 / 2000: training loss 1.394060 val loss: 1.444704\n",
      "iteration 300 / 2000: training loss 1.226543 val loss: 1.299622\n",
      "iteration 350 / 2000: training loss 1.275933 val loss: 1.219841\n",
      "iteration 400 / 2000: training loss 1.240688 val loss: 1.175644\n",
      "iteration 450 / 2000: training loss 1.217354 val loss: 1.211540\n",
      "iteration 500 / 2000: training loss 1.084915 val loss: 1.115903\n",
      "iteration 550 / 2000: training loss 1.111885 val loss: 1.116412\n",
      "iteration 600 / 2000: training loss 1.098342 val loss: 1.135472\n",
      "iteration 650 / 2000: training loss 1.006787 val loss: 1.091616\n",
      "iteration 700 / 2000: training loss 1.151361 val loss: 1.118016\n",
      "iteration 750 / 2000: training loss 1.109635 val loss: 1.066973\n",
      "iteration 800 / 2000: training loss 1.068434 val loss: 1.086768\n",
      "hidden_size: 200 lr: 7.000000e-01 lr_decay: 8.500000e-01 reg: 4.000000e-02 train accuracy: 0.920000 val accuracy: 0.874000\n",
      "iteration 0 / 2000: training loss 2.302596 val loss: 2.302855\n",
      "iteration 50 / 2000: training loss 1.682425 val loss: 1.555471\n",
      "iteration 100 / 2000: training loss 1.478407 val loss: 1.459046\n",
      "iteration 150 / 2000: training loss 1.534081 val loss: 1.433672\n",
      "iteration 200 / 2000: training loss 1.189487 val loss: 1.279650\n",
      "iteration 250 / 2000: training loss 1.249327 val loss: 1.334306\n",
      "iteration 300 / 2000: training loss 1.116580 val loss: 1.299436\n",
      "iteration 350 / 2000: training loss 1.284271 val loss: 1.212382\n",
      "iteration 400 / 2000: training loss 1.162417 val loss: 1.127144\n",
      "iteration 450 / 2000: training loss 1.174610 val loss: 1.164939\n",
      "iteration 500 / 2000: training loss 1.087950 val loss: 1.116439\n",
      "iteration 550 / 2000: training loss 1.036722 val loss: 1.105970\n",
      "iteration 600 / 2000: training loss 1.040411 val loss: 1.126618\n",
      "iteration 650 / 2000: training loss 0.984052 val loss: 1.058412\n",
      "iteration 700 / 2000: training loss 1.063231 val loss: 1.083818\n",
      "iteration 750 / 2000: training loss 1.062086 val loss: 1.045807\n",
      "iteration 800 / 2000: training loss 1.082048 val loss: 1.057194\n",
      "iteration 850 / 2000: training loss 1.058804 val loss: 1.065262\n",
      "iteration 900 / 2000: training loss 1.037095 val loss: 1.054211\n",
      "iteration 950 / 2000: training loss 1.019033 val loss: 1.048183\n",
      "iteration 1000 / 2000: training loss 1.021190 val loss: 1.040541\n",
      "iteration 1050 / 2000: training loss 1.020674 val loss: 1.042461\n",
      "hidden_size: 200 lr: 7.000000e-01 lr_decay: 8.000000e-01 reg: 4.000000e-02 train accuracy: 0.880000 val accuracy: 0.911000\n",
      "iteration 0 / 2000: training loss 2.302601 val loss: 2.302859\n",
      "iteration 50 / 2000: training loss 3.094477 val loss: 3.285037\n",
      "iteration 100 / 2000: training loss 7.417597 val loss: 7.375229\n",
      "iteration 150 / 2000: training loss 15.662858 val loss: 9.048599\n",
      "iteration 200 / 2000: training loss nan val loss: nan\n",
      "iteration 250 / 2000: training loss nan val loss: nan\n",
      "iteration 300 / 2000: training loss nan val loss: nan\n",
      "iteration 350 / 2000: training loss nan val loss: nan\n",
      "iteration 400 / 2000: training loss nan val loss: nan\n",
      "iteration 450 / 2000: training loss nan val loss: nan\n",
      "iteration 500 / 2000: training loss nan val loss: nan\n",
      "iteration 550 / 2000: training loss nan val loss: nan\n",
      "iteration 600 / 2000: training loss nan val loss: nan\n",
      "iteration 650 / 2000: training loss nan val loss: nan\n",
      "iteration 700 / 2000: training loss nan val loss: nan\n",
      "iteration 750 / 2000: training loss nan val loss: nan\n",
      "iteration 800 / 2000: training loss nan val loss: nan\n",
      "iteration 850 / 2000: training loss nan val loss: nan\n",
      "iteration 900 / 2000: training loss nan val loss: nan\n",
      "iteration 950 / 2000: training loss nan val loss: nan\n",
      "iteration 1000 / 2000: training loss nan val loss: nan\n",
      "iteration 1050 / 2000: training loss nan val loss: nan\n",
      "iteration 1100 / 2000: training loss nan val loss: nan\n",
      "iteration 1150 / 2000: training loss nan val loss: nan\n",
      "iteration 1200 / 2000: training loss nan val loss: nan\n",
      "iteration 1250 / 2000: training loss nan val loss: nan\n",
      "iteration 1300 / 2000: training loss nan val loss: nan\n",
      "iteration 1350 / 2000: training loss nan val loss: nan\n",
      "iteration 1400 / 2000: training loss nan val loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1450 / 2000: training loss nan val loss: nan\n",
      "iteration 1500 / 2000: training loss nan val loss: nan\n",
      "iteration 1550 / 2000: training loss nan val loss: nan\n",
      "iteration 1600 / 2000: training loss nan val loss: nan\n",
      "iteration 1650 / 2000: training loss nan val loss: nan\n",
      "iteration 1700 / 2000: training loss nan val loss: nan\n",
      "iteration 1750 / 2000: training loss nan val loss: nan\n",
      "iteration 1800 / 2000: training loss nan val loss: nan\n",
      "iteration 1850 / 2000: training loss nan val loss: nan\n",
      "iteration 1900 / 2000: training loss nan val loss: nan\n",
      "iteration 1950 / 2000: training loss nan val loss: nan\n",
      "hidden_size: 200 lr: 7.000000e-01 lr_decay: 9.800000e-01 reg: 6.000000e-02 train accuracy: 0.080000 val accuracy: 0.100000\n",
      "iteration 0 / 2000: training loss 2.302601 val loss: 2.302859\n",
      "iteration 50 / 2000: training loss 2.633135 val loss: 2.587295\n",
      "iteration 100 / 2000: training loss 2.238436 val loss: 1.831848\n",
      "iteration 150 / 2000: training loss 1.967852 val loss: 1.705533\n",
      "iteration 200 / 2000: training loss 1.604829 val loss: 1.831927\n",
      "iteration 250 / 2000: training loss 1.637945 val loss: 1.866981\n",
      "iteration 300 / 2000: training loss 1.792059 val loss: 1.619524\n",
      "iteration 350 / 2000: training loss 1.605061 val loss: 1.665193\n",
      "iteration 400 / 2000: training loss 1.866538 val loss: 1.704663\n",
      "iteration 450 / 2000: training loss 1.493521 val loss: 1.484681\n",
      "iteration 500 / 2000: training loss 1.577941 val loss: 1.540794\n",
      "iteration 550 / 2000: training loss 1.578982 val loss: 1.570508\n",
      "iteration 600 / 2000: training loss 1.488199 val loss: 1.584370\n",
      "iteration 650 / 2000: training loss 1.365145 val loss: 1.457665\n",
      "iteration 700 / 2000: training loss 1.516076 val loss: 1.576033\n",
      "iteration 750 / 2000: training loss 1.468991 val loss: 1.418495\n",
      "iteration 800 / 2000: training loss 1.371079 val loss: 1.411787\n",
      "iteration 850 / 2000: training loss 1.432201 val loss: 1.398766\n",
      "iteration 900 / 2000: training loss 1.394754 val loss: 1.403954\n",
      "iteration 950 / 2000: training loss 1.303539 val loss: 1.467563\n",
      "iteration 1000 / 2000: training loss 1.361921 val loss: 1.489566\n",
      "iteration 1050 / 2000: training loss 1.369598 val loss: 1.368222\n",
      "iteration 1100 / 2000: training loss 1.289750 val loss: 1.358204\n",
      "iteration 1150 / 2000: training loss 1.290389 val loss: 1.398938\n",
      "iteration 1200 / 2000: training loss 1.377359 val loss: 1.332627\n",
      "iteration 1250 / 2000: training loss 1.351588 val loss: 1.348459\n",
      "iteration 1300 / 2000: training loss 1.427645 val loss: 1.367929\n",
      "iteration 1350 / 2000: training loss 1.344286 val loss: 1.326217\n",
      "iteration 1400 / 2000: training loss 1.345945 val loss: 1.330060\n",
      "iteration 1450 / 2000: training loss 1.422765 val loss: 1.356628\n",
      "iteration 1500 / 2000: training loss 1.294094 val loss: 1.292774\n",
      "iteration 1550 / 2000: training loss 1.298914 val loss: 1.289511\n",
      "iteration 1600 / 2000: training loss 1.388289 val loss: 1.319093\n",
      "iteration 1650 / 2000: training loss 1.276012 val loss: 1.302713\n",
      "iteration 1700 / 2000: training loss 1.246546 val loss: 1.293975\n",
      "hidden_size: 200 lr: 7.000000e-01 lr_decay: 9.000000e-01 reg: 6.000000e-02 train accuracy: 0.830000 val accuracy: 0.857000\n",
      "iteration 0 / 2000: training loss 2.302601 val loss: 2.302859\n",
      "iteration 50 / 2000: training loss 2.766020 val loss: 2.538795\n",
      "iteration 100 / 2000: training loss 1.957871 val loss: 1.819610\n",
      "iteration 150 / 2000: training loss 1.788119 val loss: 1.714078\n",
      "iteration 200 / 2000: training loss 1.515183 val loss: 1.605924\n",
      "iteration 250 / 2000: training loss 1.615402 val loss: 1.773618\n",
      "iteration 300 / 2000: training loss 1.462796 val loss: 1.578763\n",
      "iteration 350 / 2000: training loss 1.518051 val loss: 1.461164\n",
      "iteration 400 / 2000: training loss 1.480876 val loss: 1.440423\n",
      "iteration 450 / 2000: training loss 1.487397 val loss: 1.391679\n",
      "iteration 500 / 2000: training loss 1.329893 val loss: 1.369958\n",
      "iteration 550 / 2000: training loss 1.324268 val loss: 1.380157\n",
      "iteration 600 / 2000: training loss 1.366949 val loss: 1.395321\n",
      "iteration 650 / 2000: training loss 1.274886 val loss: 1.347165\n",
      "iteration 700 / 2000: training loss 1.425641 val loss: 1.440274\n",
      "iteration 750 / 2000: training loss 1.341890 val loss: 1.312848\n",
      "iteration 800 / 2000: training loss 1.306463 val loss: 1.316580\n",
      "hidden_size: 200 lr: 7.000000e-01 lr_decay: 8.500000e-01 reg: 6.000000e-02 train accuracy: 0.870000 val accuracy: 0.824000\n",
      "iteration 0 / 2000: training loss 2.302601 val loss: 2.302859\n",
      "iteration 50 / 2000: training loss 2.062614 val loss: 2.048741\n",
      "iteration 100 / 2000: training loss 2.093314 val loss: 2.072720\n",
      "iteration 150 / 2000: training loss 1.750487 val loss: 1.671657\n",
      "iteration 200 / 2000: training loss 1.496974 val loss: 1.523241\n",
      "iteration 250 / 2000: training loss 1.511688 val loss: 1.549032\n",
      "iteration 300 / 2000: training loss 1.405574 val loss: 1.639297\n",
      "iteration 350 / 2000: training loss 1.705167 val loss: 1.430318\n",
      "iteration 400 / 2000: training loss 1.459337 val loss: 1.395507\n",
      "iteration 450 / 2000: training loss 1.396141 val loss: 1.396941\n",
      "iteration 500 / 2000: training loss 1.295087 val loss: 1.344437\n",
      "iteration 550 / 2000: training loss 1.275412 val loss: 1.364847\n",
      "iteration 600 / 2000: training loss 1.318723 val loss: 1.367980\n",
      "iteration 650 / 2000: training loss 1.219183 val loss: 1.294948\n",
      "iteration 700 / 2000: training loss 1.295852 val loss: 1.331919\n",
      "iteration 750 / 2000: training loss 1.314012 val loss: 1.299444\n",
      "iteration 800 / 2000: training loss 1.313674 val loss: 1.294659\n",
      "iteration 850 / 2000: training loss 1.302410 val loss: 1.300193\n",
      "iteration 900 / 2000: training loss 1.284182 val loss: 1.290414\n",
      "iteration 950 / 2000: training loss 1.259665 val loss: 1.289456\n",
      "iteration 1000 / 2000: training loss 1.263495 val loss: 1.285613\n",
      "iteration 1050 / 2000: training loss 1.273296 val loss: 1.284728\n",
      "hidden_size: 200 lr: 7.000000e-01 lr_decay: 8.000000e-01 reg: 6.000000e-02 train accuracy: 0.840000 val accuracy: 0.891000\n",
      "iteration 0 / 2000: training loss 2.302588 val loss: 2.303018\n",
      "iteration 50 / 2000: training loss 2.814961 val loss: 3.432657\n",
      "iteration 100 / 2000: training loss 1.556816 val loss: 1.809867\n",
      "iteration 150 / 2000: training loss 2.547377 val loss: 2.280097\n",
      "iteration 200 / 2000: training loss 1.749656 val loss: 1.611086\n",
      "iteration 250 / 2000: training loss 2.025584 val loss: 2.124593\n",
      "iteration 300 / 2000: training loss 2.023964 val loss: 2.298701\n",
      "iteration 350 / 2000: training loss 1.685800 val loss: 1.742355\n",
      "hidden_size: 200 lr: 9.000000e-01 lr_decay: 9.800000e-01 reg: 1.000000e-02 train accuracy: 0.680000 val accuracy: 0.590000\n",
      "iteration 0 / 2000: training loss 2.302588 val loss: 2.303018\n",
      "iteration 50 / 2000: training loss 2.461809 val loss: 2.087456\n",
      "iteration 100 / 2000: training loss 1.287141 val loss: 1.275947\n",
      "iteration 150 / 2000: training loss 1.793423 val loss: 1.482521\n",
      "iteration 200 / 2000: training loss 1.323035 val loss: 1.286331\n",
      "iteration 250 / 2000: training loss 1.114112 val loss: 1.094183\n",
      "iteration 300 / 2000: training loss 1.121616 val loss: 1.387103\n",
      "iteration 350 / 2000: training loss 1.552130 val loss: 1.275850\n",
      "iteration 400 / 2000: training loss 1.112313 val loss: 0.867799\n",
      "iteration 450 / 2000: training loss 1.183872 val loss: 0.938149\n",
      "iteration 500 / 2000: training loss 0.711932 val loss: 0.878885\n",
      "iteration 550 / 2000: training loss 0.775240 val loss: 0.747896\n",
      "iteration 600 / 2000: training loss 0.613256 val loss: 0.684421\n",
      "iteration 650 / 2000: training loss 0.619402 val loss: 0.692357\n",
      "iteration 700 / 2000: training loss 0.928577 val loss: 0.698771\n",
      "iteration 750 / 2000: training loss 0.614170 val loss: 0.629802\n",
      "iteration 800 / 2000: training loss 0.676567 val loss: 0.654786\n",
      "hidden_size: 200 lr: 9.000000e-01 lr_decay: 9.000000e-01 reg: 1.000000e-02 train accuracy: 0.960000 val accuracy: 0.897000\n",
      "iteration 0 / 2000: training loss 2.302588 val loss: 2.303018\n",
      "iteration 50 / 2000: training loss 2.917311 val loss: 2.392213\n",
      "iteration 100 / 2000: training loss 1.094974 val loss: 1.178864\n",
      "iteration 150 / 2000: training loss 1.478279 val loss: 1.085721\n",
      "iteration 200 / 2000: training loss 0.853930 val loss: 0.926118\n",
      "iteration 250 / 2000: training loss 0.811661 val loss: 0.883674\n",
      "iteration 300 / 2000: training loss 0.768655 val loss: 0.962473\n",
      "iteration 350 / 2000: training loss 0.673266 val loss: 0.763654\n",
      "iteration 400 / 2000: training loss 0.743861 val loss: 0.715129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 450 / 2000: training loss 0.705637 val loss: 0.722675\n",
      "iteration 500 / 2000: training loss 0.618489 val loss: 0.713792\n",
      "iteration 550 / 2000: training loss 0.612928 val loss: 0.655020\n",
      "iteration 600 / 2000: training loss 0.542032 val loss: 0.668356\n",
      "iteration 650 / 2000: training loss 0.500849 val loss: 0.581352\n",
      "iteration 700 / 2000: training loss 0.665588 val loss: 0.633288\n",
      "iteration 750 / 2000: training loss 0.601554 val loss: 0.545192\n",
      "iteration 800 / 2000: training loss 0.588853 val loss: 0.578123\n",
      "iteration 850 / 2000: training loss 0.589929 val loss: 0.575228\n",
      "iteration 900 / 2000: training loss 0.529845 val loss: 0.549989\n",
      "iteration 950 / 2000: training loss 0.496385 val loss: 0.555972\n",
      "iteration 1000 / 2000: training loss 0.490855 val loss: 0.534386\n",
      "iteration 1050 / 2000: training loss 0.507113 val loss: 0.574582\n",
      "iteration 1100 / 2000: training loss 0.552346 val loss: 0.558287\n",
      "iteration 1150 / 2000: training loss 0.460478 val loss: 0.542388\n",
      "iteration 1200 / 2000: training loss 0.530990 val loss: 0.532373\n",
      "iteration 1250 / 2000: training loss 0.574132 val loss: 0.537629\n",
      "iteration 1300 / 2000: training loss 0.492204 val loss: 0.519184\n",
      "iteration 1350 / 2000: training loss 0.578494 val loss: 0.526949\n",
      "iteration 1400 / 2000: training loss 0.448165 val loss: 0.513094\n",
      "iteration 1450 / 2000: training loss 0.545002 val loss: 0.516246\n",
      "iteration 1500 / 2000: training loss 0.506504 val loss: 0.514490\n",
      "iteration 1550 / 2000: training loss 0.557988 val loss: 0.510847\n",
      "iteration 1600 / 2000: training loss 0.521177 val loss: 0.503747\n",
      "iteration 1650 / 2000: training loss 0.510144 val loss: 0.501768\n",
      "iteration 1700 / 2000: training loss 0.451868 val loss: 0.502750\n",
      "iteration 1750 / 2000: training loss 0.417457 val loss: 0.500033\n",
      "iteration 1800 / 2000: training loss 0.505503 val loss: 0.503815\n",
      "iteration 1850 / 2000: training loss 0.482873 val loss: 0.500832\n",
      "iteration 1900 / 2000: training loss 0.405987 val loss: 0.503863\n",
      "iteration 1950 / 2000: training loss 0.478251 val loss: 0.505856\n",
      "hidden_size: 200 lr: 9.000000e-01 lr_decay: 8.500000e-01 reg: 1.000000e-02 train accuracy: 0.960000 val accuracy: 0.945000\n",
      "iteration 0 / 2000: training loss 2.302588 val loss: 2.303018\n",
      "iteration 50 / 2000: training loss 1.783538 val loss: 1.486609\n",
      "iteration 100 / 2000: training loss 1.211962 val loss: 1.151228\n",
      "iteration 150 / 2000: training loss 1.647347 val loss: 1.040862\n",
      "iteration 200 / 2000: training loss 0.910094 val loss: 0.891240\n",
      "iteration 250 / 2000: training loss 0.882684 val loss: 0.828276\n",
      "iteration 300 / 2000: training loss 0.733986 val loss: 0.853763\n",
      "iteration 350 / 2000: training loss 0.694291 val loss: 0.718095\n",
      "iteration 400 / 2000: training loss 0.607134 val loss: 0.592615\n",
      "iteration 450 / 2000: training loss 0.577362 val loss: 0.633249\n",
      "iteration 500 / 2000: training loss 0.535066 val loss: 0.595831\n",
      "iteration 550 / 2000: training loss 0.557142 val loss: 0.572695\n",
      "iteration 600 / 2000: training loss 0.443334 val loss: 0.558569\n",
      "iteration 650 / 2000: training loss 0.458762 val loss: 0.551023\n",
      "iteration 700 / 2000: training loss 0.533488 val loss: 0.521137\n",
      "iteration 750 / 2000: training loss 0.498709 val loss: 0.529700\n",
      "iteration 800 / 2000: training loss 0.541237 val loss: 0.549604\n",
      "iteration 850 / 2000: training loss 0.500319 val loss: 0.539451\n",
      "iteration 900 / 2000: training loss 0.481531 val loss: 0.529651\n",
      "iteration 950 / 2000: training loss 0.487123 val loss: 0.514273\n",
      "iteration 1000 / 2000: training loss 0.483528 val loss: 0.507755\n",
      "iteration 1050 / 2000: training loss 0.473778 val loss: 0.511615\n",
      "iteration 1100 / 2000: training loss 0.511110 val loss: 0.513778\n",
      "iteration 1150 / 2000: training loss 0.449301 val loss: 0.511903\n",
      "iteration 1200 / 2000: training loss 0.522061 val loss: 0.513949\n",
      "iteration 1250 / 2000: training loss 0.543346 val loss: 0.513917\n",
      "iteration 1300 / 2000: training loss 0.479008 val loss: 0.504686\n",
      "iteration 1350 / 2000: training loss 0.537057 val loss: 0.510011\n",
      "iteration 1400 / 2000: training loss 0.442445 val loss: 0.502735\n",
      "iteration 1450 / 2000: training loss 0.511747 val loss: 0.507922\n",
      "iteration 1500 / 2000: training loss 0.478439 val loss: 0.498912\n",
      "iteration 1550 / 2000: training loss 0.549182 val loss: 0.499882\n",
      "iteration 1600 / 2000: training loss 0.494821 val loss: 0.495884\n",
      "iteration 1650 / 2000: training loss 0.498097 val loss: 0.495085\n",
      "iteration 1700 / 2000: training loss 0.459281 val loss: 0.495234\n",
      "iteration 1750 / 2000: training loss 0.422608 val loss: 0.497079\n",
      "iteration 1800 / 2000: training loss 0.515185 val loss: 0.495525\n",
      "iteration 1850 / 2000: training loss 0.481968 val loss: 0.495815\n",
      "hidden_size: 200 lr: 9.000000e-01 lr_decay: 8.000000e-01 reg: 1.000000e-02 train accuracy: 0.930000 val accuracy: 0.944000\n",
      "iteration 0 / 2000: training loss 2.302590 val loss: 2.303020\n",
      "iteration 50 / 2000: training loss 2.302438 val loss: 2.036837\n",
      "iteration 100 / 2000: training loss 3.085197 val loss: 3.221273\n",
      "iteration 150 / 2000: training loss 1.962367 val loss: 2.115634\n",
      "iteration 200 / 2000: training loss 4.242038 val loss: 4.080817\n",
      "iteration 250 / 2000: training loss 2.181248 val loss: 2.038455\n",
      "hidden_size: 200 lr: 9.000000e-01 lr_decay: 9.800000e-01 reg: 2.000000e-02 train accuracy: 0.390000 val accuracy: 0.344000\n",
      "iteration 0 / 2000: training loss 2.302590 val loss: 2.303020\n",
      "iteration 50 / 2000: training loss 2.117977 val loss: 2.187391\n",
      "iteration 100 / 2000: training loss 2.197831 val loss: 1.953410\n",
      "iteration 150 / 2000: training loss 2.217280 val loss: 1.806442\n",
      "iteration 200 / 2000: training loss 1.405338 val loss: 1.282573\n",
      "iteration 250 / 2000: training loss 1.304821 val loss: 1.529516\n",
      "iteration 300 / 2000: training loss 1.231280 val loss: 1.318797\n",
      "iteration 350 / 2000: training loss 1.405253 val loss: 1.436428\n",
      "iteration 400 / 2000: training loss 1.021546 val loss: 1.168937\n",
      "iteration 450 / 2000: training loss 1.175705 val loss: 1.032767\n",
      "iteration 500 / 2000: training loss 1.018165 val loss: 1.176544\n",
      "iteration 550 / 2000: training loss 0.978008 val loss: 0.919898\n",
      "iteration 600 / 2000: training loss 0.829057 val loss: 0.937396\n",
      "iteration 650 / 2000: training loss 0.826103 val loss: 0.986347\n",
      "iteration 700 / 2000: training loss 1.152141 val loss: 1.020710\n",
      "iteration 750 / 2000: training loss 0.916237 val loss: 1.018014\n",
      "iteration 800 / 2000: training loss 0.815465 val loss: 0.950109\n",
      "iteration 850 / 2000: training loss 0.959364 val loss: 0.906374\n",
      "iteration 900 / 2000: training loss 0.825879 val loss: 0.875566\n",
      "iteration 950 / 2000: training loss 0.738251 val loss: 0.824493\n",
      "iteration 1000 / 2000: training loss 0.802473 val loss: 0.990373\n",
      "iteration 1050 / 2000: training loss 0.842616 val loss: 0.873129\n",
      "iteration 1100 / 2000: training loss 0.775901 val loss: 0.805176\n",
      "iteration 1150 / 2000: training loss 0.755955 val loss: 0.818152\n",
      "iteration 1200 / 2000: training loss 0.833555 val loss: 0.789016\n",
      "iteration 1250 / 2000: training loss 0.825927 val loss: 0.784589\n",
      "iteration 1300 / 2000: training loss 0.851682 val loss: 0.808196\n",
      "iteration 1350 / 2000: training loss 0.787841 val loss: 0.797380\n",
      "iteration 1400 / 2000: training loss 0.791078 val loss: 0.783549\n",
      "iteration 1450 / 2000: training loss 0.812732 val loss: 0.788253\n",
      "iteration 1500 / 2000: training loss 0.763077 val loss: 0.745519\n",
      "iteration 1550 / 2000: training loss 0.816033 val loss: 0.749138\n",
      "iteration 1600 / 2000: training loss 0.831145 val loss: 0.820055\n",
      "iteration 1650 / 2000: training loss 0.734371 val loss: 0.747657\n",
      "iteration 1700 / 2000: training loss 0.699937 val loss: 0.770857\n",
      "iteration 1750 / 2000: training loss 0.654407 val loss: 0.778725\n",
      "iteration 1800 / 2000: training loss 0.764399 val loss: 0.758869\n",
      "iteration 1850 / 2000: training loss 0.729294 val loss: 0.752153\n",
      "iteration 1900 / 2000: training loss 0.649495 val loss: 0.734586\n",
      "iteration 1950 / 2000: training loss 0.718600 val loss: 0.740367\n",
      "hidden_size: 200 lr: 9.000000e-01 lr_decay: 9.000000e-01 reg: 2.000000e-02 train accuracy: 0.930000 val accuracy: 0.923000\n",
      "iteration 0 / 2000: training loss 2.302590 val loss: 2.303020\n",
      "iteration 50 / 2000: training loss 2.350756 val loss: 2.158597\n",
      "iteration 100 / 2000: training loss 1.680218 val loss: 1.559246\n",
      "iteration 150 / 2000: training loss 1.303481 val loss: 1.206354\n",
      "iteration 200 / 2000: training loss 1.133347 val loss: 1.113729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 250 / 2000: training loss 1.157027 val loss: 1.245615\n",
      "iteration 300 / 2000: training loss 0.932128 val loss: 1.122113\n",
      "iteration 350 / 2000: training loss 0.907529 val loss: 0.981384\n",
      "iteration 400 / 2000: training loss 0.936908 val loss: 0.863954\n",
      "iteration 450 / 2000: training loss 0.954122 val loss: 0.873131\n",
      "iteration 500 / 2000: training loss 0.809149 val loss: 0.824806\n",
      "iteration 550 / 2000: training loss 0.842648 val loss: 0.834245\n",
      "iteration 600 / 2000: training loss 0.764460 val loss: 0.813259\n",
      "iteration 650 / 2000: training loss 0.706879 val loss: 0.799256\n",
      "iteration 700 / 2000: training loss 0.886545 val loss: 0.821023\n",
      "iteration 750 / 2000: training loss 0.776167 val loss: 0.781278\n",
      "iteration 800 / 2000: training loss 0.773277 val loss: 0.812504\n",
      "iteration 850 / 2000: training loss 0.841270 val loss: 0.770038\n",
      "iteration 900 / 2000: training loss 0.785852 val loss: 0.772144\n",
      "iteration 950 / 2000: training loss 0.753429 val loss: 0.789713\n",
      "iteration 1000 / 2000: training loss 0.723600 val loss: 0.761648\n",
      "iteration 1050 / 2000: training loss 0.716653 val loss: 0.757443\n",
      "iteration 1100 / 2000: training loss 0.750667 val loss: 0.753453\n",
      "iteration 1150 / 2000: training loss 0.689581 val loss: 0.754408\n",
      "iteration 1200 / 2000: training loss 0.746316 val loss: 0.751589\n",
      "iteration 1250 / 2000: training loss 0.804055 val loss: 0.750815\n",
      "iteration 1300 / 2000: training loss 0.716535 val loss: 0.744808\n",
      "iteration 1350 / 2000: training loss 0.791370 val loss: 0.734073\n",
      "iteration 1400 / 2000: training loss 0.678052 val loss: 0.734578\n",
      "iteration 1450 / 2000: training loss 0.771771 val loss: 0.730225\n",
      "iteration 1500 / 2000: training loss 0.723965 val loss: 0.725711\n",
      "iteration 1550 / 2000: training loss 0.742488 val loss: 0.721296\n",
      "iteration 1600 / 2000: training loss 0.743549 val loss: 0.721873\n",
      "iteration 1650 / 2000: training loss 0.715526 val loss: 0.721076\n",
      "iteration 1700 / 2000: training loss 0.672777 val loss: 0.717981\n",
      "iteration 1750 / 2000: training loss 0.635043 val loss: 0.713563\n",
      "iteration 1800 / 2000: training loss 0.737481 val loss: 0.722543\n",
      "iteration 1850 / 2000: training loss 0.713482 val loss: 0.713834\n",
      "iteration 1900 / 2000: training loss 0.622672 val loss: 0.717150\n",
      "iteration 1950 / 2000: training loss 0.705958 val loss: 0.714719\n",
      "hidden_size: 200 lr: 9.000000e-01 lr_decay: 8.500000e-01 reg: 2.000000e-02 train accuracy: 0.920000 val accuracy: 0.936000\n",
      "iteration 0 / 2000: training loss 2.302590 val loss: 2.303020\n",
      "iteration 50 / 2000: training loss 2.550747 val loss: 2.549184\n",
      "iteration 100 / 2000: training loss 1.575637 val loss: 1.859504\n",
      "iteration 150 / 2000: training loss 1.196762 val loss: 1.217081\n",
      "iteration 200 / 2000: training loss 1.024184 val loss: 1.076059\n",
      "iteration 250 / 2000: training loss 1.039935 val loss: 1.027810\n",
      "iteration 300 / 2000: training loss 0.913790 val loss: 1.063847\n",
      "iteration 350 / 2000: training loss 0.917770 val loss: 0.893379\n",
      "iteration 400 / 2000: training loss 0.850681 val loss: 0.826366\n",
      "iteration 450 / 2000: training loss 0.892933 val loss: 0.861101\n",
      "iteration 500 / 2000: training loss 0.823113 val loss: 0.853559\n",
      "iteration 550 / 2000: training loss 0.752863 val loss: 0.815988\n",
      "iteration 600 / 2000: training loss 0.722893 val loss: 0.833495\n",
      "iteration 650 / 2000: training loss 0.674242 val loss: 0.752946\n",
      "iteration 700 / 2000: training loss 0.769213 val loss: 0.768199\n",
      "iteration 750 / 2000: training loss 0.758802 val loss: 0.740113\n",
      "iteration 800 / 2000: training loss 0.791047 val loss: 0.748102\n",
      "iteration 850 / 2000: training loss 0.738996 val loss: 0.772942\n",
      "iteration 900 / 2000: training loss 0.726583 val loss: 0.748007\n",
      "iteration 950 / 2000: training loss 0.708879 val loss: 0.736645\n",
      "iteration 1000 / 2000: training loss 0.702287 val loss: 0.731041\n",
      "iteration 1050 / 2000: training loss 0.698338 val loss: 0.736076\n",
      "hidden_size: 200 lr: 9.000000e-01 lr_decay: 8.000000e-01 reg: 2.000000e-02 train accuracy: 0.910000 val accuracy: 0.922000\n",
      "iteration 0 / 2000: training loss 2.302596 val loss: 2.303025\n",
      "iteration 50 / 2000: training loss 5.903685 val loss: 5.968757\n",
      "iteration 100 / 2000: training loss 7.798629 val loss: 6.517552\n",
      "iteration 150 / 2000: training loss 6.849454 val loss: 9.975529\n",
      "iteration 200 / 2000: training loss 9.789399 val loss: 12.038767\n",
      "iteration 250 / 2000: training loss 3.488576 val loss: 3.199141\n",
      "iteration 300 / 2000: training loss 2.709572 val loss: 2.757450\n",
      "iteration 350 / 2000: training loss 3.325769 val loss: 3.358596\n",
      "iteration 400 / 2000: training loss 2.154269 val loss: 2.318513\n",
      "iteration 450 / 2000: training loss 2.713399 val loss: 2.685145\n",
      "hidden_size: 200 lr: 9.000000e-01 lr_decay: 9.800000e-01 reg: 4.000000e-02 train accuracy: 0.150000 val accuracy: 0.164000\n",
      "iteration 0 / 2000: training loss 2.302596 val loss: 2.303025\n",
      "iteration 50 / 2000: training loss 4.615858 val loss: 4.245297\n",
      "iteration 100 / 2000: training loss 6.865924 val loss: 8.374116\n",
      "iteration 150 / 2000: training loss 4.944871 val loss: 4.394380\n",
      "iteration 200 / 2000: training loss 2.175242 val loss: 2.496999\n",
      "iteration 250 / 2000: training loss 1.995079 val loss: 2.039090\n",
      "iteration 300 / 2000: training loss 1.751951 val loss: 1.836882\n",
      "iteration 350 / 2000: training loss 1.526939 val loss: 1.584497\n",
      "iteration 400 / 2000: training loss 1.629185 val loss: 1.679782\n",
      "iteration 450 / 2000: training loss 1.440649 val loss: 1.497202\n",
      "iteration 500 / 2000: training loss 1.379357 val loss: 1.387966\n",
      "iteration 550 / 2000: training loss 1.466973 val loss: 1.504509\n",
      "iteration 600 / 2000: training loss 1.454952 val loss: 1.396837\n",
      "iteration 650 / 2000: training loss 1.248704 val loss: 1.277961\n",
      "iteration 700 / 2000: training loss 1.284156 val loss: 1.356014\n",
      "iteration 750 / 2000: training loss 1.228994 val loss: 1.186258\n",
      "iteration 800 / 2000: training loss 1.290742 val loss: 1.309773\n",
      "iteration 850 / 2000: training loss 1.272169 val loss: 1.168043\n",
      "iteration 900 / 2000: training loss 1.258960 val loss: 1.288344\n",
      "iteration 950 / 2000: training loss 1.091612 val loss: 1.234325\n",
      "iteration 1000 / 2000: training loss 1.206958 val loss: 1.233704\n",
      "iteration 1050 / 2000: training loss 1.141520 val loss: 1.151409\n",
      "iteration 1100 / 2000: training loss 1.081218 val loss: 1.098418\n",
      "iteration 1150 / 2000: training loss 1.134087 val loss: 1.177666\n",
      "iteration 1200 / 2000: training loss 1.165341 val loss: 1.109877\n",
      "iteration 1250 / 2000: training loss 1.143778 val loss: 1.097414\n",
      "iteration 1300 / 2000: training loss 1.223560 val loss: 1.120468\n",
      "iteration 1350 / 2000: training loss 1.108654 val loss: 1.145912\n",
      "iteration 1400 / 2000: training loss 1.113766 val loss: 1.109797\n",
      "iteration 1450 / 2000: training loss 1.178169 val loss: 1.137907\n",
      "iteration 1500 / 2000: training loss 1.062624 val loss: 1.068010\n",
      "iteration 1550 / 2000: training loss 1.106884 val loss: 1.066572\n",
      "iteration 1600 / 2000: training loss 1.159494 val loss: 1.102912\n",
      "iteration 1650 / 2000: training loss 1.060178 val loss: 1.076633\n",
      "iteration 1700 / 2000: training loss 0.990735 val loss: 1.068974\n",
      "hidden_size: 200 lr: 9.000000e-01 lr_decay: 9.000000e-01 reg: 4.000000e-02 train accuracy: 0.840000 val accuracy: 0.876000\n",
      "iteration 0 / 2000: training loss 2.302596 val loss: 2.303025\n",
      "iteration 50 / 2000: training loss 4.520818 val loss: 4.087497\n",
      "iteration 100 / 2000: training loss 2.587862 val loss: 2.278721\n",
      "iteration 150 / 2000: training loss 1.671049 val loss: 1.883740\n",
      "iteration 200 / 2000: training loss 1.724778 val loss: 1.716671\n",
      "iteration 250 / 2000: training loss 1.727074 val loss: 2.027419\n",
      "iteration 300 / 2000: training loss 1.498291 val loss: 1.566100\n",
      "iteration 350 / 2000: training loss 1.484288 val loss: 1.364462\n",
      "iteration 400 / 2000: training loss 1.545834 val loss: 1.321142\n",
      "iteration 450 / 2000: training loss 1.471935 val loss: 1.283921\n",
      "iteration 500 / 2000: training loss 1.142968 val loss: 1.312553\n",
      "iteration 550 / 2000: training loss 1.165818 val loss: 1.198411\n",
      "iteration 600 / 2000: training loss 1.074050 val loss: 1.206342\n",
      "iteration 650 / 2000: training loss 1.098807 val loss: 1.155782\n",
      "iteration 700 / 2000: training loss 1.296916 val loss: 1.196883\n",
      "iteration 750 / 2000: training loss 1.116669 val loss: 1.113319\n",
      "iteration 800 / 2000: training loss 1.077802 val loss: 1.131648\n",
      "iteration 850 / 2000: training loss 1.157868 val loss: 1.104801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 900 / 2000: training loss 1.163113 val loss: 1.097629\n",
      "iteration 950 / 2000: training loss 1.002669 val loss: 1.095600\n",
      "iteration 1000 / 2000: training loss 1.098456 val loss: 1.088028\n",
      "iteration 1050 / 2000: training loss 1.042524 val loss: 1.069570\n",
      "iteration 1100 / 2000: training loss 1.048695 val loss: 1.083546\n",
      "iteration 1150 / 2000: training loss 1.006347 val loss: 1.084766\n",
      "iteration 1200 / 2000: training loss 1.076066 val loss: 1.057665\n",
      "iteration 1250 / 2000: training loss 1.108937 val loss: 1.073904\n",
      "iteration 1300 / 2000: training loss 1.073151 val loss: 1.066828\n",
      "iteration 1350 / 2000: training loss 1.104849 val loss: 1.056797\n",
      "iteration 1400 / 2000: training loss 1.034039 val loss: 1.043730\n",
      "iteration 1450 / 2000: training loss 1.108538 val loss: 1.049617\n",
      "iteration 1500 / 2000: training loss 1.043067 val loss: 1.041259\n",
      "iteration 1550 / 2000: training loss 1.058826 val loss: 1.047686\n",
      "iteration 1600 / 2000: training loss 1.060460 val loss: 1.049289\n",
      "iteration 1650 / 2000: training loss 1.025566 val loss: 1.044915\n",
      "iteration 1700 / 2000: training loss 0.992169 val loss: 1.031502\n",
      "iteration 1750 / 2000: training loss 0.941523 val loss: 1.025216\n",
      "iteration 1800 / 2000: training loss 1.080500 val loss: 1.042344\n",
      "iteration 1850 / 2000: training loss 1.051478 val loss: 1.034165\n",
      "iteration 1900 / 2000: training loss 0.947315 val loss: 1.029232\n",
      "iteration 1950 / 2000: training loss 1.028868 val loss: 1.025382\n",
      "hidden_size: 200 lr: 9.000000e-01 lr_decay: 8.500000e-01 reg: 4.000000e-02 train accuracy: 0.890000 val accuracy: 0.922000\n",
      "iteration 0 / 2000: training loss 2.302596 val loss: 2.303025\n",
      "iteration 50 / 2000: training loss 2.664104 val loss: 3.173128\n",
      "iteration 100 / 2000: training loss 2.023315 val loss: 2.000217\n",
      "iteration 150 / 2000: training loss 1.655767 val loss: 1.603535\n",
      "iteration 200 / 2000: training loss 1.291231 val loss: 1.435850\n",
      "iteration 250 / 2000: training loss 1.389894 val loss: 1.543754\n",
      "iteration 300 / 2000: training loss 1.324819 val loss: 1.549851\n",
      "iteration 350 / 2000: training loss 1.277351 val loss: 1.200285\n",
      "iteration 400 / 2000: training loss 1.208465 val loss: 1.164472\n",
      "iteration 450 / 2000: training loss 1.210042 val loss: 1.219138\n",
      "hidden_size: 200 lr: 9.000000e-01 lr_decay: 8.000000e-01 reg: 4.000000e-02 train accuracy: 0.840000 val accuracy: 0.825000\n",
      "iteration 0 / 2000: training loss 2.302601 val loss: 2.303029\n",
      "iteration 50 / 2000: training loss 5.461641 val loss: 5.394662\n",
      "iteration 100 / 2000: training loss 126.627579 val loss: 111.663849\n",
      "iteration 150 / 2000: training loss nan val loss: nan\n",
      "iteration 200 / 2000: training loss nan val loss: nan\n",
      "iteration 250 / 2000: training loss nan val loss: nan\n",
      "iteration 300 / 2000: training loss nan val loss: nan\n",
      "iteration 350 / 2000: training loss nan val loss: nan\n",
      "iteration 400 / 2000: training loss nan val loss: nan\n",
      "iteration 450 / 2000: training loss nan val loss: nan\n",
      "iteration 500 / 2000: training loss nan val loss: nan\n",
      "iteration 550 / 2000: training loss nan val loss: nan\n",
      "iteration 600 / 2000: training loss nan val loss: nan\n",
      "iteration 650 / 2000: training loss nan val loss: nan\n",
      "iteration 700 / 2000: training loss nan val loss: nan\n",
      "iteration 750 / 2000: training loss nan val loss: nan\n",
      "iteration 800 / 2000: training loss nan val loss: nan\n",
      "iteration 850 / 2000: training loss nan val loss: nan\n",
      "iteration 900 / 2000: training loss nan val loss: nan\n",
      "iteration 950 / 2000: training loss nan val loss: nan\n",
      "iteration 1000 / 2000: training loss nan val loss: nan\n",
      "iteration 1050 / 2000: training loss nan val loss: nan\n",
      "iteration 1100 / 2000: training loss nan val loss: nan\n",
      "iteration 1150 / 2000: training loss nan val loss: nan\n",
      "iteration 1200 / 2000: training loss nan val loss: nan\n",
      "iteration 1250 / 2000: training loss nan val loss: nan\n",
      "iteration 1300 / 2000: training loss nan val loss: nan\n",
      "iteration 1350 / 2000: training loss nan val loss: nan\n",
      "iteration 1400 / 2000: training loss nan val loss: nan\n",
      "iteration 1450 / 2000: training loss nan val loss: nan\n",
      "iteration 1500 / 2000: training loss nan val loss: nan\n",
      "iteration 1550 / 2000: training loss nan val loss: nan\n",
      "iteration 1600 / 2000: training loss nan val loss: nan\n",
      "iteration 1650 / 2000: training loss nan val loss: nan\n",
      "iteration 1700 / 2000: training loss nan val loss: nan\n",
      "iteration 1750 / 2000: training loss nan val loss: nan\n",
      "iteration 1800 / 2000: training loss nan val loss: nan\n",
      "iteration 1850 / 2000: training loss nan val loss: nan\n",
      "iteration 1900 / 2000: training loss nan val loss: nan\n",
      "iteration 1950 / 2000: training loss nan val loss: nan\n",
      "hidden_size: 200 lr: 9.000000e-01 lr_decay: 9.800000e-01 reg: 6.000000e-02 train accuracy: 0.080000 val accuracy: 0.100000\n",
      "iteration 0 / 2000: training loss 2.302601 val loss: 2.303029\n",
      "iteration 50 / 2000: training loss 3.045788 val loss: 3.134520\n",
      "iteration 100 / 2000: training loss 3.943115 val loss: 4.323918\n",
      "iteration 150 / 2000: training loss 4.713182 val loss: 7.246430\n",
      "iteration 200 / 2000: training loss 6.983171 val loss: 5.810544\n",
      "iteration 250 / 2000: training loss 2.743322 val loss: 2.736732\n",
      "iteration 300 / 2000: training loss 2.427444 val loss: 2.107958\n",
      "iteration 350 / 2000: training loss 2.113593 val loss: 2.039614\n",
      "iteration 400 / 2000: training loss 2.129568 val loss: 2.420470\n",
      "iteration 450 / 2000: training loss 2.297837 val loss: 2.071182\n",
      "iteration 500 / 2000: training loss 2.200406 val loss: 2.210979\n",
      "iteration 550 / 2000: training loss 2.305088 val loss: 2.258758\n",
      "iteration 600 / 2000: training loss 1.973909 val loss: 1.800834\n",
      "iteration 650 / 2000: training loss 1.784079 val loss: 1.791357\n",
      "iteration 700 / 2000: training loss 1.855789 val loss: 1.844501\n",
      "iteration 750 / 2000: training loss 1.649755 val loss: 1.575946\n",
      "iteration 800 / 2000: training loss 1.721797 val loss: 1.637266\n",
      "iteration 850 / 2000: training loss 1.624534 val loss: 1.567789\n",
      "iteration 900 / 2000: training loss 1.760963 val loss: 1.866684\n",
      "iteration 950 / 2000: training loss 1.474371 val loss: 1.652177\n",
      "iteration 1000 / 2000: training loss 1.577664 val loss: 1.546069\n",
      "iteration 1050 / 2000: training loss 1.432646 val loss: 1.606101\n",
      "iteration 1100 / 2000: training loss 1.347134 val loss: 1.481000\n",
      "iteration 1150 / 2000: training loss 1.567565 val loss: 1.525624\n",
      "iteration 1200 / 2000: training loss 1.618647 val loss: 1.455680\n",
      "iteration 1250 / 2000: training loss 1.481981 val loss: 1.447433\n",
      "iteration 1300 / 2000: training loss 1.525259 val loss: 1.459294\n",
      "iteration 1350 / 2000: training loss 1.581569 val loss: 1.513192\n",
      "iteration 1400 / 2000: training loss 1.433919 val loss: 1.486918\n",
      "iteration 1450 / 2000: training loss 1.547413 val loss: 1.547330\n",
      "iteration 1500 / 2000: training loss 1.419122 val loss: 1.505449\n",
      "iteration 1550 / 2000: training loss 1.518741 val loss: 1.502794\n",
      "iteration 1600 / 2000: training loss 1.468598 val loss: 1.473013\n",
      "iteration 1650 / 2000: training loss 1.376073 val loss: 1.433290\n",
      "iteration 1700 / 2000: training loss 1.368411 val loss: 1.396474\n",
      "iteration 1750 / 2000: training loss 1.328669 val loss: 1.480622\n",
      "iteration 1800 / 2000: training loss 1.395834 val loss: 1.420398\n",
      "iteration 1850 / 2000: training loss 1.471336 val loss: 1.413353\n",
      "iteration 1900 / 2000: training loss 1.280837 val loss: 1.429623\n",
      "iteration 1950 / 2000: training loss 1.404239 val loss: 1.405212\n",
      "hidden_size: 200 lr: 9.000000e-01 lr_decay: 9.000000e-01 reg: 6.000000e-02 train accuracy: 0.800000 val accuracy: 0.787000\n",
      "iteration 0 / 2000: training loss 2.302601 val loss: 2.303029\n",
      "iteration 50 / 2000: training loss 4.686759 val loss: 4.472066\n",
      "iteration 100 / 2000: training loss 4.476927 val loss: 4.784313\n",
      "iteration 150 / 2000: training loss 2.158723 val loss: 2.766781\n",
      "iteration 200 / 2000: training loss 2.223984 val loss: 2.312642\n",
      "iteration 250 / 2000: training loss 2.453202 val loss: 2.556483\n",
      "iteration 300 / 2000: training loss 1.772281 val loss: 1.831199\n",
      "iteration 350 / 2000: training loss 1.576308 val loss: 1.574753\n",
      "iteration 400 / 2000: training loss 1.755222 val loss: 1.633234\n",
      "iteration 450 / 2000: training loss 1.513938 val loss: 1.529013\n",
      "iteration 500 / 2000: training loss 1.361202 val loss: 1.565625\n",
      "iteration 550 / 2000: training loss 1.460982 val loss: 1.509657\n",
      "iteration 600 / 2000: training loss 1.409579 val loss: 1.543942\n",
      "iteration 650 / 2000: training loss 1.321607 val loss: 1.405095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 700 / 2000: training loss 1.476922 val loss: 1.507023\n",
      "iteration 750 / 2000: training loss 1.430656 val loss: 1.368541\n",
      "iteration 800 / 2000: training loss 1.326253 val loss: 1.336991\n",
      "iteration 850 / 2000: training loss 1.405016 val loss: 1.342852\n",
      "iteration 900 / 2000: training loss 1.353982 val loss: 1.384349\n",
      "iteration 950 / 2000: training loss 1.274726 val loss: 1.372149\n",
      "iteration 1000 / 2000: training loss 1.305473 val loss: 1.334969\n",
      "iteration 1050 / 2000: training loss 1.289266 val loss: 1.315881\n",
      "iteration 1100 / 2000: training loss 1.308323 val loss: 1.310586\n",
      "iteration 1150 / 2000: training loss 1.265799 val loss: 1.352572\n",
      "iteration 1200 / 2000: training loss 1.324502 val loss: 1.299952\n",
      "iteration 1250 / 2000: training loss 1.339045 val loss: 1.314790\n",
      "iteration 1300 / 2000: training loss 1.338284 val loss: 1.305104\n",
      "iteration 1350 / 2000: training loss 1.340367 val loss: 1.295707\n",
      "iteration 1400 / 2000: training loss 1.294367 val loss: 1.277750\n",
      "iteration 1450 / 2000: training loss 1.335309 val loss: 1.289975\n",
      "iteration 1500 / 2000: training loss 1.286916 val loss: 1.283005\n",
      "iteration 1550 / 2000: training loss 1.303932 val loss: 1.299299\n",
      "iteration 1600 / 2000: training loss 1.317692 val loss: 1.291258\n",
      "iteration 1650 / 2000: training loss 1.276663 val loss: 1.287868\n",
      "iteration 1700 / 2000: training loss 1.239213 val loss: 1.273624\n",
      "iteration 1750 / 2000: training loss 1.179554 val loss: 1.261986\n",
      "iteration 1800 / 2000: training loss 1.322487 val loss: 1.287811\n",
      "iteration 1850 / 2000: training loss 1.294400 val loss: 1.271143\n",
      "iteration 1900 / 2000: training loss 1.192256 val loss: 1.268357\n",
      "iteration 1950 / 2000: training loss 1.264213 val loss: 1.262756\n",
      "hidden_size: 200 lr: 9.000000e-01 lr_decay: 8.500000e-01 reg: 6.000000e-02 train accuracy: 0.880000 val accuracy: 0.901000\n",
      "iteration 0 / 2000: training loss 2.302601 val loss: 2.303029\n",
      "iteration 50 / 2000: training loss 3.691217 val loss: 2.956934\n",
      "iteration 100 / 2000: training loss 3.966507 val loss: 3.468910\n",
      "iteration 150 / 2000: training loss 2.149961 val loss: 1.920480\n",
      "iteration 200 / 2000: training loss 1.945725 val loss: 1.762399\n",
      "iteration 250 / 2000: training loss 1.539732 val loss: 1.752645\n",
      "iteration 300 / 2000: training loss 1.444475 val loss: 1.585044\n",
      "iteration 350 / 2000: training loss 1.592839 val loss: 1.456951\n",
      "iteration 400 / 2000: training loss 1.490270 val loss: 1.457474\n",
      "iteration 450 / 2000: training loss 1.505680 val loss: 1.442320\n",
      "iteration 500 / 2000: training loss 1.353335 val loss: 1.384317\n",
      "iteration 550 / 2000: training loss 1.312053 val loss: 1.377697\n",
      "iteration 600 / 2000: training loss 1.340796 val loss: 1.369308\n",
      "iteration 650 / 2000: training loss 1.227667 val loss: 1.304090\n",
      "iteration 700 / 2000: training loss 1.341077 val loss: 1.391009\n",
      "iteration 750 / 2000: training loss 1.372026 val loss: 1.322107\n",
      "iteration 800 / 2000: training loss 1.311566 val loss: 1.308564\n",
      "iteration 850 / 2000: training loss 1.333432 val loss: 1.316902\n",
      "iteration 900 / 2000: training loss 1.311305 val loss: 1.310518\n",
      "iteration 950 / 2000: training loss 1.258244 val loss: 1.295996\n",
      "iteration 1000 / 2000: training loss 1.266452 val loss: 1.304806\n",
      "iteration 1050 / 2000: training loss 1.262887 val loss: 1.280696\n",
      "iteration 1100 / 2000: training loss 1.262801 val loss: 1.278646\n",
      "iteration 1150 / 2000: training loss 1.220512 val loss: 1.297905\n",
      "iteration 1200 / 2000: training loss 1.312302 val loss: 1.281527\n",
      "iteration 1250 / 2000: training loss 1.320127 val loss: 1.286508\n",
      "iteration 1300 / 2000: training loss 1.255892 val loss: 1.277351\n",
      "iteration 1350 / 2000: training loss 1.365481 val loss: 1.285724\n",
      "iteration 1400 / 2000: training loss 1.287026 val loss: 1.270254\n",
      "iteration 1450 / 2000: training loss 1.335960 val loss: 1.270450\n",
      "iteration 1500 / 2000: training loss 1.251681 val loss: 1.264041\n",
      "iteration 1550 / 2000: training loss 1.252250 val loss: 1.269633\n",
      "iteration 1600 / 2000: training loss 1.295433 val loss: 1.261696\n",
      "iteration 1650 / 2000: training loss 1.275157 val loss: 1.258386\n",
      "iteration 1700 / 2000: training loss 1.222735 val loss: 1.263182\n",
      "iteration 1750 / 2000: training loss 1.174601 val loss: 1.261393\n",
      "iteration 1800 / 2000: training loss 1.301233 val loss: 1.260970\n",
      "iteration 1850 / 2000: training loss 1.275924 val loss: 1.256953\n",
      "hidden_size: 200 lr: 9.000000e-01 lr_decay: 8.000000e-01 reg: 6.000000e-02 train accuracy: 0.910000 val accuracy: 0.901000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302594\n",
      "iteration 50 / 2000: training loss 1.882764 val loss: 1.823913\n",
      "iteration 100 / 2000: training loss 0.697696 val loss: 0.781268\n",
      "iteration 150 / 2000: training loss 0.468953 val loss: 0.575921\n",
      "iteration 200 / 2000: training loss 0.499775 val loss: 0.569943\n",
      "iteration 250 / 2000: training loss 0.473727 val loss: 0.554494\n",
      "iteration 300 / 2000: training loss 0.498018 val loss: 0.566195\n",
      "iteration 350 / 2000: training loss 0.528220 val loss: 0.560548\n",
      "iteration 400 / 2000: training loss 0.501302 val loss: 0.540344\n",
      "iteration 450 / 2000: training loss 0.534454 val loss: 0.558919\n",
      "iteration 500 / 2000: training loss 0.526341 val loss: 0.528030\n",
      "iteration 550 / 2000: training loss 0.513630 val loss: 0.527597\n",
      "iteration 600 / 2000: training loss 0.446103 val loss: 0.534967\n",
      "iteration 650 / 2000: training loss 0.424859 val loss: 0.524659\n",
      "iteration 700 / 2000: training loss 0.547418 val loss: 0.529613\n",
      "iteration 750 / 2000: training loss 0.510668 val loss: 0.537523\n",
      "iteration 800 / 2000: training loss 0.510795 val loss: 0.540729\n",
      "iteration 850 / 2000: training loss 0.509654 val loss: 0.545979\n",
      "iteration 900 / 2000: training loss 0.483539 val loss: 0.531700\n",
      "iteration 950 / 2000: training loss 0.493950 val loss: 0.520614\n",
      "iteration 1000 / 2000: training loss 0.487730 val loss: 0.528889\n",
      "iteration 1050 / 2000: training loss 0.492120 val loss: 0.538566\n",
      "iteration 1100 / 2000: training loss 0.546369 val loss: 0.541940\n",
      "iteration 1150 / 2000: training loss 0.456247 val loss: 0.533582\n",
      "iteration 1200 / 2000: training loss 0.520629 val loss: 0.519511\n",
      "iteration 1250 / 2000: training loss 0.578742 val loss: 0.534569\n",
      "hidden_size: 256 lr: 1.000000e-01 lr_decay: 9.800000e-01 reg: 1.000000e-02 train accuracy: 0.970000 val accuracy: 0.928000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302594\n",
      "iteration 50 / 2000: training loss 1.966927 val loss: 1.931627\n",
      "iteration 100 / 2000: training loss 0.674355 val loss: 0.755213\n",
      "iteration 150 / 2000: training loss 0.461250 val loss: 0.569752\n",
      "iteration 200 / 2000: training loss 0.499419 val loss: 0.565785\n",
      "iteration 250 / 2000: training loss 0.465897 val loss: 0.544596\n",
      "iteration 300 / 2000: training loss 0.486565 val loss: 0.566120\n",
      "iteration 350 / 2000: training loss 0.528950 val loss: 0.537824\n",
      "iteration 400 / 2000: training loss 0.512919 val loss: 0.527336\n",
      "iteration 450 / 2000: training loss 0.522016 val loss: 0.535918\n",
      "iteration 500 / 2000: training loss 0.491433 val loss: 0.524645\n",
      "iteration 550 / 2000: training loss 0.524644 val loss: 0.520268\n",
      "iteration 600 / 2000: training loss 0.436188 val loss: 0.525742\n",
      "hidden_size: 256 lr: 1.000000e-01 lr_decay: 9.000000e-01 reg: 1.000000e-02 train accuracy: 0.970000 val accuracy: 0.928000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302594\n",
      "iteration 50 / 2000: training loss 2.031117 val loss: 2.008443\n",
      "iteration 100 / 2000: training loss 0.658496 val loss: 0.730748\n",
      "iteration 150 / 2000: training loss 0.458090 val loss: 0.573117\n",
      "iteration 200 / 2000: training loss 0.501691 val loss: 0.556689\n",
      "iteration 250 / 2000: training loss 0.469606 val loss: 0.547344\n",
      "iteration 300 / 2000: training loss 0.473026 val loss: 0.560767\n",
      "iteration 350 / 2000: training loss 0.538815 val loss: 0.534796\n",
      "iteration 400 / 2000: training loss 0.529881 val loss: 0.532592\n",
      "iteration 450 / 2000: training loss 0.528887 val loss: 0.531479\n",
      "iteration 500 / 2000: training loss 0.488374 val loss: 0.525785\n",
      "iteration 550 / 2000: training loss 0.520462 val loss: 0.529272\n",
      "iteration 600 / 2000: training loss 0.429033 val loss: 0.519495\n",
      "hidden_size: 256 lr: 1.000000e-01 lr_decay: 8.500000e-01 reg: 1.000000e-02 train accuracy: 0.940000 val accuracy: 0.929000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302594\n",
      "iteration 50 / 2000: training loss 2.095227 val loss: 2.080045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 2000: training loss 0.656689 val loss: 0.715530\n",
      "iteration 150 / 2000: training loss 0.456694 val loss: 0.578865\n",
      "iteration 200 / 2000: training loss 0.504126 val loss: 0.559792\n",
      "iteration 250 / 2000: training loss 0.468850 val loss: 0.553188\n",
      "iteration 300 / 2000: training loss 0.461576 val loss: 0.553609\n",
      "iteration 350 / 2000: training loss 0.547881 val loss: 0.538611\n",
      "iteration 400 / 2000: training loss 0.532641 val loss: 0.537669\n",
      "iteration 450 / 2000: training loss 0.527290 val loss: 0.528758\n",
      "iteration 500 / 2000: training loss 0.492907 val loss: 0.522937\n",
      "iteration 550 / 2000: training loss 0.511204 val loss: 0.529472\n",
      "iteration 600 / 2000: training loss 0.431929 val loss: 0.518199\n",
      "iteration 650 / 2000: training loss 0.424365 val loss: 0.522285\n",
      "iteration 700 / 2000: training loss 0.525384 val loss: 0.525118\n",
      "iteration 750 / 2000: training loss 0.502876 val loss: 0.515190\n",
      "iteration 800 / 2000: training loss 0.546342 val loss: 0.525228\n",
      "iteration 850 / 2000: training loss 0.510282 val loss: 0.517407\n",
      "iteration 900 / 2000: training loss 0.479149 val loss: 0.516405\n",
      "iteration 950 / 2000: training loss 0.445720 val loss: 0.517291\n",
      "iteration 1000 / 2000: training loss 0.479329 val loss: 0.515443\n",
      "iteration 1050 / 2000: training loss 0.485923 val loss: 0.514634\n",
      "hidden_size: 256 lr: 1.000000e-01 lr_decay: 8.000000e-01 reg: 1.000000e-02 train accuracy: 0.940000 val accuracy: 0.940000\n",
      "iteration 0 / 2000: training loss 2.302592 val loss: 2.302597\n",
      "iteration 50 / 2000: training loss 1.928076 val loss: 1.882206\n",
      "iteration 100 / 2000: training loss 0.916430 val loss: 0.952467\n",
      "iteration 150 / 2000: training loss 0.708677 val loss: 0.762829\n",
      "iteration 200 / 2000: training loss 0.722675 val loss: 0.765605\n",
      "iteration 250 / 2000: training loss 0.695775 val loss: 0.775272\n",
      "iteration 300 / 2000: training loss 0.694420 val loss: 0.763450\n",
      "iteration 350 / 2000: training loss 0.751933 val loss: 0.784047\n",
      "iteration 400 / 2000: training loss 0.724178 val loss: 0.736093\n",
      "iteration 450 / 2000: training loss 0.808395 val loss: 0.775868\n",
      "iteration 500 / 2000: training loss 0.740314 val loss: 0.744517\n",
      "iteration 550 / 2000: training loss 0.725444 val loss: 0.751287\n",
      "iteration 600 / 2000: training loss 0.658650 val loss: 0.744971\n",
      "iteration 650 / 2000: training loss 0.655916 val loss: 0.758511\n",
      "iteration 700 / 2000: training loss 0.756187 val loss: 0.733015\n",
      "iteration 750 / 2000: training loss 0.729740 val loss: 0.730380\n",
      "iteration 800 / 2000: training loss 0.748606 val loss: 0.746450\n",
      "iteration 850 / 2000: training loss 0.756125 val loss: 0.771459\n",
      "iteration 900 / 2000: training loss 0.731912 val loss: 0.742886\n",
      "iteration 950 / 2000: training loss 0.701504 val loss: 0.741646\n",
      "iteration 1000 / 2000: training loss 0.712979 val loss: 0.750982\n",
      "iteration 1050 / 2000: training loss 0.721527 val loss: 0.748224\n",
      "hidden_size: 256 lr: 1.000000e-01 lr_decay: 9.800000e-01 reg: 2.000000e-02 train accuracy: 0.890000 val accuracy: 0.921000\n",
      "iteration 0 / 2000: training loss 2.302592 val loss: 2.302597\n",
      "iteration 50 / 2000: training loss 2.008046 val loss: 1.981359\n",
      "iteration 100 / 2000: training loss 0.886296 val loss: 0.939623\n",
      "iteration 150 / 2000: training loss 0.694781 val loss: 0.773707\n",
      "iteration 200 / 2000: training loss 0.717231 val loss: 0.768529\n",
      "iteration 250 / 2000: training loss 0.672693 val loss: 0.752828\n",
      "iteration 300 / 2000: training loss 0.697540 val loss: 0.769710\n",
      "iteration 350 / 2000: training loss 0.748755 val loss: 0.750246\n",
      "iteration 400 / 2000: training loss 0.731031 val loss: 0.739443\n",
      "iteration 450 / 2000: training loss 0.749493 val loss: 0.742417\n",
      "iteration 500 / 2000: training loss 0.717764 val loss: 0.727632\n",
      "iteration 550 / 2000: training loss 0.719587 val loss: 0.723081\n",
      "iteration 600 / 2000: training loss 0.649292 val loss: 0.731184\n",
      "iteration 650 / 2000: training loss 0.636752 val loss: 0.725486\n",
      "iteration 700 / 2000: training loss 0.742600 val loss: 0.737013\n",
      "iteration 750 / 2000: training loss 0.722611 val loss: 0.726248\n",
      "iteration 800 / 2000: training loss 0.720060 val loss: 0.739898\n",
      "iteration 850 / 2000: training loss 0.740267 val loss: 0.721912\n",
      "iteration 900 / 2000: training loss 0.692280 val loss: 0.726184\n",
      "iteration 950 / 2000: training loss 0.678012 val loss: 0.717760\n",
      "iteration 1000 / 2000: training loss 0.698541 val loss: 0.717017\n",
      "iteration 1050 / 2000: training loss 0.699905 val loss: 0.717374\n",
      "iteration 1100 / 2000: training loss 0.709539 val loss: 0.722829\n",
      "iteration 1150 / 2000: training loss 0.674118 val loss: 0.721104\n",
      "iteration 1200 / 2000: training loss 0.744781 val loss: 0.724080\n",
      "iteration 1250 / 2000: training loss 0.765157 val loss: 0.725979\n",
      "iteration 1300 / 2000: training loss 0.721343 val loss: 0.719474\n",
      "iteration 1350 / 2000: training loss 0.761317 val loss: 0.724931\n",
      "iteration 1400 / 2000: training loss 0.680472 val loss: 0.716181\n",
      "iteration 1450 / 2000: training loss 0.760283 val loss: 0.722147\n",
      "iteration 1500 / 2000: training loss 0.692762 val loss: 0.717450\n",
      "iteration 1550 / 2000: training loss 0.748490 val loss: 0.719242\n",
      "iteration 1600 / 2000: training loss 0.735119 val loss: 0.714603\n",
      "iteration 1650 / 2000: training loss 0.717573 val loss: 0.711475\n",
      "iteration 1700 / 2000: training loss 0.667366 val loss: 0.712071\n",
      "iteration 1750 / 2000: training loss 0.637457 val loss: 0.712845\n",
      "iteration 1800 / 2000: training loss 0.759347 val loss: 0.712694\n",
      "iteration 1850 / 2000: training loss 0.714670 val loss: 0.712455\n",
      "iteration 1900 / 2000: training loss 0.624305 val loss: 0.711919\n",
      "iteration 1950 / 2000: training loss 0.713646 val loss: 0.716631\n",
      "hidden_size: 256 lr: 1.000000e-01 lr_decay: 9.000000e-01 reg: 2.000000e-02 train accuracy: 0.930000 val accuracy: 0.933000\n",
      "iteration 0 / 2000: training loss 2.302592 val loss: 2.302597\n",
      "iteration 50 / 2000: training loss 2.066461 val loss: 2.050357\n",
      "iteration 100 / 2000: training loss 0.867908 val loss: 0.916843\n",
      "iteration 150 / 2000: training loss 0.688652 val loss: 0.776192\n",
      "iteration 200 / 2000: training loss 0.710521 val loss: 0.766913\n",
      "iteration 250 / 2000: training loss 0.672508 val loss: 0.747483\n",
      "iteration 300 / 2000: training loss 0.686314 val loss: 0.766799\n",
      "iteration 350 / 2000: training loss 0.743730 val loss: 0.741886\n",
      "iteration 400 / 2000: training loss 0.746987 val loss: 0.736738\n",
      "iteration 450 / 2000: training loss 0.755316 val loss: 0.739693\n",
      "iteration 500 / 2000: training loss 0.705123 val loss: 0.732765\n",
      "iteration 550 / 2000: training loss 0.727448 val loss: 0.730203\n",
      "iteration 600 / 2000: training loss 0.648674 val loss: 0.729483\n",
      "iteration 650 / 2000: training loss 0.640871 val loss: 0.727212\n",
      "iteration 700 / 2000: training loss 0.736603 val loss: 0.727279\n",
      "iteration 750 / 2000: training loss 0.726192 val loss: 0.721558\n",
      "iteration 800 / 2000: training loss 0.737791 val loss: 0.734098\n",
      "iteration 850 / 2000: training loss 0.746735 val loss: 0.723558\n",
      "iteration 900 / 2000: training loss 0.697943 val loss: 0.725296\n",
      "iteration 950 / 2000: training loss 0.671098 val loss: 0.720149\n",
      "iteration 1000 / 2000: training loss 0.701777 val loss: 0.718688\n",
      "iteration 1050 / 2000: training loss 0.697796 val loss: 0.718929\n",
      "iteration 1100 / 2000: training loss 0.701975 val loss: 0.718651\n",
      "iteration 1150 / 2000: training loss 0.673286 val loss: 0.717805\n",
      "iteration 1200 / 2000: training loss 0.734220 val loss: 0.717060\n",
      "iteration 1250 / 2000: training loss 0.750865 val loss: 0.719686\n",
      "iteration 1300 / 2000: training loss 0.720028 val loss: 0.718446\n",
      "iteration 1350 / 2000: training loss 0.741446 val loss: 0.718719\n",
      "hidden_size: 256 lr: 1.000000e-01 lr_decay: 8.500000e-01 reg: 2.000000e-02 train accuracy: 0.940000 val accuracy: 0.932000\n",
      "iteration 0 / 2000: training loss 2.302592 val loss: 2.302597\n",
      "iteration 50 / 2000: training loss 2.124745 val loss: 2.112111\n",
      "iteration 100 / 2000: training loss 0.864940 val loss: 0.905897\n",
      "iteration 150 / 2000: training loss 0.683859 val loss: 0.781941\n",
      "iteration 200 / 2000: training loss 0.709858 val loss: 0.760447\n",
      "iteration 250 / 2000: training loss 0.677321 val loss: 0.750805\n",
      "iteration 300 / 2000: training loss 0.677546 val loss: 0.762053\n",
      "iteration 350 / 2000: training loss 0.753198 val loss: 0.740883\n",
      "iteration 400 / 2000: training loss 0.765314 val loss: 0.742026\n",
      "iteration 450 / 2000: training loss 0.757129 val loss: 0.734947\n",
      "iteration 500 / 2000: training loss 0.707719 val loss: 0.728297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 550 / 2000: training loss 0.719668 val loss: 0.734404\n",
      "iteration 600 / 2000: training loss 0.645747 val loss: 0.723130\n",
      "iteration 650 / 2000: training loss 0.639370 val loss: 0.727719\n",
      "iteration 700 / 2000: training loss 0.744327 val loss: 0.729800\n",
      "iteration 750 / 2000: training loss 0.724688 val loss: 0.721446\n",
      "iteration 800 / 2000: training loss 0.750859 val loss: 0.729534\n",
      "iteration 850 / 2000: training loss 0.735376 val loss: 0.722352\n",
      "iteration 900 / 2000: training loss 0.698638 val loss: 0.722036\n",
      "iteration 950 / 2000: training loss 0.663018 val loss: 0.721734\n",
      "iteration 1000 / 2000: training loss 0.699629 val loss: 0.720575\n",
      "iteration 1050 / 2000: training loss 0.700098 val loss: 0.720051\n",
      "iteration 1100 / 2000: training loss 0.705297 val loss: 0.720695\n",
      "iteration 1150 / 2000: training loss 0.680676 val loss: 0.718955\n",
      "iteration 1200 / 2000: training loss 0.726191 val loss: 0.719251\n",
      "iteration 1250 / 2000: training loss 0.748549 val loss: 0.719577\n",
      "iteration 1300 / 2000: training loss 0.725814 val loss: 0.719065\n",
      "iteration 1350 / 2000: training loss 0.738014 val loss: 0.719089\n",
      "iteration 1400 / 2000: training loss 0.684140 val loss: 0.718523\n",
      "iteration 1450 / 2000: training loss 0.741404 val loss: 0.718746\n",
      "iteration 1500 / 2000: training loss 0.691544 val loss: 0.718106\n",
      "iteration 1550 / 2000: training loss 0.744377 val loss: 0.719019\n",
      "iteration 1600 / 2000: training loss 0.733333 val loss: 0.718633\n",
      "iteration 1650 / 2000: training loss 0.711126 val loss: 0.717180\n",
      "iteration 1700 / 2000: training loss 0.671536 val loss: 0.717920\n",
      "iteration 1750 / 2000: training loss 0.635024 val loss: 0.717021\n",
      "iteration 1800 / 2000: training loss 0.758016 val loss: 0.717799\n",
      "iteration 1850 / 2000: training loss 0.708368 val loss: 0.717040\n",
      "iteration 1900 / 2000: training loss 0.633671 val loss: 0.717565\n",
      "iteration 1950 / 2000: training loss 0.715045 val loss: 0.717188\n",
      "hidden_size: 256 lr: 1.000000e-01 lr_decay: 8.000000e-01 reg: 2.000000e-02 train accuracy: 0.920000 val accuracy: 0.931000\n",
      "iteration 0 / 2000: training loss 2.302599 val loss: 2.302604\n",
      "iteration 50 / 2000: training loss 2.006434 val loss: 1.979094\n",
      "iteration 100 / 2000: training loss 1.213629 val loss: 1.205028\n",
      "iteration 150 / 2000: training loss 1.048205 val loss: 1.073436\n",
      "iteration 200 / 2000: training loss 1.035273 val loss: 1.093828\n",
      "iteration 250 / 2000: training loss 1.036104 val loss: 1.089356\n",
      "iteration 300 / 2000: training loss 1.004552 val loss: 1.073664\n",
      "iteration 350 / 2000: training loss 1.082657 val loss: 1.105453\n",
      "iteration 400 / 2000: training loss 1.093989 val loss: 1.048426\n",
      "iteration 450 / 2000: training loss 1.132590 val loss: 1.087588\n",
      "hidden_size: 256 lr: 1.000000e-01 lr_decay: 9.800000e-01 reg: 4.000000e-02 train accuracy: 0.890000 val accuracy: 0.892000\n",
      "iteration 0 / 2000: training loss 2.302599 val loss: 2.302604\n",
      "iteration 50 / 2000: training loss 2.078518 val loss: 2.066256\n",
      "iteration 100 / 2000: training loss 1.174742 val loss: 1.199815\n",
      "iteration 150 / 2000: training loss 1.025893 val loss: 1.077481\n",
      "iteration 200 / 2000: training loss 1.037848 val loss: 1.073404\n",
      "iteration 250 / 2000: training loss 0.989778 val loss: 1.067034\n",
      "iteration 300 / 2000: training loss 1.002731 val loss: 1.080505\n",
      "iteration 350 / 2000: training loss 1.079798 val loss: 1.067642\n",
      "iteration 400 / 2000: training loss 1.059801 val loss: 1.051159\n",
      "iteration 450 / 2000: training loss 1.090522 val loss: 1.050736\n",
      "iteration 500 / 2000: training loss 1.035856 val loss: 1.041018\n",
      "iteration 550 / 2000: training loss 1.015527 val loss: 1.035210\n",
      "iteration 600 / 2000: training loss 0.964178 val loss: 1.043384\n",
      "iteration 650 / 2000: training loss 0.948642 val loss: 1.035288\n",
      "iteration 700 / 2000: training loss 1.047038 val loss: 1.047007\n",
      "iteration 750 / 2000: training loss 1.037461 val loss: 1.036872\n",
      "iteration 800 / 2000: training loss 1.024181 val loss: 1.055402\n",
      "iteration 850 / 2000: training loss 1.068350 val loss: 1.030228\n",
      "iteration 900 / 2000: training loss 1.000620 val loss: 1.036227\n",
      "iteration 950 / 2000: training loss 0.988798 val loss: 1.028010\n",
      "iteration 1000 / 2000: training loss 1.015890 val loss: 1.028609\n",
      "iteration 1050 / 2000: training loss 1.015668 val loss: 1.023903\n",
      "iteration 1100 / 2000: training loss 1.013861 val loss: 1.030125\n",
      "iteration 1150 / 2000: training loss 0.985188 val loss: 1.032943\n",
      "iteration 1200 / 2000: training loss 1.068386 val loss: 1.036419\n",
      "iteration 1250 / 2000: training loss 1.077757 val loss: 1.030848\n",
      "iteration 1300 / 2000: training loss 1.036522 val loss: 1.030098\n",
      "iteration 1350 / 2000: training loss 1.110734 val loss: 1.043244\n",
      "hidden_size: 256 lr: 1.000000e-01 lr_decay: 9.000000e-01 reg: 4.000000e-02 train accuracy: 0.870000 val accuracy: 0.909000\n",
      "iteration 0 / 2000: training loss 2.302599 val loss: 2.302604\n",
      "iteration 50 / 2000: training loss 2.126759 val loss: 2.118299\n",
      "iteration 100 / 2000: training loss 1.166567 val loss: 1.191173\n",
      "iteration 150 / 2000: training loss 1.016667 val loss: 1.081788\n",
      "iteration 200 / 2000: training loss 1.039453 val loss: 1.075280\n",
      "iteration 250 / 2000: training loss 0.984600 val loss: 1.058050\n",
      "iteration 300 / 2000: training loss 0.994366 val loss: 1.076270\n",
      "iteration 350 / 2000: training loss 1.071333 val loss: 1.059801\n",
      "iteration 400 / 2000: training loss 1.072867 val loss: 1.049016\n",
      "iteration 450 / 2000: training loss 1.080756 val loss: 1.047558\n",
      "iteration 500 / 2000: training loss 1.019471 val loss: 1.039841\n",
      "iteration 550 / 2000: training loss 1.028451 val loss: 1.031951\n",
      "iteration 600 / 2000: training loss 0.966697 val loss: 1.037558\n",
      "iteration 650 / 2000: training loss 0.955854 val loss: 1.036228\n",
      "iteration 700 / 2000: training loss 1.045982 val loss: 1.038496\n",
      "iteration 750 / 2000: training loss 1.039997 val loss: 1.029633\n",
      "iteration 800 / 2000: training loss 1.036906 val loss: 1.044192\n",
      "iteration 850 / 2000: training loss 1.073558 val loss: 1.031590\n",
      "iteration 900 / 2000: training loss 1.009761 val loss: 1.034693\n",
      "iteration 950 / 2000: training loss 0.985901 val loss: 1.026861\n",
      "iteration 1000 / 2000: training loss 1.022593 val loss: 1.025052\n",
      "iteration 1050 / 2000: training loss 1.017282 val loss: 1.026958\n",
      "iteration 1100 / 2000: training loss 1.003541 val loss: 1.026028\n",
      "iteration 1150 / 2000: training loss 0.979105 val loss: 1.024137\n",
      "iteration 1200 / 2000: training loss 1.052105 val loss: 1.024161\n",
      "iteration 1250 / 2000: training loss 1.062380 val loss: 1.026999\n",
      "iteration 1300 / 2000: training loss 1.033628 val loss: 1.026368\n",
      "iteration 1350 / 2000: training loss 1.069411 val loss: 1.025395\n",
      "iteration 1400 / 2000: training loss 1.019018 val loss: 1.025586\n",
      "iteration 1450 / 2000: training loss 1.061793 val loss: 1.028707\n",
      "iteration 1500 / 2000: training loss 1.005648 val loss: 1.022490\n",
      "iteration 1550 / 2000: training loss 1.025660 val loss: 1.023543\n",
      "iteration 1600 / 2000: training loss 1.055310 val loss: 1.021317\n",
      "iteration 1650 / 2000: training loss 1.024757 val loss: 1.019761\n",
      "iteration 1700 / 2000: training loss 0.979087 val loss: 1.022107\n",
      "iteration 1750 / 2000: training loss 0.934488 val loss: 1.020583\n",
      "iteration 1800 / 2000: training loss 1.070803 val loss: 1.019180\n",
      "iteration 1850 / 2000: training loss 1.025327 val loss: 1.020017\n",
      "iteration 1900 / 2000: training loss 0.943507 val loss: 1.019893\n",
      "iteration 1950 / 2000: training loss 1.025377 val loss: 1.019438\n",
      "hidden_size: 256 lr: 1.000000e-01 lr_decay: 8.500000e-01 reg: 4.000000e-02 train accuracy: 0.910000 val accuracy: 0.924000\n",
      "iteration 0 / 2000: training loss 2.302599 val loss: 2.302604\n",
      "iteration 50 / 2000: training loss 2.175532 val loss: 2.164991\n",
      "iteration 100 / 2000: training loss 1.171629 val loss: 1.194240\n",
      "iteration 150 / 2000: training loss 1.012885 val loss: 1.087491\n",
      "iteration 200 / 2000: training loss 1.030154 val loss: 1.073498\n",
      "iteration 250 / 2000: training loss 0.988754 val loss: 1.057210\n",
      "iteration 300 / 2000: training loss 0.989577 val loss: 1.073563\n",
      "iteration 350 / 2000: training loss 1.065307 val loss: 1.051518\n",
      "iteration 400 / 2000: training loss 1.097024 val loss: 1.053111\n",
      "iteration 450 / 2000: training loss 1.085248 val loss: 1.043074\n",
      "iteration 500 / 2000: training loss 1.016685 val loss: 1.036611\n",
      "iteration 550 / 2000: training loss 1.029285 val loss: 1.039505\n",
      "iteration 600 / 2000: training loss 0.961344 val loss: 1.030356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_size: 256 lr: 1.000000e-01 lr_decay: 8.000000e-01 reg: 4.000000e-02 train accuracy: 0.940000 val accuracy: 0.909000\n",
      "iteration 0 / 2000: training loss 2.302606 val loss: 2.302610\n",
      "iteration 50 / 2000: training loss 2.073326 val loss: 2.060763\n",
      "iteration 100 / 2000: training loss 1.414736 val loss: 1.406357\n",
      "iteration 150 / 2000: training loss 1.280947 val loss: 1.313974\n",
      "iteration 200 / 2000: training loss 1.270784 val loss: 1.334201\n",
      "iteration 250 / 2000: training loss 1.274988 val loss: 1.324182\n",
      "iteration 300 / 2000: training loss 1.240688 val loss: 1.310539\n",
      "iteration 350 / 2000: training loss 1.338410 val loss: 1.343694\n",
      "iteration 400 / 2000: training loss 1.359740 val loss: 1.289999\n",
      "iteration 450 / 2000: training loss 1.382268 val loss: 1.328805\n",
      "hidden_size: 256 lr: 1.000000e-01 lr_decay: 9.800000e-01 reg: 6.000000e-02 train accuracy: 0.890000 val accuracy: 0.867000\n",
      "iteration 0 / 2000: training loss 2.302606 val loss: 2.302610\n",
      "iteration 50 / 2000: training loss 2.135203 val loss: 2.130760\n",
      "iteration 100 / 2000: training loss 1.399622 val loss: 1.410602\n",
      "iteration 150 / 2000: training loss 1.270599 val loss: 1.323135\n",
      "iteration 200 / 2000: training loss 1.274633 val loss: 1.317894\n",
      "iteration 250 / 2000: training loss 1.238303 val loss: 1.303228\n",
      "iteration 300 / 2000: training loss 1.243273 val loss: 1.320427\n",
      "iteration 350 / 2000: training loss 1.326757 val loss: 1.309102\n",
      "iteration 400 / 2000: training loss 1.303371 val loss: 1.284612\n",
      "iteration 450 / 2000: training loss 1.345255 val loss: 1.290386\n",
      "iteration 500 / 2000: training loss 1.266952 val loss: 1.282548\n",
      "iteration 550 / 2000: training loss 1.251228 val loss: 1.274782\n",
      "iteration 600 / 2000: training loss 1.209308 val loss: 1.286021\n",
      "iteration 650 / 2000: training loss 1.196316 val loss: 1.279611\n",
      "iteration 700 / 2000: training loss 1.274503 val loss: 1.283013\n",
      "hidden_size: 256 lr: 1.000000e-01 lr_decay: 9.000000e-01 reg: 6.000000e-02 train accuracy: 0.910000 val accuracy: 0.888000\n",
      "iteration 0 / 2000: training loss 2.302606 val loss: 2.302610\n",
      "iteration 50 / 2000: training loss 2.176384 val loss: 2.169612\n",
      "iteration 100 / 2000: training loss 1.400134 val loss: 1.412006\n",
      "iteration 150 / 2000: training loss 1.260825 val loss: 1.322577\n",
      "iteration 200 / 2000: training loss 1.281472 val loss: 1.313402\n",
      "iteration 250 / 2000: training loss 1.228873 val loss: 1.297354\n",
      "iteration 300 / 2000: training loss 1.228357 val loss: 1.314024\n",
      "iteration 350 / 2000: training loss 1.325979 val loss: 1.302115\n",
      "iteration 400 / 2000: training loss 1.318315 val loss: 1.293456\n",
      "iteration 450 / 2000: training loss 1.324841 val loss: 1.283653\n",
      "iteration 500 / 2000: training loss 1.258264 val loss: 1.277852\n",
      "iteration 550 / 2000: training loss 1.259423 val loss: 1.267288\n",
      "iteration 600 / 2000: training loss 1.207547 val loss: 1.274649\n",
      "iteration 650 / 2000: training loss 1.196765 val loss: 1.273382\n",
      "iteration 700 / 2000: training loss 1.277559 val loss: 1.276514\n",
      "iteration 750 / 2000: training loss 1.278122 val loss: 1.266203\n",
      "iteration 800 / 2000: training loss 1.267267 val loss: 1.283894\n",
      "iteration 850 / 2000: training loss 1.318700 val loss: 1.269450\n",
      "iteration 900 / 2000: training loss 1.239330 val loss: 1.272772\n",
      "iteration 950 / 2000: training loss 1.223753 val loss: 1.262977\n",
      "iteration 1000 / 2000: training loss 1.262236 val loss: 1.261079\n",
      "iteration 1050 / 2000: training loss 1.262217 val loss: 1.263844\n",
      "iteration 1100 / 2000: training loss 1.237864 val loss: 1.261902\n",
      "iteration 1150 / 2000: training loss 1.211447 val loss: 1.260735\n",
      "hidden_size: 256 lr: 1.000000e-01 lr_decay: 8.500000e-01 reg: 6.000000e-02 train accuracy: 0.890000 val accuracy: 0.904000\n",
      "iteration 0 / 2000: training loss 2.302606 val loss: 2.302610\n",
      "iteration 50 / 2000: training loss 2.216064 val loss: 2.206371\n",
      "iteration 100 / 2000: training loss 1.425098 val loss: 1.433246\n",
      "iteration 150 / 2000: training loss 1.256661 val loss: 1.323548\n",
      "iteration 200 / 2000: training loss 1.280341 val loss: 1.314294\n",
      "iteration 250 / 2000: training loss 1.229864 val loss: 1.296795\n",
      "iteration 300 / 2000: training loss 1.223745 val loss: 1.310890\n",
      "iteration 350 / 2000: training loss 1.314099 val loss: 1.291659\n",
      "hidden_size: 256 lr: 1.000000e-01 lr_decay: 8.000000e-01 reg: 6.000000e-02 train accuracy: 0.890000 val accuracy: 0.879000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302636\n",
      "iteration 50 / 2000: training loss 0.864766 val loss: 0.902599\n",
      "iteration 100 / 2000: training loss 0.568612 val loss: 0.650488\n",
      "iteration 150 / 2000: training loss 0.832102 val loss: 0.659471\n",
      "iteration 200 / 2000: training loss 0.604391 val loss: 0.606718\n",
      "iteration 250 / 2000: training loss 0.659426 val loss: 0.665604\n",
      "iteration 300 / 2000: training loss 0.639076 val loss: 0.643294\n",
      "iteration 350 / 2000: training loss 0.585434 val loss: 0.668117\n",
      "iteration 400 / 2000: training loss 0.569210 val loss: 0.561716\n",
      "iteration 450 / 2000: training loss 0.635892 val loss: 0.605522\n",
      "iteration 500 / 2000: training loss 0.635994 val loss: 0.646089\n",
      "iteration 550 / 2000: training loss 0.648469 val loss: 0.669237\n",
      "iteration 600 / 2000: training loss 0.513907 val loss: 0.622881\n",
      "iteration 650 / 2000: training loss 0.491760 val loss: 0.609179\n",
      "iteration 700 / 2000: training loss 0.675327 val loss: 0.629007\n",
      "iteration 750 / 2000: training loss 0.554723 val loss: 0.580088\n",
      "iteration 800 / 2000: training loss 0.596131 val loss: 0.615327\n",
      "iteration 850 / 2000: training loss 0.606781 val loss: 0.580033\n",
      "iteration 900 / 2000: training loss 0.567043 val loss: 0.572171\n",
      "iteration 950 / 2000: training loss 0.545499 val loss: 0.610855\n",
      "iteration 1000 / 2000: training loss 0.555846 val loss: 0.594627\n",
      "iteration 1050 / 2000: training loss 0.537128 val loss: 0.595320\n",
      "hidden_size: 256 lr: 3.000000e-01 lr_decay: 9.800000e-01 reg: 1.000000e-02 train accuracy: 0.970000 val accuracy: 0.920000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302636\n",
      "iteration 50 / 2000: training loss 0.833274 val loss: 0.823894\n",
      "iteration 100 / 2000: training loss 0.597475 val loss: 0.608645\n",
      "iteration 150 / 2000: training loss 0.661408 val loss: 0.683050\n",
      "iteration 200 / 2000: training loss 0.584383 val loss: 0.640208\n",
      "iteration 250 / 2000: training loss 0.559433 val loss: 0.642059\n",
      "iteration 300 / 2000: training loss 0.562430 val loss: 0.616642\n",
      "iteration 350 / 2000: training loss 0.559330 val loss: 0.585807\n",
      "iteration 400 / 2000: training loss 0.547694 val loss: 0.541426\n",
      "iteration 450 / 2000: training loss 0.598615 val loss: 0.583345\n",
      "iteration 500 / 2000: training loss 0.553467 val loss: 0.569981\n",
      "iteration 550 / 2000: training loss 0.513096 val loss: 0.523807\n",
      "iteration 600 / 2000: training loss 0.446394 val loss: 0.550001\n",
      "iteration 650 / 2000: training loss 0.456196 val loss: 0.552530\n",
      "iteration 700 / 2000: training loss 0.534227 val loss: 0.516961\n",
      "iteration 750 / 2000: training loss 0.498770 val loss: 0.526755\n",
      "iteration 800 / 2000: training loss 0.542702 val loss: 0.536728\n",
      "iteration 850 / 2000: training loss 0.515072 val loss: 0.553815\n",
      "iteration 900 / 2000: training loss 0.497505 val loss: 0.532392\n",
      "iteration 950 / 2000: training loss 0.494161 val loss: 0.517583\n",
      "iteration 1000 / 2000: training loss 0.484010 val loss: 0.518839\n",
      "iteration 1050 / 2000: training loss 0.482906 val loss: 0.533591\n",
      "hidden_size: 256 lr: 3.000000e-01 lr_decay: 9.000000e-01 reg: 1.000000e-02 train accuracy: 0.920000 val accuracy: 0.935000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302636\n",
      "iteration 50 / 2000: training loss 0.763045 val loss: 0.811537\n",
      "iteration 100 / 2000: training loss 0.635496 val loss: 0.602818\n",
      "iteration 150 / 2000: training loss 0.580974 val loss: 0.632577\n",
      "iteration 200 / 2000: training loss 0.537578 val loss: 0.623188\n",
      "iteration 250 / 2000: training loss 0.527430 val loss: 0.604224\n",
      "iteration 300 / 2000: training loss 0.500389 val loss: 0.571395\n",
      "iteration 350 / 2000: training loss 0.564986 val loss: 0.576938\n",
      "iteration 400 / 2000: training loss 0.505840 val loss: 0.530578\n",
      "iteration 450 / 2000: training loss 0.582457 val loss: 0.578658\n",
      "hidden_size: 256 lr: 3.000000e-01 lr_decay: 8.500000e-01 reg: 1.000000e-02 train accuracy: 0.920000 val accuracy: 0.921000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302636\n",
      "iteration 50 / 2000: training loss 0.758219 val loss: 0.817792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 2000: training loss 0.635616 val loss: 0.609758\n",
      "iteration 150 / 2000: training loss 0.575192 val loss: 0.594148\n",
      "iteration 200 / 2000: training loss 0.513824 val loss: 0.605578\n",
      "iteration 250 / 2000: training loss 0.530345 val loss: 0.572292\n",
      "iteration 300 / 2000: training loss 0.496483 val loss: 0.557844\n",
      "iteration 350 / 2000: training loss 0.539102 val loss: 0.574241\n",
      "iteration 400 / 2000: training loss 0.499548 val loss: 0.532259\n",
      "iteration 450 / 2000: training loss 0.514473 val loss: 0.548163\n",
      "iteration 500 / 2000: training loss 0.496102 val loss: 0.511536\n",
      "iteration 550 / 2000: training loss 0.513011 val loss: 0.507394\n",
      "iteration 600 / 2000: training loss 0.434130 val loss: 0.514332\n",
      "iteration 650 / 2000: training loss 0.427718 val loss: 0.510115\n",
      "iteration 700 / 2000: training loss 0.514126 val loss: 0.514394\n",
      "iteration 750 / 2000: training loss 0.482899 val loss: 0.506097\n",
      "iteration 800 / 2000: training loss 0.516872 val loss: 0.516135\n",
      "iteration 850 / 2000: training loss 0.511656 val loss: 0.506947\n",
      "iteration 900 / 2000: training loss 0.473667 val loss: 0.511168\n",
      "iteration 950 / 2000: training loss 0.452499 val loss: 0.503446\n",
      "iteration 1000 / 2000: training loss 0.476432 val loss: 0.501589\n",
      "iteration 1050 / 2000: training loss 0.473784 val loss: 0.500151\n",
      "iteration 1100 / 2000: training loss 0.493208 val loss: 0.498884\n",
      "iteration 1150 / 2000: training loss 0.449302 val loss: 0.501329\n",
      "iteration 1200 / 2000: training loss 0.497481 val loss: 0.499866\n",
      "iteration 1250 / 2000: training loss 0.526746 val loss: 0.502442\n",
      "hidden_size: 256 lr: 3.000000e-01 lr_decay: 8.000000e-01 reg: 1.000000e-02 train accuracy: 0.970000 val accuracy: 0.942000\n",
      "iteration 0 / 2000: training loss 2.302592 val loss: 2.302639\n",
      "iteration 50 / 2000: training loss 1.093113 val loss: 1.124788\n",
      "iteration 100 / 2000: training loss 0.877294 val loss: 0.849428\n",
      "iteration 150 / 2000: training loss 1.003635 val loss: 0.939919\n",
      "iteration 200 / 2000: training loss 0.807995 val loss: 0.836554\n",
      "iteration 250 / 2000: training loss 0.851965 val loss: 0.853161\n",
      "iteration 300 / 2000: training loss 0.824995 val loss: 0.917711\n",
      "iteration 350 / 2000: training loss 0.910746 val loss: 0.880438\n",
      "iteration 400 / 2000: training loss 0.852686 val loss: 0.839395\n",
      "iteration 450 / 2000: training loss 0.862158 val loss: 0.878944\n",
      "iteration 500 / 2000: training loss 0.768773 val loss: 0.818410\n",
      "hidden_size: 256 lr: 3.000000e-01 lr_decay: 9.800000e-01 reg: 2.000000e-02 train accuracy: 0.910000 val accuracy: 0.862000\n",
      "iteration 0 / 2000: training loss 2.302592 val loss: 2.302639\n",
      "iteration 50 / 2000: training loss 1.014463 val loss: 1.070876\n",
      "iteration 100 / 2000: training loss 0.856194 val loss: 0.848662\n",
      "iteration 150 / 2000: training loss 0.981661 val loss: 0.884510\n",
      "iteration 200 / 2000: training loss 0.799033 val loss: 0.845197\n",
      "iteration 250 / 2000: training loss 0.856576 val loss: 0.832671\n",
      "iteration 300 / 2000: training loss 0.777334 val loss: 0.815226\n",
      "iteration 350 / 2000: training loss 0.793327 val loss: 0.861851\n",
      "iteration 400 / 2000: training loss 0.801253 val loss: 0.780246\n",
      "iteration 450 / 2000: training loss 0.843064 val loss: 0.790285\n",
      "iteration 500 / 2000: training loss 0.734547 val loss: 0.761961\n",
      "iteration 550 / 2000: training loss 0.736056 val loss: 0.779669\n",
      "iteration 600 / 2000: training loss 0.680375 val loss: 0.807197\n",
      "iteration 650 / 2000: training loss 0.680912 val loss: 0.756750\n",
      "iteration 700 / 2000: training loss 0.754718 val loss: 0.753413\n",
      "iteration 750 / 2000: training loss 0.772901 val loss: 0.733919\n",
      "iteration 800 / 2000: training loss 0.791290 val loss: 0.742953\n",
      "iteration 850 / 2000: training loss 0.759652 val loss: 0.760303\n",
      "iteration 900 / 2000: training loss 0.757701 val loss: 0.735528\n",
      "iteration 950 / 2000: training loss 0.700883 val loss: 0.742584\n",
      "iteration 1000 / 2000: training loss 0.709218 val loss: 0.749089\n",
      "iteration 1050 / 2000: training loss 0.717762 val loss: 0.748973\n",
      "iteration 1100 / 2000: training loss 0.750331 val loss: 0.747473\n",
      "iteration 1150 / 2000: training loss 0.677513 val loss: 0.746706\n",
      "iteration 1200 / 2000: training loss 0.741025 val loss: 0.728018\n",
      "iteration 1250 / 2000: training loss 0.790753 val loss: 0.744343\n",
      "iteration 1300 / 2000: training loss 0.713926 val loss: 0.726541\n",
      "iteration 1350 / 2000: training loss 0.787742 val loss: 0.730717\n",
      "iteration 1400 / 2000: training loss 0.682069 val loss: 0.727579\n",
      "iteration 1450 / 2000: training loss 0.762509 val loss: 0.729814\n",
      "iteration 1500 / 2000: training loss 0.716905 val loss: 0.723494\n",
      "iteration 1550 / 2000: training loss 0.750790 val loss: 0.720917\n",
      "iteration 1600 / 2000: training loss 0.748657 val loss: 0.719316\n",
      "iteration 1650 / 2000: training loss 0.713808 val loss: 0.721737\n",
      "iteration 1700 / 2000: training loss 0.680545 val loss: 0.718886\n",
      "hidden_size: 256 lr: 3.000000e-01 lr_decay: 9.000000e-01 reg: 2.000000e-02 train accuracy: 0.890000 val accuracy: 0.926000\n",
      "iteration 0 / 2000: training loss 2.302592 val loss: 2.302639\n",
      "iteration 50 / 2000: training loss 0.966115 val loss: 1.050271\n",
      "iteration 100 / 2000: training loss 0.817417 val loss: 0.822288\n",
      "iteration 150 / 2000: training loss 0.951989 val loss: 0.874692\n",
      "iteration 200 / 2000: training loss 0.793047 val loss: 0.845142\n",
      "iteration 250 / 2000: training loss 0.763667 val loss: 0.843266\n",
      "iteration 300 / 2000: training loss 0.743174 val loss: 0.792342\n",
      "iteration 350 / 2000: training loss 0.755028 val loss: 0.803252\n",
      "iteration 400 / 2000: training loss 0.783726 val loss: 0.755051\n",
      "iteration 450 / 2000: training loss 0.834720 val loss: 0.794433\n",
      "iteration 500 / 2000: training loss 0.764071 val loss: 0.764259\n",
      "iteration 550 / 2000: training loss 0.730679 val loss: 0.746169\n",
      "iteration 600 / 2000: training loss 0.657274 val loss: 0.742974\n",
      "iteration 650 / 2000: training loss 0.646418 val loss: 0.749196\n",
      "iteration 700 / 2000: training loss 0.752942 val loss: 0.728267\n",
      "iteration 750 / 2000: training loss 0.719822 val loss: 0.737558\n",
      "iteration 800 / 2000: training loss 0.719826 val loss: 0.742881\n",
      "iteration 850 / 2000: training loss 0.722495 val loss: 0.734486\n",
      "iteration 900 / 2000: training loss 0.698841 val loss: 0.735021\n",
      "iteration 950 / 2000: training loss 0.681878 val loss: 0.723939\n",
      "iteration 1000 / 2000: training loss 0.695792 val loss: 0.720778\n",
      "iteration 1050 / 2000: training loss 0.693905 val loss: 0.718914\n",
      "iteration 1100 / 2000: training loss 0.717912 val loss: 0.721746\n",
      "iteration 1150 / 2000: training loss 0.668682 val loss: 0.724450\n",
      "iteration 1200 / 2000: training loss 0.749218 val loss: 0.727240\n",
      "iteration 1250 / 2000: training loss 0.766235 val loss: 0.721948\n",
      "hidden_size: 256 lr: 3.000000e-01 lr_decay: 8.500000e-01 reg: 2.000000e-02 train accuracy: 0.950000 val accuracy: 0.928000\n",
      "iteration 0 / 2000: training loss 2.302592 val loss: 2.302639\n",
      "iteration 50 / 2000: training loss 0.979110 val loss: 1.050373\n",
      "iteration 100 / 2000: training loss 0.797762 val loss: 0.795233\n",
      "iteration 150 / 2000: training loss 0.847984 val loss: 0.863393\n",
      "iteration 200 / 2000: training loss 0.765045 val loss: 0.827414\n",
      "iteration 250 / 2000: training loss 0.738282 val loss: 0.820041\n",
      "iteration 300 / 2000: training loss 0.692666 val loss: 0.759003\n",
      "iteration 350 / 2000: training loss 0.769455 val loss: 0.793994\n",
      "iteration 400 / 2000: training loss 0.724815 val loss: 0.727200\n",
      "iteration 450 / 2000: training loss 0.804798 val loss: 0.776481\n",
      "hidden_size: 256 lr: 3.000000e-01 lr_decay: 8.000000e-01 reg: 2.000000e-02 train accuracy: 0.900000 val accuracy: 0.914000\n",
      "iteration 0 / 2000: training loss 2.302599 val loss: 2.302646\n",
      "iteration 50 / 2000: training loss 1.309764 val loss: 1.398093\n",
      "iteration 100 / 2000: training loss 1.167948 val loss: 1.281585\n",
      "iteration 150 / 2000: training loss 1.299563 val loss: 1.322287\n",
      "iteration 200 / 2000: training loss 1.121315 val loss: 1.150407\n",
      "iteration 250 / 2000: training loss 1.258961 val loss: 1.198481\n",
      "iteration 300 / 2000: training loss 1.087001 val loss: 1.284837\n",
      "iteration 350 / 2000: training loss 1.265730 val loss: 1.223814\n",
      "iteration 400 / 2000: training loss 1.262054 val loss: 1.145519\n",
      "iteration 450 / 2000: training loss 1.215081 val loss: 1.161814\n",
      "iteration 500 / 2000: training loss 1.100616 val loss: 1.173794\n",
      "iteration 550 / 2000: training loss 1.085101 val loss: 1.131321\n",
      "iteration 600 / 2000: training loss 1.109954 val loss: 1.174604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 650 / 2000: training loss 1.155543 val loss: 1.148321\n",
      "iteration 700 / 2000: training loss 1.262357 val loss: 1.219738\n",
      "iteration 750 / 2000: training loss 1.154379 val loss: 1.148530\n",
      "iteration 800 / 2000: training loss 1.089214 val loss: 1.145151\n",
      "iteration 850 / 2000: training loss 1.180549 val loss: 1.151186\n",
      "iteration 900 / 2000: training loss 1.115845 val loss: 1.163532\n",
      "iteration 950 / 2000: training loss 1.065834 val loss: 1.209910\n",
      "iteration 1000 / 2000: training loss 1.197198 val loss: 1.342680\n",
      "iteration 1050 / 2000: training loss 1.128254 val loss: 1.146775\n",
      "iteration 1100 / 2000: training loss 1.060183 val loss: 1.106408\n",
      "iteration 1150 / 2000: training loss 1.125335 val loss: 1.163405\n",
      "iteration 1200 / 2000: training loss 1.173760 val loss: 1.127089\n",
      "iteration 1250 / 2000: training loss 1.179591 val loss: 1.097662\n",
      "iteration 1300 / 2000: training loss 1.233023 val loss: 1.101381\n",
      "iteration 1350 / 2000: training loss 1.196908 val loss: 1.185484\n",
      "iteration 1400 / 2000: training loss 1.120529 val loss: 1.178678\n",
      "iteration 1450 / 2000: training loss 1.256749 val loss: 1.152496\n",
      "iteration 1500 / 2000: training loss 1.117858 val loss: 1.163755\n",
      "iteration 1550 / 2000: training loss 1.245953 val loss: 1.105469\n",
      "iteration 1600 / 2000: training loss 1.194646 val loss: 1.159873\n",
      "iteration 1650 / 2000: training loss 1.107468 val loss: 1.106137\n",
      "iteration 1700 / 2000: training loss 1.019054 val loss: 1.118654\n",
      "iteration 1750 / 2000: training loss 1.123081 val loss: 1.145824\n",
      "iteration 1800 / 2000: training loss 1.143893 val loss: 1.107693\n",
      "iteration 1850 / 2000: training loss 1.108583 val loss: 1.117034\n",
      "hidden_size: 256 lr: 3.000000e-01 lr_decay: 9.800000e-01 reg: 4.000000e-02 train accuracy: 0.920000 val accuracy: 0.877000\n",
      "iteration 0 / 2000: training loss 2.302599 val loss: 2.302646\n",
      "iteration 50 / 2000: training loss 1.197324 val loss: 1.358438\n",
      "iteration 100 / 2000: training loss 1.179114 val loss: 1.220194\n",
      "iteration 150 / 2000: training loss 1.257437 val loss: 1.242344\n",
      "iteration 200 / 2000: training loss 1.101068 val loss: 1.126200\n",
      "iteration 250 / 2000: training loss 1.186417 val loss: 1.135587\n",
      "iteration 300 / 2000: training loss 1.050828 val loss: 1.159218\n",
      "iteration 350 / 2000: training loss 1.151450 val loss: 1.119170\n",
      "iteration 400 / 2000: training loss 1.151495 val loss: 1.108236\n",
      "iteration 450 / 2000: training loss 1.143664 val loss: 1.117251\n",
      "iteration 500 / 2000: training loss 1.075360 val loss: 1.106438\n",
      "hidden_size: 256 lr: 3.000000e-01 lr_decay: 9.000000e-01 reg: 4.000000e-02 train accuracy: 0.890000 val accuracy: 0.839000\n",
      "iteration 0 / 2000: training loss 2.302599 val loss: 2.302646\n",
      "iteration 50 / 2000: training loss 1.254074 val loss: 1.303060\n",
      "iteration 100 / 2000: training loss 1.186735 val loss: 1.166863\n",
      "iteration 150 / 2000: training loss 1.209459 val loss: 1.221646\n",
      "iteration 200 / 2000: training loss 1.099722 val loss: 1.145080\n",
      "iteration 250 / 2000: training loss 1.139204 val loss: 1.101329\n",
      "iteration 300 / 2000: training loss 1.043120 val loss: 1.093982\n",
      "iteration 350 / 2000: training loss 1.107808 val loss: 1.177846\n",
      "iteration 400 / 2000: training loss 1.121273 val loss: 1.082919\n",
      "iteration 450 / 2000: training loss 1.161818 val loss: 1.101214\n",
      "iteration 500 / 2000: training loss 1.054027 val loss: 1.063573\n",
      "hidden_size: 256 lr: 3.000000e-01 lr_decay: 8.500000e-01 reg: 4.000000e-02 train accuracy: 0.930000 val accuracy: 0.853000\n",
      "iteration 0 / 2000: training loss 2.302599 val loss: 2.302646\n",
      "iteration 50 / 2000: training loss 1.333222 val loss: 1.324845\n",
      "iteration 100 / 2000: training loss 1.141742 val loss: 1.131407\n",
      "iteration 150 / 2000: training loss 1.168167 val loss: 1.194144\n",
      "iteration 200 / 2000: training loss 1.084437 val loss: 1.149962\n",
      "iteration 250 / 2000: training loss 1.091732 val loss: 1.111422\n",
      "iteration 300 / 2000: training loss 1.029987 val loss: 1.080418\n",
      "iteration 350 / 2000: training loss 1.057270 val loss: 1.108727\n",
      "iteration 400 / 2000: training loss 1.108246 val loss: 1.052049\n",
      "iteration 450 / 2000: training loss 1.142405 val loss: 1.096379\n",
      "iteration 500 / 2000: training loss 1.043302 val loss: 1.048312\n",
      "iteration 550 / 2000: training loss 1.015588 val loss: 1.055311\n",
      "iteration 600 / 2000: training loss 0.972233 val loss: 1.052017\n",
      "iteration 650 / 2000: training loss 0.947524 val loss: 1.038634\n",
      "iteration 700 / 2000: training loss 1.045382 val loss: 1.037591\n",
      "iteration 750 / 2000: training loss 1.034370 val loss: 1.034085\n",
      "iteration 800 / 2000: training loss 1.021887 val loss: 1.054080\n",
      "iteration 850 / 2000: training loss 1.066902 val loss: 1.026271\n",
      "iteration 900 / 2000: training loss 0.997738 val loss: 1.032414\n",
      "iteration 950 / 2000: training loss 0.986366 val loss: 1.021813\n",
      "iteration 1000 / 2000: training loss 1.012471 val loss: 1.019179\n",
      "iteration 1050 / 2000: training loss 1.012971 val loss: 1.020688\n",
      "iteration 1100 / 2000: training loss 1.003222 val loss: 1.024670\n",
      "iteration 1150 / 2000: training loss 0.973307 val loss: 1.019747\n",
      "iteration 1200 / 2000: training loss 1.052455 val loss: 1.021129\n",
      "iteration 1250 / 2000: training loss 1.064873 val loss: 1.024359\n",
      "iteration 1300 / 2000: training loss 1.026646 val loss: 1.022555\n",
      "iteration 1350 / 2000: training loss 1.070183 val loss: 1.021733\n",
      "iteration 1400 / 2000: training loss 1.014800 val loss: 1.020542\n",
      "iteration 1450 / 2000: training loss 1.057700 val loss: 1.024599\n",
      "iteration 1500 / 2000: training loss 1.000917 val loss: 1.017651\n",
      "iteration 1550 / 2000: training loss 1.020781 val loss: 1.018643\n",
      "iteration 1600 / 2000: training loss 1.049988 val loss: 1.016417\n",
      "iteration 1650 / 2000: training loss 1.018212 val loss: 1.014506\n",
      "iteration 1700 / 2000: training loss 0.975368 val loss: 1.016812\n",
      "iteration 1750 / 2000: training loss 0.931009 val loss: 1.015079\n",
      "iteration 1800 / 2000: training loss 1.062370 val loss: 1.014176\n",
      "iteration 1850 / 2000: training loss 1.023464 val loss: 1.015043\n",
      "iteration 1900 / 2000: training loss 0.940813 val loss: 1.014912\n",
      "iteration 1950 / 2000: training loss 1.017943 val loss: 1.014413\n",
      "hidden_size: 256 lr: 3.000000e-01 lr_decay: 8.000000e-01 reg: 4.000000e-02 train accuracy: 0.910000 val accuracy: 0.924000\n",
      "iteration 0 / 2000: training loss 2.302606 val loss: 2.302652\n",
      "iteration 50 / 2000: training loss 1.430948 val loss: 1.590409\n",
      "iteration 100 / 2000: training loss 1.480948 val loss: 1.567665\n",
      "iteration 150 / 2000: training loss 1.578583 val loss: 1.538474\n",
      "iteration 200 / 2000: training loss 1.388642 val loss: 1.406173\n",
      "iteration 250 / 2000: training loss 1.543837 val loss: 1.410325\n",
      "iteration 300 / 2000: training loss 1.432518 val loss: 1.532453\n",
      "iteration 350 / 2000: training loss 1.631079 val loss: 1.445213\n",
      "iteration 400 / 2000: training loss 1.498654 val loss: 1.408641\n",
      "iteration 450 / 2000: training loss 1.441369 val loss: 1.408529\n",
      "iteration 500 / 2000: training loss 1.324632 val loss: 1.372812\n",
      "iteration 550 / 2000: training loss 1.361296 val loss: 1.397678\n",
      "iteration 600 / 2000: training loss 1.397913 val loss: 1.447345\n",
      "iteration 650 / 2000: training loss 1.344183 val loss: 1.404846\n",
      "iteration 700 / 2000: training loss 1.451451 val loss: 1.490017\n",
      "iteration 750 / 2000: training loss 1.427062 val loss: 1.415747\n",
      "iteration 800 / 2000: training loss 1.338529 val loss: 1.375170\n",
      "iteration 850 / 2000: training loss 1.456295 val loss: 1.397475\n",
      "iteration 900 / 2000: training loss 1.419881 val loss: 1.422978\n",
      "iteration 950 / 2000: training loss 1.349895 val loss: 1.521697\n",
      "iteration 1000 / 2000: training loss 1.463687 val loss: 1.508435\n",
      "iteration 1050 / 2000: training loss 1.420986 val loss: 1.433655\n",
      "iteration 1100 / 2000: training loss 1.285255 val loss: 1.338897\n",
      "iteration 1150 / 2000: training loss 1.423248 val loss: 1.406929\n",
      "iteration 1200 / 2000: training loss 1.451226 val loss: 1.361480\n",
      "iteration 1250 / 2000: training loss 1.411716 val loss: 1.363283\n",
      "hidden_size: 256 lr: 3.000000e-01 lr_decay: 9.800000e-01 reg: 6.000000e-02 train accuracy: 0.930000 val accuracy: 0.830000\n",
      "iteration 0 / 2000: training loss 2.302606 val loss: 2.302652\n",
      "iteration 50 / 2000: training loss 1.457644 val loss: 1.507749\n",
      "iteration 100 / 2000: training loss 1.396969 val loss: 1.485985\n",
      "iteration 150 / 2000: training loss 1.488572 val loss: 1.494153\n",
      "iteration 200 / 2000: training loss 1.335387 val loss: 1.342880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 250 / 2000: training loss 1.426545 val loss: 1.375304\n",
      "iteration 300 / 2000: training loss 1.346872 val loss: 1.449204\n",
      "iteration 350 / 2000: training loss 1.429755 val loss: 1.442955\n",
      "iteration 400 / 2000: training loss 1.435871 val loss: 1.364465\n",
      "iteration 450 / 2000: training loss 1.419829 val loss: 1.363356\n",
      "iteration 500 / 2000: training loss 1.302493 val loss: 1.343531\n",
      "iteration 550 / 2000: training loss 1.285730 val loss: 1.351897\n",
      "iteration 600 / 2000: training loss 1.320229 val loss: 1.368869\n",
      "iteration 650 / 2000: training loss 1.222301 val loss: 1.290175\n",
      "iteration 700 / 2000: training loss 1.337658 val loss: 1.366538\n",
      "iteration 750 / 2000: training loss 1.368685 val loss: 1.310237\n",
      "iteration 800 / 2000: training loss 1.304779 val loss: 1.304541\n",
      "iteration 850 / 2000: training loss 1.358012 val loss: 1.317661\n",
      "iteration 900 / 2000: training loss 1.348154 val loss: 1.323509\n",
      "iteration 950 / 2000: training loss 1.268882 val loss: 1.316248\n",
      "iteration 1000 / 2000: training loss 1.300584 val loss: 1.348821\n",
      "iteration 1050 / 2000: training loss 1.280820 val loss: 1.293623\n",
      "hidden_size: 256 lr: 3.000000e-01 lr_decay: 9.000000e-01 reg: 6.000000e-02 train accuracy: 0.850000 val accuracy: 0.867000\n",
      "iteration 0 / 2000: training loss 2.302606 val loss: 2.302652\n",
      "iteration 50 / 2000: training loss 1.524143 val loss: 1.505363\n",
      "iteration 100 / 2000: training loss 1.403504 val loss: 1.426166\n",
      "iteration 150 / 2000: training loss 1.463922 val loss: 1.503516\n",
      "iteration 200 / 2000: training loss 1.355425 val loss: 1.398802\n",
      "iteration 250 / 2000: training loss 1.429369 val loss: 1.357822\n",
      "iteration 300 / 2000: training loss 1.285932 val loss: 1.360609\n",
      "iteration 350 / 2000: training loss 1.383289 val loss: 1.412882\n",
      "iteration 400 / 2000: training loss 1.356356 val loss: 1.327427\n",
      "iteration 450 / 2000: training loss 1.407883 val loss: 1.336045\n",
      "hidden_size: 256 lr: 3.000000e-01 lr_decay: 8.500000e-01 reg: 6.000000e-02 train accuracy: 0.880000 val accuracy: 0.873000\n",
      "iteration 0 / 2000: training loss 2.302606 val loss: 2.302652\n",
      "iteration 50 / 2000: training loss 1.572736 val loss: 1.564914\n",
      "iteration 100 / 2000: training loss 1.405311 val loss: 1.393689\n",
      "iteration 150 / 2000: training loss 1.418961 val loss: 1.464798\n",
      "iteration 200 / 2000: training loss 1.334071 val loss: 1.398393\n",
      "iteration 250 / 2000: training loss 1.330195 val loss: 1.317655\n",
      "iteration 300 / 2000: training loss 1.273957 val loss: 1.333948\n",
      "iteration 350 / 2000: training loss 1.294787 val loss: 1.361379\n",
      "iteration 400 / 2000: training loss 1.377947 val loss: 1.289187\n",
      "iteration 450 / 2000: training loss 1.386383 val loss: 1.331504\n",
      "iteration 500 / 2000: training loss 1.279548 val loss: 1.290471\n",
      "iteration 550 / 2000: training loss 1.260717 val loss: 1.299171\n",
      "iteration 600 / 2000: training loss 1.228852 val loss: 1.294801\n",
      "iteration 650 / 2000: training loss 1.202740 val loss: 1.291218\n",
      "iteration 700 / 2000: training loss 1.269675 val loss: 1.274611\n",
      "iteration 750 / 2000: training loss 1.271192 val loss: 1.274567\n",
      "iteration 800 / 2000: training loss 1.256204 val loss: 1.293036\n",
      "iteration 850 / 2000: training loss 1.316495 val loss: 1.265110\n",
      "iteration 900 / 2000: training loss 1.229856 val loss: 1.273215\n",
      "iteration 950 / 2000: training loss 1.224114 val loss: 1.260256\n",
      "iteration 1000 / 2000: training loss 1.251758 val loss: 1.258342\n",
      "iteration 1050 / 2000: training loss 1.255088 val loss: 1.257914\n",
      "iteration 1100 / 2000: training loss 1.239690 val loss: 1.262261\n",
      "iteration 1150 / 2000: training loss 1.209791 val loss: 1.258913\n",
      "hidden_size: 256 lr: 3.000000e-01 lr_decay: 8.000000e-01 reg: 6.000000e-02 train accuracy: 0.900000 val accuracy: 0.905000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302721\n",
      "iteration 50 / 2000: training loss 1.090859 val loss: 1.050780\n",
      "iteration 100 / 2000: training loss 0.832678 val loss: 0.898116\n",
      "iteration 150 / 2000: training loss 1.535126 val loss: 1.011634\n",
      "iteration 200 / 2000: training loss 0.816884 val loss: 0.854545\n",
      "iteration 250 / 2000: training loss 0.842403 val loss: 0.905993\n",
      "iteration 300 / 2000: training loss 0.770398 val loss: 0.811962\n",
      "iteration 350 / 2000: training loss 0.798141 val loss: 0.829735\n",
      "iteration 400 / 2000: training loss 0.686632 val loss: 0.736662\n",
      "iteration 450 / 2000: training loss 0.772019 val loss: 0.739739\n",
      "iteration 500 / 2000: training loss 0.859768 val loss: 0.788624\n",
      "iteration 550 / 2000: training loss 0.676593 val loss: 0.778595\n",
      "iteration 600 / 2000: training loss 0.661017 val loss: 0.749400\n",
      "iteration 650 / 2000: training loss 0.643473 val loss: 0.779927\n",
      "iteration 700 / 2000: training loss 0.813026 val loss: 0.738698\n",
      "iteration 750 / 2000: training loss 0.660557 val loss: 0.633812\n",
      "iteration 800 / 2000: training loss 0.693523 val loss: 0.711018\n",
      "iteration 850 / 2000: training loss 0.758825 val loss: 0.715763\n",
      "iteration 900 / 2000: training loss 0.690925 val loss: 0.747716\n",
      "iteration 950 / 2000: training loss 0.601115 val loss: 0.714833\n",
      "iteration 1000 / 2000: training loss 0.707046 val loss: 0.861986\n",
      "iteration 1050 / 2000: training loss 0.667595 val loss: 0.751990\n",
      "iteration 1100 / 2000: training loss 0.693677 val loss: 0.655638\n",
      "iteration 1150 / 2000: training loss 0.620019 val loss: 0.691572\n",
      "hidden_size: 256 lr: 5.000000e-01 lr_decay: 9.800000e-01 reg: 1.000000e-02 train accuracy: 0.930000 val accuracy: 0.877000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302721\n",
      "iteration 50 / 2000: training loss 1.131774 val loss: 1.046268\n",
      "iteration 100 / 2000: training loss 0.666911 val loss: 0.755837\n",
      "iteration 150 / 2000: training loss 0.932422 val loss: 0.802717\n",
      "iteration 200 / 2000: training loss 0.706309 val loss: 0.745330\n",
      "iteration 250 / 2000: training loss 0.702525 val loss: 0.720717\n",
      "iteration 300 / 2000: training loss 0.640409 val loss: 0.748705\n",
      "iteration 350 / 2000: training loss 0.682403 val loss: 0.686969\n",
      "iteration 400 / 2000: training loss 0.568975 val loss: 0.608765\n",
      "iteration 450 / 2000: training loss 0.627292 val loss: 0.587721\n",
      "iteration 500 / 2000: training loss 0.564890 val loss: 0.607156\n",
      "iteration 550 / 2000: training loss 0.602122 val loss: 0.635335\n",
      "iteration 600 / 2000: training loss 0.482974 val loss: 0.618396\n",
      "iteration 650 / 2000: training loss 0.477263 val loss: 0.548715\n",
      "iteration 700 / 2000: training loss 0.604651 val loss: 0.583491\n",
      "iteration 750 / 2000: training loss 0.585638 val loss: 0.533666\n",
      "iteration 800 / 2000: training loss 0.590214 val loss: 0.557986\n",
      "iteration 850 / 2000: training loss 0.577729 val loss: 0.570171\n",
      "iteration 900 / 2000: training loss 0.519361 val loss: 0.543411\n",
      "iteration 950 / 2000: training loss 0.502460 val loss: 0.558378\n",
      "iteration 1000 / 2000: training loss 0.499109 val loss: 0.545524\n",
      "iteration 1050 / 2000: training loss 0.520825 val loss: 0.575852\n",
      "hidden_size: 256 lr: 5.000000e-01 lr_decay: 9.000000e-01 reg: 1.000000e-02 train accuracy: 0.940000 val accuracy: 0.927000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302721\n",
      "iteration 50 / 2000: training loss 1.122351 val loss: 1.069581\n",
      "iteration 100 / 2000: training loss 0.730330 val loss: 0.732752\n",
      "iteration 150 / 2000: training loss 0.792354 val loss: 0.715022\n",
      "iteration 200 / 2000: training loss 0.606169 val loss: 0.642891\n",
      "iteration 250 / 2000: training loss 0.650576 val loss: 0.628794\n",
      "iteration 300 / 2000: training loss 0.649396 val loss: 0.635084\n",
      "iteration 350 / 2000: training loss 0.576241 val loss: 0.636585\n",
      "iteration 400 / 2000: training loss 0.578976 val loss: 0.561747\n",
      "iteration 450 / 2000: training loss 0.618095 val loss: 0.595731\n",
      "iteration 500 / 2000: training loss 0.549114 val loss: 0.566659\n",
      "iteration 550 / 2000: training loss 0.528474 val loss: 0.535191\n",
      "iteration 600 / 2000: training loss 0.444730 val loss: 0.546354\n",
      "iteration 650 / 2000: training loss 0.456171 val loss: 0.549921\n",
      "iteration 700 / 2000: training loss 0.535711 val loss: 0.515267\n",
      "iteration 750 / 2000: training loss 0.496269 val loss: 0.528590\n",
      "iteration 800 / 2000: training loss 0.536513 val loss: 0.541202\n",
      "iteration 850 / 2000: training loss 0.500819 val loss: 0.541324\n",
      "iteration 900 / 2000: training loss 0.482706 val loss: 0.531934\n",
      "iteration 950 / 2000: training loss 0.495249 val loss: 0.518438\n",
      "iteration 1000 / 2000: training loss 0.484241 val loss: 0.513333\n",
      "iteration 1050 / 2000: training loss 0.473421 val loss: 0.520054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1100 / 2000: training loss 0.522723 val loss: 0.516664\n",
      "iteration 1150 / 2000: training loss 0.443841 val loss: 0.513891\n",
      "iteration 1200 / 2000: training loss 0.520001 val loss: 0.519293\n",
      "iteration 1250 / 2000: training loss 0.546283 val loss: 0.514327\n",
      "iteration 1300 / 2000: training loss 0.489655 val loss: 0.512226\n",
      "iteration 1350 / 2000: training loss 0.555462 val loss: 0.519263\n",
      "iteration 1400 / 2000: training loss 0.444331 val loss: 0.501792\n",
      "iteration 1450 / 2000: training loss 0.523285 val loss: 0.507367\n",
      "iteration 1500 / 2000: training loss 0.480249 val loss: 0.501069\n",
      "iteration 1550 / 2000: training loss 0.554847 val loss: 0.502821\n",
      "iteration 1600 / 2000: training loss 0.499454 val loss: 0.497191\n",
      "iteration 1650 / 2000: training loss 0.505281 val loss: 0.495271\n",
      "iteration 1700 / 2000: training loss 0.452708 val loss: 0.494302\n",
      "iteration 1750 / 2000: training loss 0.419523 val loss: 0.495045\n",
      "iteration 1800 / 2000: training loss 0.518595 val loss: 0.496766\n",
      "hidden_size: 256 lr: 5.000000e-01 lr_decay: 8.500000e-01 reg: 1.000000e-02 train accuracy: 0.960000 val accuracy: 0.942000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302721\n",
      "iteration 50 / 2000: training loss 0.949638 val loss: 1.018175\n",
      "iteration 100 / 2000: training loss 0.693012 val loss: 0.698975\n",
      "iteration 150 / 2000: training loss 0.824407 val loss: 0.693119\n",
      "iteration 200 / 2000: training loss 0.604036 val loss: 0.646130\n",
      "iteration 250 / 2000: training loss 0.611040 val loss: 0.655349\n",
      "iteration 300 / 2000: training loss 0.545316 val loss: 0.608554\n",
      "iteration 350 / 2000: training loss 0.560497 val loss: 0.575946\n",
      "iteration 400 / 2000: training loss 0.517265 val loss: 0.524988\n",
      "iteration 450 / 2000: training loss 0.587181 val loss: 0.592238\n",
      "hidden_size: 256 lr: 5.000000e-01 lr_decay: 8.000000e-01 reg: 1.000000e-02 train accuracy: 0.930000 val accuracy: 0.920000\n",
      "iteration 0 / 2000: training loss 2.302592 val loss: 2.302724\n",
      "iteration 50 / 2000: training loss 1.487814 val loss: 1.312620\n",
      "iteration 100 / 2000: training loss 0.912677 val loss: 0.965302\n",
      "iteration 150 / 2000: training loss 1.138350 val loss: 1.064555\n",
      "iteration 200 / 2000: training loss 1.011545 val loss: 1.132823\n",
      "iteration 250 / 2000: training loss 0.999887 val loss: 1.262571\n",
      "iteration 300 / 2000: training loss 0.950797 val loss: 1.114309\n",
      "iteration 350 / 2000: training loss 0.943492 val loss: 0.934947\n",
      "iteration 400 / 2000: training loss 1.154760 val loss: 0.961808\n",
      "iteration 450 / 2000: training loss 0.989178 val loss: 0.929915\n",
      "iteration 500 / 2000: training loss 1.268334 val loss: 1.203663\n",
      "iteration 550 / 2000: training loss 0.942381 val loss: 1.092721\n",
      "iteration 600 / 2000: training loss 0.914617 val loss: 1.045685\n",
      "iteration 650 / 2000: training loss 0.838767 val loss: 0.968439\n",
      "iteration 700 / 2000: training loss 1.149814 val loss: 1.027575\n",
      "iteration 750 / 2000: training loss 1.017246 val loss: 0.928533\n",
      "iteration 800 / 2000: training loss 0.910634 val loss: 0.924362\n",
      "iteration 850 / 2000: training loss 1.042383 val loss: 0.932375\n",
      "iteration 900 / 2000: training loss 0.954846 val loss: 1.111788\n",
      "iteration 950 / 2000: training loss 0.842362 val loss: 0.980577\n",
      "iteration 1000 / 2000: training loss 0.931701 val loss: 0.933865\n",
      "iteration 1050 / 2000: training loss 0.917293 val loss: 1.007701\n",
      "iteration 1100 / 2000: training loss 0.985921 val loss: 0.938735\n",
      "iteration 1150 / 2000: training loss 0.973279 val loss: 0.952335\n",
      "iteration 1200 / 2000: training loss 0.861082 val loss: 0.934733\n",
      "iteration 1250 / 2000: training loss 0.994386 val loss: 0.978430\n",
      "iteration 1300 / 2000: training loss 0.879783 val loss: 1.013277\n",
      "iteration 1350 / 2000: training loss 0.996661 val loss: 0.962535\n",
      "iteration 1400 / 2000: training loss 0.856471 val loss: 0.946505\n",
      "iteration 1450 / 2000: training loss 0.960173 val loss: 0.942586\n",
      "iteration 1500 / 2000: training loss 0.873861 val loss: 0.975905\n",
      "iteration 1550 / 2000: training loss 1.041482 val loss: 0.861814\n",
      "iteration 1600 / 2000: training loss 0.938205 val loss: 0.809454\n",
      "iteration 1650 / 2000: training loss 1.006814 val loss: 0.980836\n",
      "iteration 1700 / 2000: training loss 0.749843 val loss: 0.854909\n",
      "iteration 1750 / 2000: training loss 0.980691 val loss: 0.861650\n",
      "iteration 1800 / 2000: training loss 0.870695 val loss: 0.811076\n",
      "iteration 1850 / 2000: training loss 0.861043 val loss: 0.884029\n",
      "iteration 1900 / 2000: training loss 0.708671 val loss: 0.896775\n",
      "iteration 1950 / 2000: training loss 0.863067 val loss: 0.824864\n",
      "hidden_size: 256 lr: 5.000000e-01 lr_decay: 9.800000e-01 reg: 2.000000e-02 train accuracy: 0.930000 val accuracy: 0.879000\n",
      "iteration 0 / 2000: training loss 2.302592 val loss: 2.302724\n",
      "iteration 50 / 2000: training loss 1.251944 val loss: 1.154641\n",
      "iteration 100 / 2000: training loss 0.997836 val loss: 1.069273\n",
      "iteration 150 / 2000: training loss 1.240755 val loss: 1.180259\n",
      "iteration 200 / 2000: training loss 0.949508 val loss: 0.938729\n",
      "iteration 250 / 2000: training loss 0.918055 val loss: 1.071801\n",
      "iteration 300 / 2000: training loss 0.797057 val loss: 0.986592\n",
      "iteration 350 / 2000: training loss 0.917741 val loss: 0.901900\n",
      "iteration 400 / 2000: training loss 0.871268 val loss: 0.860355\n",
      "iteration 450 / 2000: training loss 0.879716 val loss: 0.885486\n",
      "iteration 500 / 2000: training loss 0.787215 val loss: 0.857305\n",
      "iteration 550 / 2000: training loss 0.821470 val loss: 0.826860\n",
      "iteration 600 / 2000: training loss 0.787578 val loss: 0.877016\n",
      "iteration 650 / 2000: training loss 0.681013 val loss: 0.782565\n",
      "iteration 700 / 2000: training loss 0.886372 val loss: 0.848328\n",
      "iteration 750 / 2000: training loss 0.767284 val loss: 0.774621\n",
      "iteration 800 / 2000: training loss 0.777299 val loss: 0.789530\n",
      "iteration 850 / 2000: training loss 0.834868 val loss: 0.768058\n",
      "iteration 900 / 2000: training loss 0.785881 val loss: 0.769408\n",
      "iteration 950 / 2000: training loss 0.749665 val loss: 0.783638\n",
      "iteration 1000 / 2000: training loss 0.734379 val loss: 0.761415\n",
      "iteration 1050 / 2000: training loss 0.712674 val loss: 0.759113\n",
      "iteration 1100 / 2000: training loss 0.744740 val loss: 0.761010\n",
      "iteration 1150 / 2000: training loss 0.693967 val loss: 0.759276\n",
      "iteration 1200 / 2000: training loss 0.752120 val loss: 0.746069\n",
      "iteration 1250 / 2000: training loss 0.796418 val loss: 0.755727\n",
      "iteration 1300 / 2000: training loss 0.766273 val loss: 0.754946\n",
      "iteration 1350 / 2000: training loss 0.784823 val loss: 0.743409\n",
      "iteration 1400 / 2000: training loss 0.687900 val loss: 0.736785\n",
      "iteration 1450 / 2000: training loss 0.778196 val loss: 0.751505\n",
      "iteration 1500 / 2000: training loss 0.725915 val loss: 0.727249\n",
      "iteration 1550 / 2000: training loss 0.760530 val loss: 0.735007\n",
      "iteration 1600 / 2000: training loss 0.746655 val loss: 0.737797\n",
      "iteration 1650 / 2000: training loss 0.722654 val loss: 0.724340\n",
      "iteration 1700 / 2000: training loss 0.671836 val loss: 0.724640\n",
      "hidden_size: 256 lr: 5.000000e-01 lr_decay: 9.000000e-01 reg: 2.000000e-02 train accuracy: 0.920000 val accuracy: 0.922000\n",
      "iteration 0 / 2000: training loss 2.302592 val loss: 2.302724\n",
      "iteration 50 / 2000: training loss 1.152855 val loss: 1.170595\n",
      "iteration 100 / 2000: training loss 0.875482 val loss: 0.970205\n",
      "iteration 150 / 2000: training loss 1.117215 val loss: 1.050164\n",
      "iteration 200 / 2000: training loss 0.833543 val loss: 0.850975\n",
      "iteration 250 / 2000: training loss 0.844560 val loss: 0.938429\n",
      "iteration 300 / 2000: training loss 0.785758 val loss: 0.883462\n",
      "iteration 350 / 2000: training loss 0.896621 val loss: 0.857304\n",
      "iteration 400 / 2000: training loss 0.801419 val loss: 0.790626\n",
      "iteration 450 / 2000: training loss 0.838074 val loss: 0.797327\n",
      "iteration 500 / 2000: training loss 0.754785 val loss: 0.779864\n",
      "iteration 550 / 2000: training loss 0.731843 val loss: 0.797793\n",
      "iteration 600 / 2000: training loss 0.682039 val loss: 0.816318\n",
      "iteration 650 / 2000: training loss 0.676030 val loss: 0.752447\n",
      "iteration 700 / 2000: training loss 0.753508 val loss: 0.753844\n",
      "iteration 750 / 2000: training loss 0.758899 val loss: 0.735943\n",
      "iteration 800 / 2000: training loss 0.786810 val loss: 0.744149\n",
      "iteration 850 / 2000: training loss 0.747302 val loss: 0.773436\n",
      "iteration 900 / 2000: training loss 0.740629 val loss: 0.744448\n",
      "iteration 950 / 2000: training loss 0.708966 val loss: 0.737783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1000 / 2000: training loss 0.700082 val loss: 0.731678\n",
      "iteration 1050 / 2000: training loss 0.700726 val loss: 0.743711\n",
      "hidden_size: 256 lr: 5.000000e-01 lr_decay: 8.500000e-01 reg: 2.000000e-02 train accuracy: 0.900000 val accuracy: 0.924000\n",
      "iteration 0 / 2000: training loss 2.302592 val loss: 2.302724\n",
      "iteration 50 / 2000: training loss 1.159437 val loss: 1.165792\n",
      "iteration 100 / 2000: training loss 0.923829 val loss: 0.961831\n",
      "iteration 150 / 2000: training loss 1.011461 val loss: 0.956000\n",
      "iteration 200 / 2000: training loss 0.789613 val loss: 0.824926\n",
      "iteration 250 / 2000: training loss 0.853741 val loss: 0.834698\n",
      "iteration 300 / 2000: training loss 0.769180 val loss: 0.808183\n",
      "iteration 350 / 2000: training loss 0.792721 val loss: 0.877686\n",
      "iteration 400 / 2000: training loss 0.788464 val loss: 0.760053\n",
      "iteration 450 / 2000: training loss 0.835227 val loss: 0.791928\n",
      "iteration 500 / 2000: training loss 0.770401 val loss: 0.766780\n",
      "iteration 550 / 2000: training loss 0.734147 val loss: 0.747258\n",
      "iteration 600 / 2000: training loss 0.660691 val loss: 0.744538\n",
      "iteration 650 / 2000: training loss 0.643700 val loss: 0.746194\n",
      "iteration 700 / 2000: training loss 0.752638 val loss: 0.729102\n",
      "iteration 750 / 2000: training loss 0.722769 val loss: 0.732125\n",
      "iteration 800 / 2000: training loss 0.717491 val loss: 0.752940\n",
      "iteration 850 / 2000: training loss 0.729388 val loss: 0.724599\n",
      "iteration 900 / 2000: training loss 0.694442 val loss: 0.728151\n",
      "iteration 950 / 2000: training loss 0.675266 val loss: 0.717116\n",
      "iteration 1000 / 2000: training loss 0.692599 val loss: 0.715280\n",
      "iteration 1050 / 2000: training loss 0.697159 val loss: 0.714202\n",
      "iteration 1100 / 2000: training loss 0.710326 val loss: 0.719843\n",
      "iteration 1150 / 2000: training loss 0.664092 val loss: 0.716488\n",
      "iteration 1200 / 2000: training loss 0.743211 val loss: 0.716978\n",
      "iteration 1250 / 2000: training loss 0.756669 val loss: 0.720602\n",
      "iteration 1300 / 2000: training loss 0.705527 val loss: 0.714525\n",
      "iteration 1350 / 2000: training loss 0.749901 val loss: 0.717004\n",
      "iteration 1400 / 2000: training loss 0.670596 val loss: 0.712892\n",
      "iteration 1450 / 2000: training loss 0.736691 val loss: 0.718764\n",
      "iteration 1500 / 2000: training loss 0.685864 val loss: 0.710116\n",
      "iteration 1550 / 2000: training loss 0.737391 val loss: 0.711935\n",
      "iteration 1600 / 2000: training loss 0.728462 val loss: 0.708200\n",
      "iteration 1650 / 2000: training loss 0.707026 val loss: 0.706747\n",
      "iteration 1700 / 2000: training loss 0.666265 val loss: 0.708777\n",
      "iteration 1750 / 2000: training loss 0.631585 val loss: 0.708059\n",
      "iteration 1800 / 2000: training loss 0.737828 val loss: 0.706459\n",
      "iteration 1850 / 2000: training loss 0.703371 val loss: 0.707184\n",
      "iteration 1900 / 2000: training loss 0.620793 val loss: 0.706877\n",
      "iteration 1950 / 2000: training loss 0.699205 val loss: 0.707016\n",
      "hidden_size: 256 lr: 5.000000e-01 lr_decay: 8.000000e-01 reg: 2.000000e-02 train accuracy: 0.930000 val accuracy: 0.936000\n",
      "iteration 0 / 2000: training loss 2.302599 val loss: 2.302731\n",
      "iteration 50 / 2000: training loss 2.158263 val loss: 1.997259\n",
      "iteration 100 / 2000: training loss 1.474313 val loss: 1.454852\n",
      "iteration 150 / 2000: training loss 1.663186 val loss: 1.557844\n",
      "iteration 200 / 2000: training loss 1.370707 val loss: 1.393787\n",
      "iteration 250 / 2000: training loss 1.266501 val loss: 1.551827\n",
      "iteration 300 / 2000: training loss 1.626463 val loss: 1.587651\n",
      "iteration 350 / 2000: training loss 1.235457 val loss: 1.378448\n",
      "iteration 400 / 2000: training loss 1.385976 val loss: 1.301858\n",
      "iteration 450 / 2000: training loss 1.394550 val loss: 1.296367\n",
      "iteration 500 / 2000: training loss 1.479653 val loss: 1.339133\n",
      "iteration 550 / 2000: training loss 1.522675 val loss: 1.409929\n",
      "iteration 600 / 2000: training loss 1.326952 val loss: 1.462504\n",
      "hidden_size: 256 lr: 5.000000e-01 lr_decay: 9.800000e-01 reg: 4.000000e-02 train accuracy: 0.750000 val accuracy: 0.702000\n",
      "iteration 0 / 2000: training loss 2.302599 val loss: 2.302731\n",
      "iteration 50 / 2000: training loss 1.477421 val loss: 1.453725\n",
      "iteration 100 / 2000: training loss 1.273097 val loss: 1.267804\n",
      "iteration 150 / 2000: training loss 1.291299 val loss: 1.319800\n",
      "iteration 200 / 2000: training loss 1.180498 val loss: 1.234352\n",
      "iteration 250 / 2000: training loss 1.237194 val loss: 1.430644\n",
      "iteration 300 / 2000: training loss 1.350667 val loss: 1.436762\n",
      "iteration 350 / 2000: training loss 1.404021 val loss: 1.189025\n",
      "iteration 400 / 2000: training loss 1.301882 val loss: 1.156901\n",
      "iteration 450 / 2000: training loss 1.184544 val loss: 1.149230\n",
      "iteration 500 / 2000: training loss 1.095736 val loss: 1.139527\n",
      "iteration 550 / 2000: training loss 1.120125 val loss: 1.110518\n",
      "iteration 600 / 2000: training loss 1.128312 val loss: 1.155111\n",
      "iteration 650 / 2000: training loss 1.050647 val loss: 1.124445\n",
      "iteration 700 / 2000: training loss 1.159302 val loss: 1.151918\n",
      "iteration 750 / 2000: training loss 1.100264 val loss: 1.086019\n",
      "iteration 800 / 2000: training loss 1.076333 val loss: 1.099331\n",
      "iteration 850 / 2000: training loss 1.133035 val loss: 1.083016\n",
      "iteration 900 / 2000: training loss 1.130896 val loss: 1.088126\n",
      "iteration 950 / 2000: training loss 1.005808 val loss: 1.081330\n",
      "iteration 1000 / 2000: training loss 1.103395 val loss: 1.093634\n",
      "iteration 1050 / 2000: training loss 1.036800 val loss: 1.081285\n",
      "iteration 1100 / 2000: training loss 1.049346 val loss: 1.076475\n",
      "iteration 1150 / 2000: training loss 1.009790 val loss: 1.109007\n",
      "iteration 1200 / 2000: training loss 1.089465 val loss: 1.058933\n",
      "iteration 1250 / 2000: training loss 1.127641 val loss: 1.104999\n",
      "iteration 1300 / 2000: training loss 1.094311 val loss: 1.072032\n",
      "iteration 1350 / 2000: training loss 1.108741 val loss: 1.078147\n",
      "iteration 1400 / 2000: training loss 1.052401 val loss: 1.049842\n",
      "iteration 1450 / 2000: training loss 1.105939 val loss: 1.082245\n",
      "iteration 1500 / 2000: training loss 1.062155 val loss: 1.039954\n",
      "iteration 1550 / 2000: training loss 1.083420 val loss: 1.057783\n",
      "iteration 1600 / 2000: training loss 1.068760 val loss: 1.055037\n",
      "iteration 1650 / 2000: training loss 1.023141 val loss: 1.039464\n",
      "iteration 1700 / 2000: training loss 0.986258 val loss: 1.051138\n",
      "iteration 1750 / 2000: training loss 0.939143 val loss: 1.036308\n",
      "iteration 1800 / 2000: training loss 1.102315 val loss: 1.056307\n",
      "iteration 1850 / 2000: training loss 1.039931 val loss: 1.043184\n",
      "iteration 1900 / 2000: training loss 0.952643 val loss: 1.030327\n",
      "iteration 1950 / 2000: training loss 1.017730 val loss: 1.035505\n",
      "hidden_size: 256 lr: 5.000000e-01 lr_decay: 9.000000e-01 reg: 4.000000e-02 train accuracy: 0.910000 val accuracy: 0.906000\n",
      "iteration 0 / 2000: training loss 2.302599 val loss: 2.302731\n",
      "iteration 50 / 2000: training loss 1.392560 val loss: 1.336901\n",
      "iteration 100 / 2000: training loss 1.230562 val loss: 1.326146\n",
      "iteration 150 / 2000: training loss 1.390732 val loss: 1.256715\n",
      "iteration 200 / 2000: training loss 1.197779 val loss: 1.177498\n",
      "iteration 250 / 2000: training loss 1.292274 val loss: 1.202525\n",
      "iteration 300 / 2000: training loss 1.075591 val loss: 1.241968\n",
      "iteration 350 / 2000: training loss 1.248577 val loss: 1.221542\n",
      "iteration 400 / 2000: training loss 1.175457 val loss: 1.122787\n",
      "iteration 450 / 2000: training loss 1.180684 val loss: 1.142063\n",
      "iteration 500 / 2000: training loss 1.094057 val loss: 1.118647\n",
      "iteration 550 / 2000: training loss 1.053759 val loss: 1.117706\n",
      "iteration 600 / 2000: training loss 1.064198 val loss: 1.140520\n",
      "iteration 650 / 2000: training loss 0.980454 val loss: 1.055738\n",
      "iteration 700 / 2000: training loss 1.080480 val loss: 1.106413\n",
      "iteration 750 / 2000: training loss 1.103455 val loss: 1.064597\n",
      "iteration 800 / 2000: training loss 1.078635 val loss: 1.061506\n",
      "hidden_size: 256 lr: 5.000000e-01 lr_decay: 8.500000e-01 reg: 4.000000e-02 train accuracy: 0.880000 val accuracy: 0.884000\n",
      "iteration 0 / 2000: training loss 2.302599 val loss: 2.302731\n",
      "iteration 50 / 2000: training loss 1.276500 val loss: 1.282369\n",
      "iteration 100 / 2000: training loss 1.206730 val loss: 1.314436\n",
      "iteration 150 / 2000: training loss 1.390936 val loss: 1.344348\n",
      "iteration 200 / 2000: training loss 1.140708 val loss: 1.138066\n",
      "iteration 250 / 2000: training loss 1.200060 val loss: 1.173560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 300 / 2000: training loss 1.052548 val loss: 1.169124\n",
      "iteration 350 / 2000: training loss 1.166601 val loss: 1.135219\n",
      "iteration 400 / 2000: training loss 1.128926 val loss: 1.114882\n",
      "iteration 450 / 2000: training loss 1.159594 val loss: 1.105109\n",
      "iteration 500 / 2000: training loss 1.041049 val loss: 1.070222\n",
      "hidden_size: 256 lr: 5.000000e-01 lr_decay: 8.000000e-01 reg: 4.000000e-02 train accuracy: 0.900000 val accuracy: 0.844000\n",
      "iteration 0 / 2000: training loss 2.302606 val loss: 2.302737\n",
      "iteration 50 / 2000: training loss 2.244257 val loss: 1.922162\n",
      "iteration 100 / 2000: training loss 1.732394 val loss: 1.575827\n",
      "iteration 150 / 2000: training loss 1.703490 val loss: 1.645384\n",
      "iteration 200 / 2000: training loss 1.796349 val loss: 1.953413\n",
      "iteration 250 / 2000: training loss 1.548078 val loss: 1.750476\n",
      "iteration 300 / 2000: training loss 1.679691 val loss: 1.633523\n",
      "iteration 350 / 2000: training loss 1.620324 val loss: 1.555310\n",
      "hidden_size: 256 lr: 5.000000e-01 lr_decay: 9.800000e-01 reg: 6.000000e-02 train accuracy: 0.830000 val accuracy: 0.720000\n",
      "iteration 0 / 2000: training loss 2.302606 val loss: 2.302737\n",
      "iteration 50 / 2000: training loss 1.789195 val loss: 1.574610\n",
      "iteration 100 / 2000: training loss 1.559997 val loss: 1.516612\n",
      "iteration 150 / 2000: training loss 1.563989 val loss: 1.550501\n",
      "iteration 200 / 2000: training loss 1.485526 val loss: 1.544772\n",
      "iteration 250 / 2000: training loss 1.512611 val loss: 1.585682\n",
      "iteration 300 / 2000: training loss 1.457604 val loss: 1.538486\n",
      "iteration 350 / 2000: training loss 1.516473 val loss: 1.384031\n",
      "iteration 400 / 2000: training loss 1.474406 val loss: 1.432739\n",
      "iteration 450 / 2000: training loss 1.479138 val loss: 1.443955\n",
      "iteration 500 / 2000: training loss 1.322172 val loss: 1.385125\n",
      "iteration 550 / 2000: training loss 1.349829 val loss: 1.377651\n",
      "iteration 600 / 2000: training loss 1.366642 val loss: 1.415437\n",
      "iteration 650 / 2000: training loss 1.346909 val loss: 1.377396\n",
      "iteration 700 / 2000: training loss 1.420657 val loss: 1.439006\n",
      "iteration 750 / 2000: training loss 1.337808 val loss: 1.339143\n",
      "iteration 800 / 2000: training loss 1.315553 val loss: 1.336239\n",
      "hidden_size: 256 lr: 5.000000e-01 lr_decay: 9.000000e-01 reg: 6.000000e-02 train accuracy: 0.850000 val accuracy: 0.824000\n",
      "iteration 0 / 2000: training loss 2.302606 val loss: 2.302737\n",
      "iteration 50 / 2000: training loss 1.617487 val loss: 1.521307\n",
      "iteration 100 / 2000: training loss 1.481016 val loss: 1.514094\n",
      "iteration 150 / 2000: training loss 1.626910 val loss: 1.478656\n",
      "iteration 200 / 2000: training loss 1.417951 val loss: 1.428762\n",
      "iteration 250 / 2000: training loss 1.561600 val loss: 1.423823\n",
      "iteration 300 / 2000: training loss 1.372425 val loss: 1.544600\n",
      "iteration 350 / 2000: training loss 1.617695 val loss: 1.483694\n",
      "iteration 400 / 2000: training loss 1.440828 val loss: 1.392645\n",
      "iteration 450 / 2000: training loss 1.393250 val loss: 1.381245\n",
      "iteration 500 / 2000: training loss 1.293824 val loss: 1.342652\n",
      "iteration 550 / 2000: training loss 1.292885 val loss: 1.364816\n",
      "iteration 600 / 2000: training loss 1.335718 val loss: 1.373849\n",
      "iteration 650 / 2000: training loss 1.218508 val loss: 1.292813\n",
      "iteration 700 / 2000: training loss 1.332475 val loss: 1.373299\n",
      "iteration 750 / 2000: training loss 1.376050 val loss: 1.318085\n",
      "iteration 800 / 2000: training loss 1.308014 val loss: 1.306344\n",
      "iteration 850 / 2000: training loss 1.335403 val loss: 1.315484\n",
      "iteration 900 / 2000: training loss 1.321273 val loss: 1.315084\n",
      "iteration 950 / 2000: training loss 1.256557 val loss: 1.303587\n",
      "iteration 1000 / 2000: training loss 1.265677 val loss: 1.314629\n",
      "iteration 1050 / 2000: training loss 1.263878 val loss: 1.281533\n",
      "iteration 1100 / 2000: training loss 1.270934 val loss: 1.293680\n",
      "iteration 1150 / 2000: training loss 1.226181 val loss: 1.304989\n",
      "iteration 1200 / 2000: training loss 1.282643 val loss: 1.274084\n",
      "iteration 1250 / 2000: training loss 1.314542 val loss: 1.276841\n",
      "iteration 1300 / 2000: training loss 1.241535 val loss: 1.280593\n",
      "iteration 1350 / 2000: training loss 1.369560 val loss: 1.286831\n",
      "iteration 1400 / 2000: training loss 1.284539 val loss: 1.267974\n",
      "iteration 1450 / 2000: training loss 1.327904 val loss: 1.267405\n",
      "iteration 1500 / 2000: training loss 1.263697 val loss: 1.263333\n",
      "iteration 1550 / 2000: training loss 1.253376 val loss: 1.263844\n",
      "iteration 1600 / 2000: training loss 1.303830 val loss: 1.264966\n",
      "iteration 1650 / 2000: training loss 1.259363 val loss: 1.266445\n",
      "iteration 1700 / 2000: training loss 1.226271 val loss: 1.262399\n",
      "iteration 1750 / 2000: training loss 1.172702 val loss: 1.258709\n",
      "iteration 1800 / 2000: training loss 1.301775 val loss: 1.259784\n",
      "iteration 1850 / 2000: training loss 1.273947 val loss: 1.254847\n",
      "hidden_size: 256 lr: 5.000000e-01 lr_decay: 8.500000e-01 reg: 6.000000e-02 train accuracy: 0.900000 val accuracy: 0.893000\n",
      "iteration 0 / 2000: training loss 2.302606 val loss: 2.302737\n",
      "iteration 50 / 2000: training loss 1.614745 val loss: 1.515971\n",
      "iteration 100 / 2000: training loss 1.461516 val loss: 1.585119\n",
      "iteration 150 / 2000: training loss 1.630472 val loss: 1.553339\n",
      "iteration 200 / 2000: training loss 1.375983 val loss: 1.371009\n",
      "iteration 250 / 2000: training loss 1.476562 val loss: 1.422027\n",
      "iteration 300 / 2000: training loss 1.315538 val loss: 1.413280\n",
      "iteration 350 / 2000: training loss 1.446052 val loss: 1.448755\n",
      "iteration 400 / 2000: training loss 1.390232 val loss: 1.366558\n",
      "iteration 450 / 2000: training loss 1.420364 val loss: 1.359112\n",
      "iteration 500 / 2000: training loss 1.268165 val loss: 1.332792\n",
      "iteration 550 / 2000: training loss 1.249733 val loss: 1.325571\n",
      "iteration 600 / 2000: training loss 1.256508 val loss: 1.323724\n",
      "iteration 650 / 2000: training loss 1.229606 val loss: 1.300374\n",
      "iteration 700 / 2000: training loss 1.290197 val loss: 1.299614\n",
      "iteration 750 / 2000: training loss 1.287735 val loss: 1.271946\n",
      "iteration 800 / 2000: training loss 1.294472 val loss: 1.294285\n",
      "iteration 850 / 2000: training loss 1.314536 val loss: 1.275003\n",
      "iteration 900 / 2000: training loss 1.258497 val loss: 1.287418\n",
      "iteration 950 / 2000: training loss 1.246436 val loss: 1.281297\n",
      "iteration 1000 / 2000: training loss 1.264523 val loss: 1.270532\n",
      "iteration 1050 / 2000: training loss 1.259783 val loss: 1.267707\n",
      "iteration 1100 / 2000: training loss 1.252511 val loss: 1.266098\n",
      "iteration 1150 / 2000: training loss 1.214059 val loss: 1.268818\n",
      "iteration 1200 / 2000: training loss 1.313024 val loss: 1.271366\n",
      "iteration 1250 / 2000: training loss 1.312917 val loss: 1.263913\n",
      "iteration 1300 / 2000: training loss 1.262694 val loss: 1.261601\n",
      "iteration 1350 / 2000: training loss 1.339826 val loss: 1.273868\n",
      "hidden_size: 256 lr: 5.000000e-01 lr_decay: 8.000000e-01 reg: 6.000000e-02 train accuracy: 0.820000 val accuracy: 0.895000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302848\n",
      "iteration 50 / 2000: training loss 2.236946 val loss: 1.430062\n",
      "iteration 100 / 2000: training loss 1.017390 val loss: 0.975444\n",
      "iteration 150 / 2000: training loss 1.178753 val loss: 1.354263\n",
      "iteration 200 / 2000: training loss 1.010887 val loss: 0.908459\n",
      "iteration 250 / 2000: training loss 1.089139 val loss: 0.920678\n",
      "iteration 300 / 2000: training loss 0.983403 val loss: 1.509791\n",
      "iteration 350 / 2000: training loss 0.898183 val loss: 1.251042\n",
      "iteration 400 / 2000: training loss 1.053630 val loss: 1.110037\n",
      "iteration 450 / 2000: training loss 1.034961 val loss: 0.985307\n",
      "iteration 500 / 2000: training loss 0.994590 val loss: 1.053151\n",
      "iteration 550 / 2000: training loss 1.225688 val loss: 1.643619\n",
      "iteration 600 / 2000: training loss 0.794469 val loss: 0.944492\n",
      "iteration 650 / 2000: training loss 0.831265 val loss: 1.348890\n",
      "iteration 700 / 2000: training loss 1.375654 val loss: 0.973076\n",
      "iteration 750 / 2000: training loss 0.978877 val loss: 0.899169\n",
      "iteration 800 / 2000: training loss 1.060039 val loss: 0.937729\n",
      "iteration 850 / 2000: training loss 0.875597 val loss: 0.886138\n",
      "iteration 900 / 2000: training loss 0.956235 val loss: 1.018745\n",
      "hidden_size: 256 lr: 7.000000e-01 lr_decay: 9.800000e-01 reg: 1.000000e-02 train accuracy: 0.910000 val accuracy: 0.858000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302848\n",
      "iteration 50 / 2000: training loss 1.546711 val loss: 1.276531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 2000: training loss 0.989583 val loss: 1.098444\n",
      "iteration 150 / 2000: training loss 1.358001 val loss: 1.075183\n",
      "iteration 200 / 2000: training loss 0.868213 val loss: 0.876895\n",
      "iteration 250 / 2000: training loss 0.822410 val loss: 1.014015\n",
      "iteration 300 / 2000: training loss 0.808722 val loss: 0.924958\n",
      "iteration 350 / 2000: training loss 0.756029 val loss: 0.772111\n",
      "iteration 400 / 2000: training loss 0.623542 val loss: 0.692878\n",
      "iteration 450 / 2000: training loss 0.707190 val loss: 0.732404\n",
      "iteration 500 / 2000: training loss 0.597156 val loss: 0.661819\n",
      "iteration 550 / 2000: training loss 0.672641 val loss: 0.675400\n",
      "iteration 600 / 2000: training loss 0.597370 val loss: 0.683826\n",
      "iteration 650 / 2000: training loss 0.522461 val loss: 0.597149\n",
      "iteration 700 / 2000: training loss 0.721194 val loss: 0.659603\n",
      "iteration 750 / 2000: training loss 0.578961 val loss: 0.604847\n",
      "iteration 800 / 2000: training loss 0.602627 val loss: 0.611557\n",
      "iteration 850 / 2000: training loss 0.611041 val loss: 0.577945\n",
      "iteration 900 / 2000: training loss 0.561930 val loss: 0.575488\n",
      "iteration 950 / 2000: training loss 0.561314 val loss: 0.602074\n",
      "iteration 1000 / 2000: training loss 0.534290 val loss: 0.564113\n",
      "iteration 1050 / 2000: training loss 0.496584 val loss: 0.565252\n",
      "iteration 1100 / 2000: training loss 0.562524 val loss: 0.571168\n",
      "iteration 1150 / 2000: training loss 0.491080 val loss: 0.547937\n",
      "iteration 1200 / 2000: training loss 0.554894 val loss: 0.553445\n",
      "iteration 1250 / 2000: training loss 0.579889 val loss: 0.540018\n",
      "iteration 1300 / 2000: training loss 0.571688 val loss: 0.550898\n",
      "iteration 1350 / 2000: training loss 0.564171 val loss: 0.533880\n",
      "iteration 1400 / 2000: training loss 0.461402 val loss: 0.533864\n",
      "iteration 1450 / 2000: training loss 0.554730 val loss: 0.546564\n",
      "iteration 1500 / 2000: training loss 0.522514 val loss: 0.517837\n",
      "iteration 1550 / 2000: training loss 0.565909 val loss: 0.524925\n",
      "iteration 1600 / 2000: training loss 0.531266 val loss: 0.525721\n",
      "iteration 1650 / 2000: training loss 0.517265 val loss: 0.511443\n",
      "iteration 1700 / 2000: training loss 0.464439 val loss: 0.512552\n",
      "hidden_size: 256 lr: 7.000000e-01 lr_decay: 9.000000e-01 reg: 1.000000e-02 train accuracy: 0.920000 val accuracy: 0.932000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302848\n",
      "iteration 50 / 2000: training loss 1.670262 val loss: 1.526927\n",
      "iteration 100 / 2000: training loss 0.912132 val loss: 0.995536\n",
      "iteration 150 / 2000: training loss 1.228161 val loss: 0.915006\n",
      "iteration 200 / 2000: training loss 0.778507 val loss: 0.783653\n",
      "iteration 250 / 2000: training loss 0.866887 val loss: 0.795321\n",
      "iteration 300 / 2000: training loss 0.685091 val loss: 0.790415\n",
      "iteration 350 / 2000: training loss 0.620914 val loss: 0.708481\n",
      "iteration 400 / 2000: training loss 0.586593 val loss: 0.592908\n",
      "iteration 450 / 2000: training loss 0.597615 val loss: 0.631982\n",
      "iteration 500 / 2000: training loss 0.558170 val loss: 0.604097\n",
      "iteration 550 / 2000: training loss 0.572424 val loss: 0.625318\n",
      "iteration 600 / 2000: training loss 0.467193 val loss: 0.612347\n",
      "iteration 650 / 2000: training loss 0.463373 val loss: 0.551043\n",
      "iteration 700 / 2000: training loss 0.568344 val loss: 0.548278\n",
      "iteration 750 / 2000: training loss 0.539903 val loss: 0.536014\n",
      "iteration 800 / 2000: training loss 0.588426 val loss: 0.542847\n",
      "iteration 850 / 2000: training loss 0.533902 val loss: 0.568159\n",
      "iteration 900 / 2000: training loss 0.521669 val loss: 0.535625\n",
      "iteration 950 / 2000: training loss 0.501030 val loss: 0.528275\n",
      "iteration 1000 / 2000: training loss 0.488517 val loss: 0.518422\n",
      "iteration 1050 / 2000: training loss 0.491382 val loss: 0.544900\n",
      "hidden_size: 256 lr: 7.000000e-01 lr_decay: 8.500000e-01 reg: 1.000000e-02 train accuracy: 0.920000 val accuracy: 0.934000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.302848\n",
      "iteration 50 / 2000: training loss 1.353331 val loss: 1.452684\n",
      "iteration 100 / 2000: training loss 0.767783 val loss: 0.925161\n",
      "iteration 150 / 2000: training loss 0.992205 val loss: 0.799327\n",
      "iteration 200 / 2000: training loss 0.603167 val loss: 0.667967\n",
      "iteration 250 / 2000: training loss 0.758170 val loss: 0.689268\n",
      "iteration 300 / 2000: training loss 0.622801 val loss: 0.668348\n",
      "iteration 350 / 2000: training loss 0.572191 val loss: 0.631263\n",
      "iteration 400 / 2000: training loss 0.556136 val loss: 0.562169\n",
      "iteration 450 / 2000: training loss 0.617807 val loss: 0.586701\n",
      "iteration 500 / 2000: training loss 0.549001 val loss: 0.570075\n",
      "hidden_size: 256 lr: 7.000000e-01 lr_decay: 8.000000e-01 reg: 1.000000e-02 train accuracy: 0.960000 val accuracy: 0.894000\n",
      "iteration 0 / 2000: training loss 2.302592 val loss: 2.302851\n",
      "iteration 50 / 2000: training loss 2.244677 val loss: 2.239583\n",
      "iteration 100 / 2000: training loss 1.501168 val loss: 1.421407\n",
      "iteration 150 / 2000: training loss 1.956422 val loss: 1.784588\n",
      "iteration 200 / 2000: training loss 1.625417 val loss: 1.577580\n",
      "iteration 250 / 2000: training loss 1.412772 val loss: 1.709574\n",
      "iteration 300 / 2000: training loss 1.275307 val loss: 1.461772\n",
      "iteration 350 / 2000: training loss 1.206223 val loss: 1.660093\n",
      "iteration 400 / 2000: training loss 1.500315 val loss: 1.546860\n",
      "iteration 450 / 2000: training loss 1.246537 val loss: 1.156591\n",
      "iteration 500 / 2000: training loss 1.008471 val loss: 1.178644\n",
      "iteration 550 / 2000: training loss 1.180865 val loss: 1.233701\n",
      "iteration 600 / 2000: training loss 1.185917 val loss: 1.373303\n",
      "hidden_size: 256 lr: 7.000000e-01 lr_decay: 9.800000e-01 reg: 2.000000e-02 train accuracy: 0.790000 val accuracy: 0.792000\n",
      "iteration 0 / 2000: training loss 2.302592 val loss: 2.302851\n",
      "iteration 50 / 2000: training loss 2.092231 val loss: 2.177337\n",
      "iteration 100 / 2000: training loss 1.313062 val loss: 1.387796\n",
      "iteration 150 / 2000: training loss 1.310375 val loss: 1.114167\n",
      "iteration 200 / 2000: training loss 0.992714 val loss: 1.038400\n",
      "iteration 250 / 2000: training loss 1.103141 val loss: 1.106458\n",
      "iteration 300 / 2000: training loss 0.999975 val loss: 1.277067\n",
      "iteration 350 / 2000: training loss 1.001889 val loss: 0.861927\n",
      "iteration 400 / 2000: training loss 0.937628 val loss: 0.900034\n",
      "iteration 450 / 2000: training loss 0.908214 val loss: 0.854313\n",
      "iteration 500 / 2000: training loss 0.849968 val loss: 0.919733\n",
      "iteration 550 / 2000: training loss 0.847152 val loss: 0.875500\n",
      "iteration 600 / 2000: training loss 0.779797 val loss: 0.879606\n",
      "iteration 650 / 2000: training loss 0.784446 val loss: 0.856754\n",
      "iteration 700 / 2000: training loss 0.979058 val loss: 0.875740\n",
      "iteration 750 / 2000: training loss 0.805480 val loss: 0.797120\n",
      "iteration 800 / 2000: training loss 0.798829 val loss: 0.840501\n",
      "iteration 850 / 2000: training loss 0.840247 val loss: 0.817550\n",
      "iteration 900 / 2000: training loss 0.806095 val loss: 0.821197\n",
      "iteration 950 / 2000: training loss 0.715050 val loss: 0.807351\n",
      "iteration 1000 / 2000: training loss 0.819052 val loss: 0.823124\n",
      "iteration 1050 / 2000: training loss 0.753622 val loss: 0.800221\n",
      "iteration 1100 / 2000: training loss 0.771425 val loss: 0.777472\n",
      "iteration 1150 / 2000: training loss 0.712439 val loss: 0.783515\n",
      "iteration 1200 / 2000: training loss 0.780643 val loss: 0.768191\n",
      "iteration 1250 / 2000: training loss 0.821528 val loss: 0.788874\n",
      "iteration 1300 / 2000: training loss 0.803659 val loss: 0.773935\n",
      "iteration 1350 / 2000: training loss 0.799545 val loss: 0.774175\n",
      "iteration 1400 / 2000: training loss 0.725153 val loss: 0.756460\n",
      "iteration 1450 / 2000: training loss 0.797522 val loss: 0.782694\n",
      "iteration 1500 / 2000: training loss 0.743978 val loss: 0.731054\n",
      "iteration 1550 / 2000: training loss 0.787801 val loss: 0.759841\n",
      "iteration 1600 / 2000: training loss 0.748762 val loss: 0.764162\n",
      "iteration 1650 / 2000: training loss 0.721051 val loss: 0.727272\n",
      "iteration 1700 / 2000: training loss 0.678994 val loss: 0.748301\n",
      "iteration 1750 / 2000: training loss 0.636638 val loss: 0.732494\n",
      "iteration 1800 / 2000: training loss 0.779279 val loss: 0.749519\n",
      "iteration 1850 / 2000: training loss 0.722607 val loss: 0.739374\n",
      "iteration 1900 / 2000: training loss 0.631023 val loss: 0.722021\n",
      "iteration 1950 / 2000: training loss 0.709527 val loss: 0.735506\n",
      "hidden_size: 256 lr: 7.000000e-01 lr_decay: 9.000000e-01 reg: 2.000000e-02 train accuracy: 0.930000 val accuracy: 0.920000\n",
      "iteration 0 / 2000: training loss 2.302592 val loss: 2.302851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 50 / 2000: training loss 1.829830 val loss: 1.890719\n",
      "iteration 100 / 2000: training loss 1.192715 val loss: 1.313327\n",
      "iteration 150 / 2000: training loss 1.634319 val loss: 1.196788\n",
      "iteration 200 / 2000: training loss 1.017536 val loss: 0.970391\n",
      "iteration 250 / 2000: training loss 0.920206 val loss: 1.109936\n",
      "iteration 300 / 2000: training loss 0.946178 val loss: 1.090681\n",
      "iteration 350 / 2000: training loss 0.979627 val loss: 0.929557\n",
      "iteration 400 / 2000: training loss 0.836346 val loss: 0.818295\n",
      "iteration 450 / 2000: training loss 0.909843 val loss: 0.890101\n",
      "iteration 500 / 2000: training loss 0.800823 val loss: 0.865585\n",
      "iteration 550 / 2000: training loss 0.800964 val loss: 0.839803\n",
      "iteration 600 / 2000: training loss 0.753775 val loss: 0.863067\n",
      "iteration 650 / 2000: training loss 0.685571 val loss: 0.763014\n",
      "iteration 700 / 2000: training loss 0.818116 val loss: 0.830032\n",
      "iteration 750 / 2000: training loss 0.785416 val loss: 0.747517\n",
      "iteration 800 / 2000: training loss 0.777639 val loss: 0.774793\n",
      "iteration 850 / 2000: training loss 0.786476 val loss: 0.775279\n",
      "iteration 900 / 2000: training loss 0.752370 val loss: 0.752228\n",
      "iteration 950 / 2000: training loss 0.701678 val loss: 0.755347\n",
      "iteration 1000 / 2000: training loss 0.710766 val loss: 0.755080\n",
      "iteration 1050 / 2000: training loss 0.720207 val loss: 0.762577\n",
      "iteration 1100 / 2000: training loss 0.755920 val loss: 0.758096\n",
      "iteration 1150 / 2000: training loss 0.680889 val loss: 0.752534\n",
      "hidden_size: 256 lr: 7.000000e-01 lr_decay: 8.500000e-01 reg: 2.000000e-02 train accuracy: 0.920000 val accuracy: 0.916000\n",
      "iteration 0 / 2000: training loss 2.302592 val loss: 2.302851\n",
      "iteration 50 / 2000: training loss 1.364790 val loss: 1.229227\n",
      "iteration 100 / 2000: training loss 0.899528 val loss: 1.028068\n",
      "iteration 150 / 2000: training loss 1.310455 val loss: 1.120203\n",
      "iteration 200 / 2000: training loss 0.986165 val loss: 0.960644\n",
      "iteration 250 / 2000: training loss 0.952577 val loss: 0.977415\n",
      "iteration 300 / 2000: training loss 0.796797 val loss: 0.909763\n",
      "iteration 350 / 2000: training loss 0.916689 val loss: 0.886194\n",
      "iteration 400 / 2000: training loss 0.797078 val loss: 0.793432\n",
      "iteration 450 / 2000: training loss 0.831901 val loss: 0.797462\n",
      "iteration 500 / 2000: training loss 0.737740 val loss: 0.767704\n",
      "iteration 550 / 2000: training loss 0.722961 val loss: 0.775188\n",
      "iteration 600 / 2000: training loss 0.667845 val loss: 0.787298\n",
      "iteration 650 / 2000: training loss 0.666116 val loss: 0.754111\n",
      "iteration 700 / 2000: training loss 0.748526 val loss: 0.733632\n",
      "iteration 750 / 2000: training loss 0.729858 val loss: 0.731853\n",
      "iteration 800 / 2000: training loss 0.746429 val loss: 0.746087\n",
      "iteration 850 / 2000: training loss 0.726849 val loss: 0.741663\n",
      "iteration 900 / 2000: training loss 0.703890 val loss: 0.741183\n",
      "iteration 950 / 2000: training loss 0.695361 val loss: 0.730353\n",
      "iteration 1000 / 2000: training loss 0.698146 val loss: 0.722619\n",
      "iteration 1050 / 2000: training loss 0.695901 val loss: 0.722223\n",
      "iteration 1100 / 2000: training loss 0.719220 val loss: 0.722889\n",
      "iteration 1150 / 2000: training loss 0.665482 val loss: 0.721812\n",
      "iteration 1200 / 2000: training loss 0.749539 val loss: 0.724775\n",
      "iteration 1250 / 2000: training loss 0.765848 val loss: 0.721549\n",
      "iteration 1300 / 2000: training loss 0.705825 val loss: 0.714487\n",
      "iteration 1350 / 2000: training loss 0.762172 val loss: 0.722179\n",
      "iteration 1400 / 2000: training loss 0.675397 val loss: 0.712483\n",
      "iteration 1450 / 2000: training loss 0.745356 val loss: 0.717690\n",
      "iteration 1500 / 2000: training loss 0.687948 val loss: 0.711607\n",
      "iteration 1550 / 2000: training loss 0.738887 val loss: 0.712151\n",
      "iteration 1600 / 2000: training loss 0.723830 val loss: 0.707476\n",
      "iteration 1650 / 2000: training loss 0.710411 val loss: 0.706555\n",
      "iteration 1700 / 2000: training loss 0.665966 val loss: 0.708228\n",
      "iteration 1750 / 2000: training loss 0.636437 val loss: 0.708876\n",
      "iteration 1800 / 2000: training loss 0.735877 val loss: 0.706560\n",
      "iteration 1850 / 2000: training loss 0.705183 val loss: 0.706687\n",
      "iteration 1900 / 2000: training loss 0.620189 val loss: 0.706506\n",
      "iteration 1950 / 2000: training loss 0.699085 val loss: 0.706816\n",
      "hidden_size: 256 lr: 7.000000e-01 lr_decay: 8.000000e-01 reg: 2.000000e-02 train accuracy: 0.930000 val accuracy: 0.935000\n",
      "iteration 0 / 2000: training loss 2.302599 val loss: 2.302858\n",
      "iteration 50 / 2000: training loss 3.294153 val loss: 3.534273\n",
      "iteration 100 / 2000: training loss 2.109489 val loss: 1.951195\n",
      "iteration 150 / 2000: training loss 3.213651 val loss: 2.951520\n",
      "iteration 200 / 2000: training loss 2.312684 val loss: 2.420082\n",
      "iteration 250 / 2000: training loss 4.479367 val loss: 4.509779\n",
      "hidden_size: 256 lr: 7.000000e-01 lr_decay: 9.800000e-01 reg: 4.000000e-02 train accuracy: 0.570000 val accuracy: 0.509000\n",
      "iteration 0 / 2000: training loss 2.302599 val loss: 2.302858\n",
      "iteration 50 / 2000: training loss 2.627072 val loss: 2.280048\n",
      "iteration 100 / 2000: training loss 1.650492 val loss: 1.565294\n",
      "iteration 150 / 2000: training loss 1.664961 val loss: 1.533468\n",
      "iteration 200 / 2000: training loss 1.351002 val loss: 1.454090\n",
      "iteration 250 / 2000: training loss 1.497109 val loss: 1.685164\n",
      "iteration 300 / 2000: training loss 1.378360 val loss: 1.506021\n",
      "iteration 350 / 2000: training loss 1.390406 val loss: 1.388882\n",
      "hidden_size: 256 lr: 7.000000e-01 lr_decay: 9.000000e-01 reg: 4.000000e-02 train accuracy: 0.810000 val accuracy: 0.738000\n",
      "iteration 0 / 2000: training loss 2.302599 val loss: 2.302858\n",
      "iteration 50 / 2000: training loss 2.009463 val loss: 1.953850\n",
      "iteration 100 / 2000: training loss 1.483883 val loss: 1.488645\n",
      "iteration 150 / 2000: training loss 1.477512 val loss: 1.453343\n",
      "iteration 200 / 2000: training loss 1.289365 val loss: 1.353619\n",
      "iteration 250 / 2000: training loss 1.320917 val loss: 1.371807\n",
      "iteration 300 / 2000: training loss 1.184951 val loss: 1.270707\n",
      "iteration 350 / 2000: training loss 1.273650 val loss: 1.160726\n",
      "iteration 400 / 2000: training loss 1.234693 val loss: 1.182379\n",
      "iteration 450 / 2000: training loss 1.167345 val loss: 1.197045\n",
      "iteration 500 / 2000: training loss 1.093536 val loss: 1.122861\n",
      "iteration 550 / 2000: training loss 1.100039 val loss: 1.105731\n",
      "iteration 600 / 2000: training loss 1.093936 val loss: 1.133738\n",
      "iteration 650 / 2000: training loss 1.012990 val loss: 1.095904\n",
      "iteration 700 / 2000: training loss 1.154850 val loss: 1.126239\n",
      "iteration 750 / 2000: training loss 1.106945 val loss: 1.063064\n",
      "iteration 800 / 2000: training loss 1.066546 val loss: 1.085213\n",
      "iteration 850 / 2000: training loss 1.152753 val loss: 1.072446\n",
      "iteration 900 / 2000: training loss 1.121644 val loss: 1.079263\n",
      "iteration 950 / 2000: training loss 1.048392 val loss: 1.087855\n",
      "iteration 1000 / 2000: training loss 1.049643 val loss: 1.086500\n",
      "iteration 1050 / 2000: training loss 1.034059 val loss: 1.057696\n",
      "iteration 1100 / 2000: training loss 1.039524 val loss: 1.052666\n",
      "iteration 1150 / 2000: training loss 0.992252 val loss: 1.060800\n",
      "iteration 1200 / 2000: training loss 1.045473 val loss: 1.052641\n",
      "iteration 1250 / 2000: training loss 1.088047 val loss: 1.042618\n",
      "iteration 1300 / 2000: training loss 1.021469 val loss: 1.049397\n",
      "iteration 1350 / 2000: training loss 1.124877 val loss: 1.045735\n",
      "iteration 1400 / 2000: training loss 1.023293 val loss: 1.037897\n",
      "iteration 1450 / 2000: training loss 1.088980 val loss: 1.036401\n",
      "iteration 1500 / 2000: training loss 1.028284 val loss: 1.027636\n",
      "iteration 1550 / 2000: training loss 1.036415 val loss: 1.029374\n",
      "iteration 1600 / 2000: training loss 1.066343 val loss: 1.033056\n",
      "iteration 1650 / 2000: training loss 1.023933 val loss: 1.032654\n",
      "iteration 1700 / 2000: training loss 0.985808 val loss: 1.025118\n",
      "iteration 1750 / 2000: training loss 0.938295 val loss: 1.020564\n",
      "iteration 1800 / 2000: training loss 1.069264 val loss: 1.028502\n",
      "iteration 1850 / 2000: training loss 1.034698 val loss: 1.022556\n",
      "hidden_size: 256 lr: 7.000000e-01 lr_decay: 8.500000e-01 reg: 4.000000e-02 train accuracy: 0.910000 val accuracy: 0.904000\n",
      "iteration 0 / 2000: training loss 2.302599 val loss: 2.302858\n",
      "iteration 50 / 2000: training loss 1.848534 val loss: 1.857258\n",
      "iteration 100 / 2000: training loss 1.730351 val loss: 1.645593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 150 / 2000: training loss 1.356903 val loss: 1.327353\n",
      "iteration 200 / 2000: training loss 1.198968 val loss: 1.250673\n",
      "iteration 250 / 2000: training loss 1.243153 val loss: 1.300332\n",
      "iteration 300 / 2000: training loss 1.102475 val loss: 1.281869\n",
      "iteration 350 / 2000: training loss 1.306888 val loss: 1.221232\n",
      "iteration 400 / 2000: training loss 1.171115 val loss: 1.133279\n",
      "iteration 450 / 2000: training loss 1.177043 val loss: 1.164314\n",
      "iteration 500 / 2000: training loss 1.089039 val loss: 1.121562\n",
      "iteration 550 / 2000: training loss 1.038643 val loss: 1.108551\n",
      "iteration 600 / 2000: training loss 1.037533 val loss: 1.125004\n",
      "iteration 650 / 2000: training loss 0.985226 val loss: 1.059738\n",
      "iteration 700 / 2000: training loss 1.061877 val loss: 1.084517\n",
      "iteration 750 / 2000: training loss 1.062742 val loss: 1.046263\n",
      "iteration 800 / 2000: training loss 1.084963 val loss: 1.057465\n",
      "iteration 850 / 2000: training loss 1.060610 val loss: 1.065023\n",
      "iteration 900 / 2000: training loss 1.036081 val loss: 1.052942\n",
      "iteration 950 / 2000: training loss 1.017515 val loss: 1.047085\n",
      "iteration 1000 / 2000: training loss 1.021866 val loss: 1.041033\n",
      "iteration 1050 / 2000: training loss 1.020197 val loss: 1.042368\n",
      "hidden_size: 256 lr: 7.000000e-01 lr_decay: 8.000000e-01 reg: 4.000000e-02 train accuracy: 0.880000 val accuracy: 0.912000\n",
      "iteration 0 / 2000: training loss 2.302606 val loss: 2.302864\n",
      "iteration 50 / 2000: training loss 2.671434 val loss: 2.697697\n",
      "iteration 100 / 2000: training loss 2.909606 val loss: 3.082049\n",
      "iteration 150 / 2000: training loss 8.297478 val loss: 9.397490\n",
      "iteration 200 / 2000: training loss 2.404809 val loss: 2.278476\n",
      "iteration 250 / 2000: training loss 2.663932 val loss: 2.964280\n",
      "iteration 300 / 2000: training loss 2.991685 val loss: 3.073724\n",
      "iteration 350 / 2000: training loss 2.067555 val loss: 2.090685\n",
      "hidden_size: 256 lr: 7.000000e-01 lr_decay: 9.800000e-01 reg: 6.000000e-02 train accuracy: 0.560000 val accuracy: 0.478000\n",
      "iteration 0 / 2000: training loss 2.302606 val loss: 2.302864\n",
      "iteration 50 / 2000: training loss 2.494635 val loss: 2.585086\n",
      "iteration 100 / 2000: training loss 1.965232 val loss: 1.852872\n",
      "iteration 150 / 2000: training loss 1.879542 val loss: 2.073722\n",
      "iteration 200 / 2000: training loss 1.667597 val loss: 1.767799\n",
      "iteration 250 / 2000: training loss 1.812857 val loss: 2.003807\n",
      "iteration 300 / 2000: training loss 2.043662 val loss: 1.754249\n",
      "iteration 350 / 2000: training loss 1.681900 val loss: 1.658877\n",
      "iteration 400 / 2000: training loss 1.794200 val loss: 1.706169\n",
      "iteration 450 / 2000: training loss 1.521331 val loss: 1.544333\n",
      "iteration 500 / 2000: training loss 1.520151 val loss: 1.521969\n",
      "iteration 550 / 2000: training loss 1.495424 val loss: 1.592310\n",
      "iteration 600 / 2000: training loss 1.517909 val loss: 1.608181\n",
      "iteration 650 / 2000: training loss 1.420100 val loss: 1.489672\n",
      "iteration 700 / 2000: training loss 1.533801 val loss: 1.556938\n",
      "iteration 750 / 2000: training loss 1.468735 val loss: 1.413174\n",
      "iteration 800 / 2000: training loss 1.375411 val loss: 1.406007\n",
      "iteration 850 / 2000: training loss 1.435891 val loss: 1.397814\n",
      "iteration 900 / 2000: training loss 1.409886 val loss: 1.421974\n",
      "iteration 950 / 2000: training loss 1.292061 val loss: 1.445751\n",
      "iteration 1000 / 2000: training loss 1.349517 val loss: 1.483893\n",
      "iteration 1050 / 2000: training loss 1.355383 val loss: 1.367281\n",
      "iteration 1100 / 2000: training loss 1.287999 val loss: 1.355760\n",
      "iteration 1150 / 2000: training loss 1.288331 val loss: 1.392048\n",
      "iteration 1200 / 2000: training loss 1.380958 val loss: 1.336841\n",
      "iteration 1250 / 2000: training loss 1.351904 val loss: 1.348074\n",
      "iteration 1300 / 2000: training loss 1.433462 val loss: 1.366284\n",
      "iteration 1350 / 2000: training loss 1.344119 val loss: 1.325630\n",
      "iteration 1400 / 2000: training loss 1.343963 val loss: 1.331403\n",
      "iteration 1450 / 2000: training loss 1.420162 val loss: 1.356685\n",
      "iteration 1500 / 2000: training loss 1.294495 val loss: 1.292824\n",
      "iteration 1550 / 2000: training loss 1.298585 val loss: 1.288812\n",
      "iteration 1600 / 2000: training loss 1.383693 val loss: 1.315229\n",
      "iteration 1650 / 2000: training loss 1.273800 val loss: 1.302949\n",
      "iteration 1700 / 2000: training loss 1.247100 val loss: 1.293261\n",
      "hidden_size: 256 lr: 7.000000e-01 lr_decay: 9.000000e-01 reg: 6.000000e-02 train accuracy: 0.840000 val accuracy: 0.858000\n",
      "iteration 0 / 2000: training loss 2.302606 val loss: 2.302864\n",
      "iteration 50 / 2000: training loss 2.463196 val loss: 2.491565\n",
      "iteration 100 / 2000: training loss 2.485472 val loss: 2.363552\n",
      "iteration 150 / 2000: training loss 2.356061 val loss: 2.313382\n",
      "iteration 200 / 2000: training loss 1.558421 val loss: 1.585294\n",
      "iteration 250 / 2000: training loss 1.614803 val loss: 1.743772\n",
      "iteration 300 / 2000: training loss 1.581892 val loss: 1.646460\n",
      "iteration 350 / 2000: training loss 1.423934 val loss: 1.502560\n",
      "iteration 400 / 2000: training loss 1.497300 val loss: 1.486156\n",
      "iteration 450 / 2000: training loss 1.471270 val loss: 1.406008\n",
      "iteration 500 / 2000: training loss 1.329091 val loss: 1.390301\n",
      "iteration 550 / 2000: training loss 1.329281 val loss: 1.365928\n",
      "iteration 600 / 2000: training loss 1.386679 val loss: 1.404291\n",
      "iteration 650 / 2000: training loss 1.275603 val loss: 1.341147\n",
      "iteration 700 / 2000: training loss 1.427187 val loss: 1.426965\n",
      "iteration 750 / 2000: training loss 1.332812 val loss: 1.309624\n",
      "iteration 800 / 2000: training loss 1.310770 val loss: 1.317491\n",
      "hidden_size: 256 lr: 7.000000e-01 lr_decay: 8.500000e-01 reg: 6.000000e-02 train accuracy: 0.870000 val accuracy: 0.816000\n",
      "iteration 0 / 2000: training loss 2.302606 val loss: 2.302864\n",
      "iteration 50 / 2000: training loss 2.456016 val loss: 2.182464\n",
      "iteration 100 / 2000: training loss 1.738672 val loss: 1.639551\n",
      "iteration 150 / 2000: training loss 1.652962 val loss: 1.612350\n",
      "iteration 200 / 2000: training loss 1.454953 val loss: 1.542891\n",
      "iteration 250 / 2000: training loss 1.499955 val loss: 1.577133\n",
      "iteration 300 / 2000: training loss 1.358110 val loss: 1.606245\n",
      "iteration 350 / 2000: training loss 1.594694 val loss: 1.444514\n",
      "iteration 400 / 2000: training loss 1.467570 val loss: 1.385527\n",
      "iteration 450 / 2000: training loss 1.407330 val loss: 1.408908\n",
      "iteration 500 / 2000: training loss 1.294146 val loss: 1.343552\n",
      "iteration 550 / 2000: training loss 1.272551 val loss: 1.365582\n",
      "iteration 600 / 2000: training loss 1.318024 val loss: 1.363981\n",
      "iteration 650 / 2000: training loss 1.217758 val loss: 1.296650\n",
      "iteration 700 / 2000: training loss 1.295516 val loss: 1.331150\n",
      "iteration 750 / 2000: training loss 1.309804 val loss: 1.296192\n",
      "iteration 800 / 2000: training loss 1.318418 val loss: 1.293980\n",
      "iteration 850 / 2000: training loss 1.301099 val loss: 1.301582\n",
      "iteration 900 / 2000: training loss 1.281879 val loss: 1.290562\n",
      "iteration 950 / 2000: training loss 1.258518 val loss: 1.288098\n",
      "iteration 1000 / 2000: training loss 1.261160 val loss: 1.285308\n",
      "iteration 1050 / 2000: training loss 1.272407 val loss: 1.284574\n",
      "hidden_size: 256 lr: 7.000000e-01 lr_decay: 8.000000e-01 reg: 6.000000e-02 train accuracy: 0.840000 val accuracy: 0.891000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.303019\n",
      "iteration 50 / 2000: training loss 3.160935 val loss: 3.453800\n",
      "iteration 100 / 2000: training loss 1.573174 val loss: 1.821000\n",
      "iteration 150 / 2000: training loss 1.767296 val loss: 1.670731\n",
      "iteration 200 / 2000: training loss 1.918147 val loss: 1.771585\n",
      "iteration 250 / 2000: training loss 2.111962 val loss: 2.625973\n",
      "hidden_size: 256 lr: 9.000000e-01 lr_decay: 9.800000e-01 reg: 1.000000e-02 train accuracy: 0.600000 val accuracy: 0.597000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.303019\n",
      "iteration 50 / 2000: training loss 1.983075 val loss: 1.803714\n",
      "iteration 100 / 2000: training loss 1.271564 val loss: 1.487717\n",
      "iteration 150 / 2000: training loss 1.375116 val loss: 1.356213\n",
      "iteration 200 / 2000: training loss 1.073813 val loss: 1.063355\n",
      "iteration 250 / 2000: training loss 1.111206 val loss: 0.988199\n",
      "iteration 300 / 2000: training loss 0.914524 val loss: 1.076926\n",
      "iteration 350 / 2000: training loss 1.171419 val loss: 0.954586\n",
      "iteration 400 / 2000: training loss 0.762924 val loss: 0.756188\n",
      "iteration 450 / 2000: training loss 0.844304 val loss: 0.769698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 500 / 2000: training loss 0.750417 val loss: 0.738460\n",
      "iteration 550 / 2000: training loss 0.709050 val loss: 0.728431\n",
      "iteration 600 / 2000: training loss 0.618627 val loss: 0.686000\n",
      "iteration 650 / 2000: training loss 0.621532 val loss: 0.721848\n",
      "iteration 700 / 2000: training loss 0.834881 val loss: 0.711110\n",
      "iteration 750 / 2000: training loss 0.623024 val loss: 0.659722\n",
      "iteration 800 / 2000: training loss 0.678452 val loss: 0.643269\n",
      "iteration 850 / 2000: training loss 0.673533 val loss: 0.680444\n",
      "iteration 900 / 2000: training loss 0.611268 val loss: 0.668705\n",
      "iteration 950 / 2000: training loss 0.576040 val loss: 0.627552\n",
      "iteration 1000 / 2000: training loss 0.572860 val loss: 0.631143\n",
      "iteration 1050 / 2000: training loss 0.565469 val loss: 0.630415\n",
      "iteration 1100 / 2000: training loss 0.585838 val loss: 0.587496\n",
      "iteration 1150 / 2000: training loss 0.496931 val loss: 0.578716\n",
      "iteration 1200 / 2000: training loss 0.556629 val loss: 0.583141\n",
      "iteration 1250 / 2000: training loss 0.590642 val loss: 0.572434\n",
      "iteration 1300 / 2000: training loss 0.575593 val loss: 0.561284\n",
      "iteration 1350 / 2000: training loss 0.591671 val loss: 0.571302\n",
      "iteration 1400 / 2000: training loss 0.497694 val loss: 0.573506\n",
      "iteration 1450 / 2000: training loss 0.583628 val loss: 0.558977\n",
      "iteration 1500 / 2000: training loss 0.544320 val loss: 0.529892\n",
      "iteration 1550 / 2000: training loss 0.574482 val loss: 0.557442\n",
      "iteration 1600 / 2000: training loss 0.526982 val loss: 0.561451\n",
      "iteration 1650 / 2000: training loss 0.522284 val loss: 0.522213\n",
      "iteration 1700 / 2000: training loss 0.472960 val loss: 0.544022\n",
      "iteration 1750 / 2000: training loss 0.421502 val loss: 0.526862\n",
      "iteration 1800 / 2000: training loss 0.553054 val loss: 0.550010\n",
      "iteration 1850 / 2000: training loss 0.506038 val loss: 0.535675\n",
      "iteration 1900 / 2000: training loss 0.410663 val loss: 0.518565\n",
      "iteration 1950 / 2000: training loss 0.503068 val loss: 0.530605\n",
      "hidden_size: 256 lr: 9.000000e-01 lr_decay: 9.000000e-01 reg: 1.000000e-02 train accuracy: 0.950000 val accuracy: 0.933000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.303019\n",
      "iteration 50 / 2000: training loss 2.168062 val loss: 1.947609\n",
      "iteration 100 / 2000: training loss 1.065990 val loss: 1.278383\n",
      "iteration 150 / 2000: training loss 1.188908 val loss: 1.122500\n",
      "iteration 200 / 2000: training loss 1.054938 val loss: 0.954227\n",
      "iteration 250 / 2000: training loss 1.060382 val loss: 1.037914\n",
      "iteration 300 / 2000: training loss 0.681138 val loss: 0.812546\n",
      "iteration 350 / 2000: training loss 0.651776 val loss: 0.767073\n",
      "iteration 400 / 2000: training loss 0.680043 val loss: 0.699350\n",
      "iteration 450 / 2000: training loss 0.700517 val loss: 0.725031\n",
      "iteration 500 / 2000: training loss 0.652635 val loss: 0.710575\n",
      "iteration 550 / 2000: training loss 0.650441 val loss: 0.647547\n",
      "iteration 600 / 2000: training loss 0.528539 val loss: 0.649403\n",
      "iteration 650 / 2000: training loss 0.497565 val loss: 0.574258\n",
      "iteration 700 / 2000: training loss 0.635428 val loss: 0.614666\n",
      "iteration 750 / 2000: training loss 0.580567 val loss: 0.549930\n",
      "iteration 800 / 2000: training loss 0.604288 val loss: 0.587977\n",
      "iteration 850 / 2000: training loss 0.578459 val loss: 0.576580\n",
      "iteration 900 / 2000: training loss 0.535648 val loss: 0.563384\n",
      "iteration 950 / 2000: training loss 0.493962 val loss: 0.550275\n",
      "iteration 1000 / 2000: training loss 0.492881 val loss: 0.538190\n",
      "iteration 1050 / 2000: training loss 0.503807 val loss: 0.572272\n",
      "iteration 1100 / 2000: training loss 0.556267 val loss: 0.561339\n",
      "iteration 1150 / 2000: training loss 0.457778 val loss: 0.536169\n",
      "iteration 1200 / 2000: training loss 0.534947 val loss: 0.534400\n",
      "iteration 1250 / 2000: training loss 0.577007 val loss: 0.540591\n",
      "iteration 1300 / 2000: training loss 0.492613 val loss: 0.518789\n",
      "iteration 1350 / 2000: training loss 0.581066 val loss: 0.527520\n",
      "iteration 1400 / 2000: training loss 0.447967 val loss: 0.514383\n",
      "iteration 1450 / 2000: training loss 0.545969 val loss: 0.517437\n",
      "iteration 1500 / 2000: training loss 0.506595 val loss: 0.515760\n",
      "iteration 1550 / 2000: training loss 0.560103 val loss: 0.511074\n",
      "iteration 1600 / 2000: training loss 0.518203 val loss: 0.504811\n",
      "iteration 1650 / 2000: training loss 0.506768 val loss: 0.502541\n",
      "iteration 1700 / 2000: training loss 0.454238 val loss: 0.504623\n",
      "iteration 1750 / 2000: training loss 0.419212 val loss: 0.501779\n",
      "iteration 1800 / 2000: training loss 0.510233 val loss: 0.505425\n",
      "iteration 1850 / 2000: training loss 0.484711 val loss: 0.502374\n",
      "iteration 1900 / 2000: training loss 0.406453 val loss: 0.506737\n",
      "iteration 1950 / 2000: training loss 0.480291 val loss: 0.507224\n",
      "hidden_size: 256 lr: 9.000000e-01 lr_decay: 8.500000e-01 reg: 1.000000e-02 train accuracy: 0.960000 val accuracy: 0.947000\n",
      "iteration 0 / 2000: training loss 2.302589 val loss: 2.303019\n",
      "iteration 50 / 2000: training loss 1.902544 val loss: 1.703244\n",
      "iteration 100 / 2000: training loss 1.344977 val loss: 1.263624\n",
      "iteration 150 / 2000: training loss 1.354861 val loss: 1.040426\n",
      "iteration 200 / 2000: training loss 0.859409 val loss: 0.852048\n",
      "iteration 250 / 2000: training loss 0.890667 val loss: 0.899303\n",
      "iteration 300 / 2000: training loss 0.751371 val loss: 0.841025\n",
      "iteration 350 / 2000: training loss 0.698214 val loss: 0.706466\n",
      "iteration 400 / 2000: training loss 0.615423 val loss: 0.583119\n",
      "iteration 450 / 2000: training loss 0.629241 val loss: 0.611279\n",
      "iteration 500 / 2000: training loss 0.538070 val loss: 0.604863\n",
      "iteration 550 / 2000: training loss 0.572188 val loss: 0.591996\n",
      "iteration 600 / 2000: training loss 0.445045 val loss: 0.578926\n",
      "iteration 650 / 2000: training loss 0.458589 val loss: 0.556024\n",
      "iteration 700 / 2000: training loss 0.541338 val loss: 0.525175\n",
      "iteration 750 / 2000: training loss 0.499849 val loss: 0.534926\n",
      "iteration 800 / 2000: training loss 0.534008 val loss: 0.549408\n",
      "iteration 850 / 2000: training loss 0.498570 val loss: 0.542380\n",
      "iteration 900 / 2000: training loss 0.478293 val loss: 0.529883\n",
      "iteration 950 / 2000: training loss 0.485512 val loss: 0.513119\n",
      "iteration 1000 / 2000: training loss 0.481589 val loss: 0.511907\n",
      "iteration 1050 / 2000: training loss 0.478575 val loss: 0.512825\n",
      "iteration 1100 / 2000: training loss 0.515079 val loss: 0.515053\n",
      "iteration 1150 / 2000: training loss 0.452365 val loss: 0.512480\n",
      "iteration 1200 / 2000: training loss 0.514947 val loss: 0.515742\n",
      "iteration 1250 / 2000: training loss 0.548412 val loss: 0.515406\n",
      "iteration 1300 / 2000: training loss 0.480739 val loss: 0.505202\n",
      "iteration 1350 / 2000: training loss 0.535892 val loss: 0.511304\n",
      "iteration 1400 / 2000: training loss 0.441341 val loss: 0.502921\n",
      "iteration 1450 / 2000: training loss 0.517181 val loss: 0.508089\n",
      "iteration 1500 / 2000: training loss 0.479188 val loss: 0.500054\n",
      "iteration 1550 / 2000: training loss 0.547060 val loss: 0.500941\n",
      "iteration 1600 / 2000: training loss 0.501004 val loss: 0.496661\n",
      "iteration 1650 / 2000: training loss 0.499118 val loss: 0.496107\n",
      "iteration 1700 / 2000: training loss 0.458079 val loss: 0.496076\n",
      "iteration 1750 / 2000: training loss 0.422983 val loss: 0.497697\n",
      "iteration 1800 / 2000: training loss 0.521566 val loss: 0.496108\n",
      "iteration 1850 / 2000: training loss 0.473227 val loss: 0.496758\n",
      "iteration 1900 / 2000: training loss 0.402793 val loss: 0.496446\n",
      "iteration 1950 / 2000: training loss 0.472672 val loss: 0.497600\n",
      "hidden_size: 256 lr: 9.000000e-01 lr_decay: 8.000000e-01 reg: 1.000000e-02 train accuracy: 0.950000 val accuracy: 0.947000\n",
      "iteration 0 / 2000: training loss 2.302592 val loss: 2.303022\n",
      "iteration 50 / 2000: training loss 4.244731 val loss: 4.416353\n",
      "iteration 100 / 2000: training loss 2.127361 val loss: 1.920885\n",
      "iteration 150 / 2000: training loss 2.180344 val loss: 3.252052\n",
      "iteration 200 / 2000: training loss 2.575955 val loss: 2.387276\n",
      "iteration 250 / 2000: training loss 3.105283 val loss: 3.583818\n",
      "iteration 300 / 2000: training loss 3.466015 val loss: 3.568735\n",
      "iteration 350 / 2000: training loss 2.459554 val loss: 2.531487\n",
      "hidden_size: 256 lr: 9.000000e-01 lr_decay: 9.800000e-01 reg: 2.000000e-02 train accuracy: 0.590000 val accuracy: 0.511000\n",
      "iteration 0 / 2000: training loss 2.302592 val loss: 2.303022\n",
      "iteration 50 / 2000: training loss 2.471335 val loss: 2.383793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 2000: training loss 1.882472 val loss: 2.230390\n",
      "iteration 150 / 2000: training loss 2.116615 val loss: 2.043259\n",
      "iteration 200 / 2000: training loss 2.164063 val loss: 2.299622\n",
      "iteration 250 / 2000: training loss 1.552685 val loss: 1.442371\n",
      "iteration 300 / 2000: training loss 1.214770 val loss: 1.463351\n",
      "iteration 350 / 2000: training loss 1.211173 val loss: 1.310715\n",
      "iteration 400 / 2000: training loss 1.366000 val loss: 1.220760\n",
      "iteration 450 / 2000: training loss 1.293681 val loss: 1.171017\n",
      "iteration 500 / 2000: training loss 1.097076 val loss: 1.063181\n",
      "iteration 550 / 2000: training loss 1.018807 val loss: 0.961951\n",
      "iteration 600 / 2000: training loss 0.863681 val loss: 0.954409\n",
      "iteration 650 / 2000: training loss 0.793847 val loss: 0.906009\n",
      "iteration 700 / 2000: training loss 1.165424 val loss: 0.987851\n",
      "iteration 750 / 2000: training loss 0.883134 val loss: 0.971334\n",
      "iteration 800 / 2000: training loss 0.862194 val loss: 0.969931\n",
      "iteration 850 / 2000: training loss 0.904024 val loss: 0.901710\n",
      "iteration 900 / 2000: training loss 0.833563 val loss: 0.888678\n",
      "iteration 950 / 2000: training loss 0.719334 val loss: 0.838738\n",
      "hidden_size: 256 lr: 9.000000e-01 lr_decay: 9.000000e-01 reg: 2.000000e-02 train accuracy: 0.940000 val accuracy: 0.864000\n",
      "iteration 0 / 2000: training loss 2.302592 val loss: 2.303022\n",
      "iteration 50 / 2000: training loss 2.365296 val loss: 2.745115\n",
      "iteration 100 / 2000: training loss 1.520047 val loss: 1.726597\n",
      "iteration 150 / 2000: training loss 1.415049 val loss: 1.587604\n",
      "iteration 200 / 2000: training loss 1.098769 val loss: 1.047420\n",
      "iteration 250 / 2000: training loss 1.395048 val loss: 1.519707\n",
      "iteration 300 / 2000: training loss 0.979533 val loss: 1.023858\n",
      "iteration 350 / 2000: training loss 0.979724 val loss: 0.980321\n",
      "iteration 400 / 2000: training loss 0.921725 val loss: 0.889942\n",
      "iteration 450 / 2000: training loss 0.972092 val loss: 0.862505\n",
      "iteration 500 / 2000: training loss 0.817236 val loss: 0.848695\n",
      "iteration 550 / 2000: training loss 0.835024 val loss: 0.818254\n",
      "iteration 600 / 2000: training loss 0.786735 val loss: 0.830768\n",
      "iteration 650 / 2000: training loss 0.721129 val loss: 0.785651\n",
      "iteration 700 / 2000: training loss 0.863735 val loss: 0.821147\n",
      "iteration 750 / 2000: training loss 0.781335 val loss: 0.771117\n",
      "iteration 800 / 2000: training loss 0.767536 val loss: 0.817290\n",
      "iteration 850 / 2000: training loss 0.840255 val loss: 0.765859\n",
      "iteration 900 / 2000: training loss 0.787042 val loss: 0.770619\n",
      "iteration 950 / 2000: training loss 0.749746 val loss: 0.784996\n",
      "iteration 1000 / 2000: training loss 0.720121 val loss: 0.754609\n",
      "iteration 1050 / 2000: training loss 0.716180 val loss: 0.754921\n",
      "iteration 1100 / 2000: training loss 0.748958 val loss: 0.751050\n",
      "iteration 1150 / 2000: training loss 0.685665 val loss: 0.753573\n",
      "iteration 1200 / 2000: training loss 0.744115 val loss: 0.752201\n",
      "iteration 1250 / 2000: training loss 0.803864 val loss: 0.747919\n",
      "iteration 1300 / 2000: training loss 0.715404 val loss: 0.741097\n",
      "iteration 1350 / 2000: training loss 0.785236 val loss: 0.731127\n",
      "iteration 1400 / 2000: training loss 0.678633 val loss: 0.730167\n",
      "iteration 1450 / 2000: training loss 0.776587 val loss: 0.729065\n",
      "iteration 1500 / 2000: training loss 0.719279 val loss: 0.722780\n",
      "iteration 1550 / 2000: training loss 0.743935 val loss: 0.719548\n",
      "iteration 1600 / 2000: training loss 0.744941 val loss: 0.721631\n",
      "iteration 1650 / 2000: training loss 0.715377 val loss: 0.719444\n",
      "iteration 1700 / 2000: training loss 0.673336 val loss: 0.716737\n",
      "hidden_size: 256 lr: 9.000000e-01 lr_decay: 8.500000e-01 reg: 2.000000e-02 train accuracy: 0.900000 val accuracy: 0.929000\n",
      "iteration 0 / 2000: training loss 2.302592 val loss: 2.303022\n",
      "iteration 50 / 2000: training loss 1.761414 val loss: 1.733979\n",
      "iteration 100 / 2000: training loss 1.533231 val loss: 1.652309\n",
      "iteration 150 / 2000: training loss 1.325376 val loss: 1.300420\n",
      "iteration 200 / 2000: training loss 1.064978 val loss: 0.954412\n",
      "iteration 250 / 2000: training loss 1.040235 val loss: 1.151463\n",
      "iteration 300 / 2000: training loss 0.853533 val loss: 0.972187\n",
      "iteration 350 / 2000: training loss 0.994642 val loss: 0.885498\n",
      "iteration 400 / 2000: training loss 0.846401 val loss: 0.830804\n",
      "iteration 450 / 2000: training loss 0.891320 val loss: 0.882901\n",
      "iteration 500 / 2000: training loss 0.827307 val loss: 0.860871\n",
      "iteration 550 / 2000: training loss 0.761599 val loss: 0.823832\n",
      "iteration 600 / 2000: training loss 0.714027 val loss: 0.827992\n",
      "iteration 650 / 2000: training loss 0.672309 val loss: 0.755614\n",
      "iteration 700 / 2000: training loss 0.772708 val loss: 0.768242\n",
      "iteration 750 / 2000: training loss 0.758396 val loss: 0.738906\n",
      "iteration 800 / 2000: training loss 0.789541 val loss: 0.747187\n",
      "iteration 850 / 2000: training loss 0.739976 val loss: 0.775082\n",
      "iteration 900 / 2000: training loss 0.732763 val loss: 0.748453\n",
      "iteration 950 / 2000: training loss 0.712521 val loss: 0.739161\n",
      "iteration 1000 / 2000: training loss 0.702372 val loss: 0.730952\n",
      "iteration 1050 / 2000: training loss 0.701464 val loss: 0.735432\n",
      "hidden_size: 256 lr: 9.000000e-01 lr_decay: 8.000000e-01 reg: 2.000000e-02 train accuracy: 0.900000 val accuracy: 0.922000\n",
      "iteration 0 / 2000: training loss 2.302599 val loss: 2.303028\n",
      "iteration 50 / 2000: training loss 5.833154 val loss: 6.273218\n",
      "iteration 100 / 2000: training loss nan val loss: nan\n",
      "iteration 150 / 2000: training loss nan val loss: nan\n",
      "iteration 200 / 2000: training loss nan val loss: nan\n",
      "iteration 250 / 2000: training loss nan val loss: nan\n",
      "iteration 300 / 2000: training loss nan val loss: nan\n",
      "iteration 350 / 2000: training loss nan val loss: nan\n",
      "iteration 400 / 2000: training loss nan val loss: nan\n",
      "iteration 450 / 2000: training loss nan val loss: nan\n",
      "iteration 500 / 2000: training loss nan val loss: nan\n",
      "iteration 550 / 2000: training loss nan val loss: nan\n",
      "iteration 600 / 2000: training loss nan val loss: nan\n",
      "iteration 650 / 2000: training loss nan val loss: nan\n",
      "iteration 700 / 2000: training loss nan val loss: nan\n",
      "iteration 750 / 2000: training loss nan val loss: nan\n",
      "iteration 800 / 2000: training loss nan val loss: nan\n",
      "iteration 850 / 2000: training loss nan val loss: nan\n",
      "iteration 900 / 2000: training loss nan val loss: nan\n",
      "iteration 950 / 2000: training loss nan val loss: nan\n",
      "iteration 1000 / 2000: training loss nan val loss: nan\n",
      "iteration 1050 / 2000: training loss nan val loss: nan\n",
      "iteration 1100 / 2000: training loss nan val loss: nan\n",
      "iteration 1150 / 2000: training loss nan val loss: nan\n",
      "iteration 1200 / 2000: training loss nan val loss: nan\n",
      "iteration 1250 / 2000: training loss nan val loss: nan\n",
      "iteration 1300 / 2000: training loss nan val loss: nan\n",
      "iteration 1350 / 2000: training loss nan val loss: nan\n",
      "iteration 1400 / 2000: training loss nan val loss: nan\n",
      "iteration 1450 / 2000: training loss nan val loss: nan\n",
      "iteration 1500 / 2000: training loss nan val loss: nan\n",
      "iteration 1550 / 2000: training loss nan val loss: nan\n",
      "iteration 1600 / 2000: training loss nan val loss: nan\n",
      "iteration 1650 / 2000: training loss nan val loss: nan\n",
      "iteration 1700 / 2000: training loss nan val loss: nan\n",
      "iteration 1750 / 2000: training loss nan val loss: nan\n",
      "iteration 1800 / 2000: training loss nan val loss: nan\n",
      "iteration 1850 / 2000: training loss nan val loss: nan\n",
      "iteration 1900 / 2000: training loss nan val loss: nan\n",
      "iteration 1950 / 2000: training loss nan val loss: nan\n",
      "hidden_size: 256 lr: 9.000000e-01 lr_decay: 9.800000e-01 reg: 4.000000e-02 train accuracy: 0.080000 val accuracy: 0.100000\n",
      "iteration 0 / 2000: training loss 2.302599 val loss: 2.303028\n",
      "iteration 50 / 2000: training loss 4.860973 val loss: 5.111669\n",
      "iteration 100 / 2000: training loss 4.528479 val loss: 5.606937\n",
      "iteration 150 / 2000: training loss 2.656767 val loss: 2.650357\n",
      "iteration 200 / 2000: training loss 1.611289 val loss: 1.899861\n",
      "iteration 250 / 2000: training loss 2.298641 val loss: 2.485735\n",
      "iteration 300 / 2000: training loss 2.099171 val loss: 1.818272\n",
      "iteration 350 / 2000: training loss 1.999665 val loss: 2.111455\n",
      "hidden_size: 256 lr: 9.000000e-01 lr_decay: 9.000000e-01 reg: 4.000000e-02 train accuracy: 0.570000 val accuracy: 0.530000\n",
      "iteration 0 / 2000: training loss 2.302599 val loss: 2.303028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 50 / 2000: training loss 5.367929 val loss: 5.101848\n",
      "iteration 100 / 2000: training loss 3.305809 val loss: 3.089172\n",
      "iteration 150 / 2000: training loss 2.895648 val loss: 2.706221\n",
      "iteration 200 / 2000: training loss 1.951478 val loss: 1.937974\n",
      "iteration 250 / 2000: training loss 1.531909 val loss: 1.756930\n",
      "iteration 300 / 2000: training loss 1.454694 val loss: 1.561849\n",
      "iteration 350 / 2000: training loss 1.516639 val loss: 1.376749\n",
      "iteration 400 / 2000: training loss 1.385866 val loss: 1.323577\n",
      "iteration 450 / 2000: training loss 1.384780 val loss: 1.242112\n",
      "iteration 500 / 2000: training loss 1.160714 val loss: 1.278621\n",
      "iteration 550 / 2000: training loss 1.140667 val loss: 1.175559\n",
      "iteration 600 / 2000: training loss 1.117311 val loss: 1.230274\n",
      "iteration 650 / 2000: training loss 1.159338 val loss: 1.138245\n",
      "iteration 700 / 2000: training loss 1.204546 val loss: 1.230305\n",
      "iteration 750 / 2000: training loss 1.108123 val loss: 1.099915\n",
      "iteration 800 / 2000: training loss 1.082946 val loss: 1.112499\n",
      "iteration 850 / 2000: training loss 1.137468 val loss: 1.087559\n",
      "iteration 900 / 2000: training loss 1.127625 val loss: 1.095572\n",
      "iteration 950 / 2000: training loss 1.016454 val loss: 1.084459\n",
      "iteration 1000 / 2000: training loss 1.098780 val loss: 1.097286\n",
      "iteration 1050 / 2000: training loss 1.029605 val loss: 1.072495\n",
      "iteration 1100 / 2000: training loss 1.051921 val loss: 1.070305\n",
      "iteration 1150 / 2000: training loss 1.007656 val loss: 1.094827\n",
      "iteration 1200 / 2000: training loss 1.061948 val loss: 1.055385\n",
      "iteration 1250 / 2000: training loss 1.103534 val loss: 1.074511\n",
      "iteration 1300 / 2000: training loss 1.063301 val loss: 1.063153\n",
      "iteration 1350 / 2000: training loss 1.106373 val loss: 1.052698\n",
      "iteration 1400 / 2000: training loss 1.029807 val loss: 1.044132\n",
      "iteration 1450 / 2000: training loss 1.105860 val loss: 1.048904\n",
      "iteration 1500 / 2000: training loss 1.037553 val loss: 1.037911\n",
      "iteration 1550 / 2000: training loss 1.046307 val loss: 1.042459\n",
      "iteration 1600 / 2000: training loss 1.069915 val loss: 1.048448\n",
      "iteration 1650 / 2000: training loss 1.041588 val loss: 1.042560\n",
      "iteration 1700 / 2000: training loss 0.991989 val loss: 1.030869\n",
      "iteration 1750 / 2000: training loss 0.944182 val loss: 1.024205\n",
      "iteration 1800 / 2000: training loss 1.084844 val loss: 1.041928\n",
      "iteration 1850 / 2000: training loss 1.034981 val loss: 1.032499\n",
      "iteration 1900 / 2000: training loss 0.947049 val loss: 1.028281\n",
      "iteration 1950 / 2000: training loss 1.025144 val loss: 1.025861\n",
      "hidden_size: 256 lr: 9.000000e-01 lr_decay: 8.500000e-01 reg: 4.000000e-02 train accuracy: 0.890000 val accuracy: 0.923000\n",
      "iteration 0 / 2000: training loss 2.302599 val loss: 2.303028\n",
      "iteration 50 / 2000: training loss 2.683908 val loss: 3.248715\n",
      "iteration 100 / 2000: training loss 2.417119 val loss: 2.242985\n",
      "iteration 150 / 2000: training loss 2.217773 val loss: 1.888550\n",
      "iteration 200 / 2000: training loss 1.431639 val loss: 1.460456\n",
      "iteration 250 / 2000: training loss 1.342550 val loss: 1.577441\n",
      "iteration 300 / 2000: training loss 1.243543 val loss: 1.360491\n",
      "iteration 350 / 2000: training loss 1.263391 val loss: 1.202947\n",
      "iteration 400 / 2000: training loss 1.225656 val loss: 1.172706\n",
      "iteration 450 / 2000: training loss 1.213503 val loss: 1.189521\n",
      "iteration 500 / 2000: training loss 1.076553 val loss: 1.132203\n",
      "iteration 550 / 2000: training loss 1.076471 val loss: 1.126483\n",
      "iteration 600 / 2000: training loss 1.096875 val loss: 1.138681\n",
      "iteration 650 / 2000: training loss 0.989034 val loss: 1.075297\n",
      "iteration 700 / 2000: training loss 1.099826 val loss: 1.136437\n",
      "iteration 750 / 2000: training loss 1.109002 val loss: 1.071528\n",
      "iteration 800 / 2000: training loss 1.082387 val loss: 1.065344\n",
      "iteration 850 / 2000: training loss 1.074922 val loss: 1.077432\n",
      "iteration 900 / 2000: training loss 1.065437 val loss: 1.052124\n",
      "iteration 950 / 2000: training loss 1.014879 val loss: 1.053496\n",
      "iteration 1000 / 2000: training loss 1.027169 val loss: 1.050992\n",
      "iteration 1050 / 2000: training loss 1.025255 val loss: 1.050456\n",
      "hidden_size: 256 lr: 9.000000e-01 lr_decay: 8.000000e-01 reg: 4.000000e-02 train accuracy: 0.880000 val accuracy: 0.899000\n",
      "iteration 0 / 2000: training loss 2.302606 val loss: 2.303034\n",
      "iteration 50 / 2000: training loss 14.769333 val loss: 11.825400\n",
      "iteration 100 / 2000: training loss nan val loss: nan\n",
      "iteration 150 / 2000: training loss nan val loss: nan\n",
      "iteration 200 / 2000: training loss nan val loss: nan\n",
      "iteration 250 / 2000: training loss nan val loss: nan\n",
      "iteration 300 / 2000: training loss nan val loss: nan\n",
      "iteration 350 / 2000: training loss nan val loss: nan\n",
      "iteration 400 / 2000: training loss nan val loss: nan\n",
      "iteration 450 / 2000: training loss nan val loss: nan\n",
      "iteration 500 / 2000: training loss nan val loss: nan\n",
      "iteration 550 / 2000: training loss nan val loss: nan\n",
      "iteration 600 / 2000: training loss nan val loss: nan\n",
      "iteration 650 / 2000: training loss nan val loss: nan\n",
      "iteration 700 / 2000: training loss nan val loss: nan\n",
      "iteration 750 / 2000: training loss nan val loss: nan\n",
      "iteration 800 / 2000: training loss nan val loss: nan\n",
      "iteration 850 / 2000: training loss nan val loss: nan\n",
      "iteration 900 / 2000: training loss nan val loss: nan\n",
      "iteration 950 / 2000: training loss nan val loss: nan\n",
      "iteration 1000 / 2000: training loss nan val loss: nan\n",
      "iteration 1050 / 2000: training loss nan val loss: nan\n",
      "iteration 1100 / 2000: training loss nan val loss: nan\n",
      "iteration 1150 / 2000: training loss nan val loss: nan\n",
      "iteration 1200 / 2000: training loss nan val loss: nan\n",
      "iteration 1250 / 2000: training loss nan val loss: nan\n",
      "iteration 1300 / 2000: training loss nan val loss: nan\n",
      "iteration 1350 / 2000: training loss nan val loss: nan\n",
      "iteration 1400 / 2000: training loss nan val loss: nan\n",
      "iteration 1450 / 2000: training loss nan val loss: nan\n",
      "iteration 1500 / 2000: training loss nan val loss: nan\n",
      "iteration 1550 / 2000: training loss nan val loss: nan\n",
      "iteration 1600 / 2000: training loss nan val loss: nan\n",
      "iteration 1650 / 2000: training loss nan val loss: nan\n",
      "iteration 1700 / 2000: training loss nan val loss: nan\n",
      "iteration 1750 / 2000: training loss nan val loss: nan\n",
      "iteration 1800 / 2000: training loss nan val loss: nan\n",
      "iteration 1850 / 2000: training loss nan val loss: nan\n",
      "iteration 1900 / 2000: training loss nan val loss: nan\n",
      "iteration 1950 / 2000: training loss nan val loss: nan\n",
      "hidden_size: 256 lr: 9.000000e-01 lr_decay: 9.800000e-01 reg: 6.000000e-02 train accuracy: 0.080000 val accuracy: 0.100000\n",
      "iteration 0 / 2000: training loss 2.302606 val loss: 2.303034\n",
      "iteration 50 / 2000: training loss 5.706679 val loss: 5.873613\n",
      "iteration 100 / 2000: training loss 20.124634 val loss: nan\n",
      "iteration 150 / 2000: training loss nan val loss: nan\n",
      "iteration 200 / 2000: training loss nan val loss: nan\n",
      "iteration 250 / 2000: training loss nan val loss: nan\n",
      "iteration 300 / 2000: training loss nan val loss: nan\n",
      "iteration 350 / 2000: training loss nan val loss: nan\n",
      "iteration 400 / 2000: training loss nan val loss: nan\n",
      "iteration 450 / 2000: training loss nan val loss: nan\n",
      "iteration 500 / 2000: training loss nan val loss: nan\n",
      "iteration 550 / 2000: training loss nan val loss: nan\n",
      "iteration 600 / 2000: training loss nan val loss: nan\n",
      "iteration 650 / 2000: training loss nan val loss: nan\n",
      "iteration 700 / 2000: training loss nan val loss: nan\n",
      "iteration 750 / 2000: training loss nan val loss: nan\n",
      "iteration 800 / 2000: training loss nan val loss: nan\n",
      "iteration 850 / 2000: training loss nan val loss: nan\n",
      "iteration 900 / 2000: training loss nan val loss: nan\n",
      "iteration 950 / 2000: training loss nan val loss: nan\n",
      "iteration 1000 / 2000: training loss nan val loss: nan\n",
      "iteration 1050 / 2000: training loss nan val loss: nan\n",
      "iteration 1100 / 2000: training loss nan val loss: nan\n",
      "iteration 1150 / 2000: training loss nan val loss: nan\n",
      "iteration 1200 / 2000: training loss nan val loss: nan\n",
      "iteration 1250 / 2000: training loss nan val loss: nan\n",
      "iteration 1300 / 2000: training loss nan val loss: nan\n",
      "iteration 1350 / 2000: training loss nan val loss: nan\n",
      "iteration 1400 / 2000: training loss nan val loss: nan\n",
      "iteration 1450 / 2000: training loss nan val loss: nan\n",
      "iteration 1500 / 2000: training loss nan val loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1550 / 2000: training loss nan val loss: nan\n",
      "iteration 1600 / 2000: training loss nan val loss: nan\n",
      "iteration 1650 / 2000: training loss nan val loss: nan\n",
      "iteration 1700 / 2000: training loss nan val loss: nan\n",
      "iteration 1750 / 2000: training loss nan val loss: nan\n",
      "iteration 1800 / 2000: training loss nan val loss: nan\n",
      "iteration 1850 / 2000: training loss nan val loss: nan\n",
      "iteration 1900 / 2000: training loss nan val loss: nan\n",
      "iteration 1950 / 2000: training loss nan val loss: nan\n",
      "hidden_size: 256 lr: 9.000000e-01 lr_decay: 9.000000e-01 reg: 6.000000e-02 train accuracy: 0.080000 val accuracy: 0.100000\n",
      "iteration 0 / 2000: training loss 2.302606 val loss: 2.303034\n",
      "iteration 50 / 2000: training loss 3.592236 val loss: 3.396103\n",
      "iteration 100 / 2000: training loss 3.148380 val loss: 3.166745\n",
      "iteration 150 / 2000: training loss 2.741520 val loss: 3.138071\n",
      "iteration 200 / 2000: training loss 1.671476 val loss: 1.697445\n",
      "iteration 250 / 2000: training loss 1.837206 val loss: 1.917897\n",
      "iteration 300 / 2000: training loss 1.765250 val loss: 1.787099\n",
      "iteration 350 / 2000: training loss 1.621132 val loss: 1.675413\n",
      "iteration 400 / 2000: training loss 1.621241 val loss: 1.515011\n",
      "iteration 450 / 2000: training loss 1.531470 val loss: 1.496405\n",
      "iteration 500 / 2000: training loss 1.359349 val loss: 1.487971\n",
      "iteration 550 / 2000: training loss 1.406336 val loss: 1.464476\n",
      "iteration 600 / 2000: training loss 1.383180 val loss: 1.513918\n",
      "iteration 650 / 2000: training loss 1.405790 val loss: 1.376214\n",
      "iteration 700 / 2000: training loss 1.468807 val loss: 1.461971\n",
      "iteration 750 / 2000: training loss 1.332746 val loss: 1.362299\n",
      "iteration 800 / 2000: training loss 1.323936 val loss: 1.364045\n",
      "iteration 850 / 2000: training loss 1.398120 val loss: 1.341984\n",
      "iteration 900 / 2000: training loss 1.362911 val loss: 1.346861\n",
      "iteration 950 / 2000: training loss 1.245235 val loss: 1.349539\n",
      "iteration 1000 / 2000: training loss 1.344850 val loss: 1.340103\n",
      "iteration 1050 / 2000: training loss 1.280713 val loss: 1.315451\n",
      "iteration 1100 / 2000: training loss 1.295624 val loss: 1.310824\n",
      "iteration 1150 / 2000: training loss 1.243822 val loss: 1.334823\n",
      "iteration 1200 / 2000: training loss 1.319698 val loss: 1.291445\n",
      "iteration 1250 / 2000: training loss 1.341453 val loss: 1.321801\n",
      "iteration 1300 / 2000: training loss 1.291472 val loss: 1.297741\n",
      "iteration 1350 / 2000: training loss 1.344213 val loss: 1.289603\n",
      "iteration 1400 / 2000: training loss 1.291247 val loss: 1.283147\n",
      "iteration 1450 / 2000: training loss 1.352507 val loss: 1.279364\n",
      "iteration 1500 / 2000: training loss 1.286283 val loss: 1.277147\n",
      "iteration 1550 / 2000: training loss 1.285803 val loss: 1.288403\n",
      "iteration 1600 / 2000: training loss 1.306461 val loss: 1.284689\n",
      "iteration 1650 / 2000: training loss 1.269624 val loss: 1.283573\n",
      "iteration 1700 / 2000: training loss 1.233728 val loss: 1.271351\n",
      "iteration 1750 / 2000: training loss 1.177469 val loss: 1.262848\n",
      "iteration 1800 / 2000: training loss 1.314667 val loss: 1.280490\n",
      "hidden_size: 256 lr: 9.000000e-01 lr_decay: 8.500000e-01 reg: 6.000000e-02 train accuracy: 0.870000 val accuracy: 0.883000\n",
      "iteration 0 / 2000: training loss 2.302606 val loss: 2.303034\n",
      "iteration 50 / 2000: training loss 3.427756 val loss: 3.419807\n",
      "iteration 100 / 2000: training loss 3.429756 val loss: 3.065143\n",
      "iteration 150 / 2000: training loss 2.013265 val loss: 2.287039\n",
      "iteration 200 / 2000: training loss 1.511113 val loss: 1.655008\n",
      "iteration 250 / 2000: training loss 1.550514 val loss: 1.690956\n",
      "iteration 300 / 2000: training loss 1.611094 val loss: 1.566269\n",
      "iteration 350 / 2000: training loss 1.495557 val loss: 1.501164\n",
      "iteration 400 / 2000: training loss 1.447101 val loss: 1.437885\n",
      "iteration 450 / 2000: training loss 1.471657 val loss: 1.451613\n",
      "hidden_size: 256 lr: 9.000000e-01 lr_decay: 8.000000e-01 reg: 6.000000e-02 train accuracy: 0.830000 val accuracy: 0.812000\n",
      "Best hidden_size: 200\n",
      "Best lr: 7.000000e-01\n",
      "Best lr_decay: 8.000000e-01\n",
      "Best reg: 1.000000e-02\n",
      "train accuracy: 0.960000\n",
      "val accuracy: 0.894000\n"
     ]
    }
   ],
   "source": [
    "# Test run for task 5\n",
    "\n",
    "#\n",
    "# TODO : Tuning \n",
    "#\n",
    "import Network\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rates = [1e-1, 3e-1, 5e-1, 7e-1, 9e-1]\n",
    "regularization_strengths = [0.01, 0.02, 0.04, 0.06]\n",
    "num_iters = 2000\n",
    "batch_size = 100\n",
    "learning_rate_decay = [0.98, 0.9, 0.85, 0.8]\n",
    "\n",
    "# Net structure\n",
    "input_size = 256\n",
    "hidden_size = [75, 150, 200, 256]\n",
    "num_classes = 10\n",
    "random_seed = 0\n",
    "\n",
    "# Initialization\n",
    "best_hidden_size = None\n",
    "best_lr = None\n",
    "best_lr_decay = None\n",
    "best_reg = None\n",
    "best_val = -1\n",
    "results = {}\n",
    "\n",
    "# Train the two layers network\n",
    "for i in range(len(hidden_size)):\n",
    "    for lr in learning_rates:\n",
    "        for reg in regularization_strengths:\n",
    "            for lr_decay in learning_rate_decay:\n",
    "                params = Network.init(input_size = input_size, hidden_size = hidden_size[i], output_size = num_classes, random_seed = random_seed)\n",
    "                stats, best_params = Network.train(params, X_train, y_train, X_val, y_val, random_seed = random_seed,\n",
    "                learning_rate=lr, momentum=0.9, wd_decay=reg, lr_decay = lr_decay,\n",
    "                num_iters=num_iters, batch_size=batch_size,\n",
    "                do_early_stopping=True,\n",
    "                print_every=50, verbose=True)\n",
    "                train_accuracy = stats['acc_history'][-1]\n",
    "                val_accuracy = stats['val_acc_history'][-1]\n",
    "                if val_accuracy > best_val:\n",
    "                    best_lr = lr\n",
    "                    best_lr_decay = lr_decay\n",
    "                    best_reg = reg\n",
    "                    best_val = val_accuracy\n",
    "                    best_hidden_size = hidden_size[i]\n",
    "                results[(lr, reg, lr_decay)] = train_accuracy, val_accuracy\n",
    "                print('hidden_size: %d lr: %e lr_decay: %e reg: %e train accuracy: %f val accuracy: %f' %\n",
    "                  (hidden_size[i], lr, lr_decay, reg, results[(lr, reg, lr_decay)][0], results[(lr, reg, lr_decay)][1]))\n",
    "print('Best hidden_size: %d\\nBest lr: %e\\nBest lr_decay: %e\\nBest reg: %e\\ntrain accuracy: %f\\nval accuracy: %f' %\n",
    "     (best_hidden_size, best_lr, best_lr_decay, best_reg, results[(best_lr, best_reg, best_lr_decay )][0], results[(best_lr, best_reg, best_lr_decay)][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hidden_size: 200\n",
      "Best lr: 7.000000e-01\n",
      "Best lr_decay: 8.000000e-01\n",
      "Best reg: 1.000000e-02\n",
      "train accuracy: 0.960000\n",
      "val accuracy: 0.894000\n",
      "iteration 0 / 2000: training loss 2.302588 val loss: 2.302847\n",
      "iteration 50 / 2000: training loss 1.362714 val loss: 1.287152\n",
      "iteration 100 / 2000: training loss 1.098567 val loss: 0.994102\n",
      "iteration 150 / 2000: training loss 1.015595 val loss: 0.864508\n",
      "iteration 200 / 2000: training loss 0.643137 val loss: 0.690705\n",
      "iteration 250 / 2000: training loss 0.741669 val loss: 0.699958\n",
      "iteration 300 / 2000: training loss 0.623878 val loss: 0.674713\n",
      "iteration 350 / 2000: training loss 0.582177 val loss: 0.690155\n",
      "iteration 400 / 2000: training loss 0.579571 val loss: 0.567246\n",
      "iteration 450 / 2000: training loss 0.625358 val loss: 0.597562\n",
      "iteration 500 / 2000: training loss 0.544306 val loss: 0.568199\n",
      "iteration 550 / 2000: training loss 0.531299 val loss: 0.533226\n",
      "iteration 600 / 2000: training loss 0.449569 val loss: 0.537722\n",
      "iteration 650 / 2000: training loss 0.435019 val loss: 0.528067\n",
      "iteration 700 / 2000: training loss 0.537574 val loss: 0.518481\n",
      "iteration 750 / 2000: training loss 0.508118 val loss: 0.524654\n",
      "iteration 800 / 2000: training loss 0.514575 val loss: 0.537261\n",
      "iteration 850 / 2000: training loss 0.500915 val loss: 0.525255\n",
      "iteration 900 / 2000: training loss 0.472843 val loss: 0.517846\n",
      "iteration 950 / 2000: training loss 0.468896 val loss: 0.505903\n",
      "iteration 1000 / 2000: training loss 0.478105 val loss: 0.503265\n",
      "iteration 1050 / 2000: training loss 0.476961 val loss: 0.502841\n",
      "iteration 1100 / 2000: training loss 0.502081 val loss: 0.507550\n",
      "iteration 1150 / 2000: training loss 0.445905 val loss: 0.506957\n",
      "iteration 1200 / 2000: training loss 0.509913 val loss: 0.507015\n",
      "iteration 1250 / 2000: training loss 0.539216 val loss: 0.510716\n",
      "iteration 1300 / 2000: training loss 0.477335 val loss: 0.501738\n",
      "iteration 1350 / 2000: training loss 0.526303 val loss: 0.506092\n",
      "iteration 1400 / 2000: training loss 0.438948 val loss: 0.500657\n",
      "iteration 1450 / 2000: training loss 0.506941 val loss: 0.506573\n",
      "iteration 1500 / 2000: training loss 0.474560 val loss: 0.497433\n",
      "iteration 1550 / 2000: training loss 0.543925 val loss: 0.499243\n",
      "iteration 1600 / 2000: training loss 0.496926 val loss: 0.495258\n",
      "iteration 1650 / 2000: training loss 0.492541 val loss: 0.494696\n",
      "iteration 1700 / 2000: training loss 0.460952 val loss: 0.495213\n",
      "iteration 1750 / 2000: training loss 0.420150 val loss: 0.495633\n",
      "iteration 1800 / 2000: training loss 0.509443 val loss: 0.494606\n",
      "iteration 1850 / 2000: training loss 0.478580 val loss: 0.494954\n",
      "iteration 1900 / 2000: training loss 0.401236 val loss: 0.494900\n",
      "iteration 1950 / 2000: training loss 0.468966 val loss: 0.495457\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x900 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAMICAYAAAD/obXwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAACMqUlEQVR4nOzdd3hUZfrG8Xsmk16poYUkgAhIr4JgFwv2hpVuV3TVXRdd67qr6/5U1oKVZkME666KYgdBpYNgQQmEEjrJpJeZ8/vjZCbEhBSYzDmZfD/XlYvJyWTmDaHMned5n9dhGIYhAAAAAMAhOa1eAAAAAADYHcEJAAAAAGpBcAIAAACAWhCcAAAAAKAWBCcAAAAAqAXBCQAAAABqQXACAAAAgFoQnAAAAACgFgQnAAAAAKgFwQkAEHBPPfWUHA6Hevbsacnzp6Wl6eyzz671fl999ZUcDoe++uqrej3+tGnTNGvWrMNbHACgUSI4AQACbsaMGZKk9evX6/vvv7d4NYfWv39/LV26VP3796/X5xGcAKDpITgBAAJq+fLlWrNmjUaNGiVJmj59usUrOrSEhAQde+yxSkhIsHop8ng8Ki4utnoZAIBDIDgBAALKF5QeffRRDRs2TG+++aYKCgqq3G/79u269tprlZKSooiICLVr104XX3yxdu3a5b9Pdna27rjjDnXq1EmRkZFq3bq1zjrrLP388891WsuCBQvUv39/RUdHq1u3bv5KmE91rXqbNm3SZZddpnbt2ikyMlLJyck65ZRTtHr1aklmG+D69ev19ddfy+FwyOFwKC0tzf/5mZmZuuqqq9S6dWtFRkaqe/fuevzxx+X1ev332bx5sxwOhx577DE9/PDDSk9PV2RkpBYuXKikpCRdd911Vb6WzZs3KywsTP/+97/r9LUDAALLZfUCAACho7CwUHPmzNGgQYPUs2dPTZgwQZMmTdK8efM0duxY//22b9+uQYMGqbS0VHfffbd69+6tffv26ZNPPtGBAweUnJys3NxcDR8+XJs3b9Zdd92lIUOGKC8vT998842ysrLUrVu3GteyZs0a3XHHHfrrX/+q5ORkvfzyy5o4caK6dOmi448//pCfd9ZZZ8nj8eixxx5Tx44dtXfvXi1ZskTZ2dmSpHfffVcXX3yxEhMTNW3aNElSZGSkJGnPnj0aNmyYSkpK9Pe//11paWn63//+pzvvvFO///67//4+Tz31lLp27ar/+7//U0JCgo466ihNmDBBL774oh577DElJib67ztt2jRFRERowoQJ9fqeAAACxAAAIEBeeeUVQ5Lx/PPPG4ZhGLm5uUZcXJwxYsSISvebMGGCER4ebmzYsOGQj/XQQw8ZkoyFCxfWex2pqalGVFSUsWXLFv+1wsJCo3nz5sZ1113nv/bll18akowvv/zSMAzD2Lt3ryHJmDp1ao2Pf8wxxxgnnHBClet//etfDUnG999/X+n6DTfcYDgcDuOXX34xDMMwMjIyDElG586djZKSkkr3/f333w2n02k8+eSTldbeokULY/z48XX58gEADYBWPQBAwEyfPl3R0dG67LLLJElxcXG65JJLtGjRIm3cuNF/v48//lgnnXSSunfvfsjH+vjjj9W1a1edeuqph7WWvn37qmPHjv73o6Ki1LVrV23ZsuWQn9O8eXN17txZ//73v/XEE09o1apVlVrsavPFF1+oR48eGjx4cKXr48aNk2EY+uKLLypdP/fccxUeHl7pWqdOnXT22Wdr2rRpMgxDkvTGG29o3759uvnmm+u8FgBAYBGcAAAB8dtvv+mbb77RqFGjZBiGsrOzlZ2drYsvvliSKu0v2rNnjzp06FDj49XlPjVp0aJFlWuRkZEqLCw85Oc4HA59/vnnOv300/XYY4+pf//+atWqlSZPnqzc3Nxan3Pfvn1q27Ztlevt2rXzf/xg1d1Xkm699VZt3LhRCxculCQ9++yzGjp0aL2n/wEAAoc9TgCAgJgxY4YMw9D8+fM1f/78Kh+fPXu2Hn74YYWFhalVq1batm1bjY9Xl/s0hNTUVP+Ai19//VVvvfWWHnjgAZWUlOj555+v8XNbtGihrKysKtd37NghSWrZsmWl6w6Ho9rHOfnkk9WzZ08988wziouL08qVK/Xaa68dzpcDAAgQKk4AgCPm8Xg0e/Zsde7cWV9++WWVtzvuuENZWVn6+OOPJUlnnnmmvvzyS/3yyy+HfMwzzzxTv/76a5X2tmDq2rWr/va3v6lXr15auXKl//qhKlennHKKNmzYUOm+kvTKK6/I4XDopJNOqvNzT548WR9++KGmTJmi5ORkXXLJJYf/hQAAjhgVJwDAEfv444+1Y8cO/etf/9KJJ55Y5eO+6sn06dN19tln66GHHtLHH3+s448/Xnfffbd69eql7OxsLViwQLfffru6deum2267TXPnztV5552nv/71rxo8eLAKCwv19ddf6+yzz65XCKmrtWvX6uabb9Yll1yio446ShEREfriiy+0du1a/fWvf/Xfr1evXnrzzTc1d+5cderUSVFRUerVq5f+9Kc/6ZVXXtGoUaP00EMPKTU1VR9++KGmTZumG264QV27dq3zWq666ipNmTJF33zzjf72t78pIiIi4F8vAKDuCE4AgCM2ffp0RUREaPz48dV+vGXLlrrgggs0f/587dq1S+3bt9cPP/yg+++/X48++qj27dunVq1aafjw4WrevLkkKT4+XosXL9YDDzygF198UQ8++KCaNWumQYMG6dprr22Qr6NNmzbq3Lmzpk2bpq1bt8rhcKhTp056/PHHdcstt/jv9+CDDyorK0vXXHONcnNzlZqaqs2bN6tVq1ZasmSJpkyZoilTpsjtdqtTp0567LHHdPvtt9drLdHR0TrnnHP02muv6frrrw/0lwoAqCeH4RvZAwAAbKOkpERpaWkaPny43nrrLauXAwBNHhUnAABsZM+ePfrll180c+ZM7dq1q1KLIADAOgQnAABs5MMPP9T48ePVtm1bTZs2jRHkAGATtOoBAAAAQC0YRw4AAAAAtSA4AQAAAEAtCE4AAAAAUIsmNxzC6/Vqx44dio+Pl8PhsHo5AAAAACxiGIZyc3PVrl07OZ0115SaXHDasWOHUlJSrF4GAAAAAJvYunWrOnToUON9mlxwio+Pl2T+5iQkJFi8GgAAAABWcbvdSklJ8WeEmjS54ORrz0tISCA4AQAAAKjTFh6GQwAAAABALQhOAAAAAFALghMAAAAA1KLJ7XGqC8MwVFZWJo/HY/VSEABhYWFyuVyMnwcAAMBhIzj9QUlJibKyslRQUGD1UhBAMTExatu2rSIiIqxeCgAAABohgtNBvF6vMjIyFBYWpnbt2ikiIoIqRSNnGIZKSkq0Z88eZWRk6Kijjqr1cDMAAADgjwhOBykpKZHX61VKSopiYmKsXg4CJDo6WuHh4dqyZYtKSkoUFRVl9ZIAAADQyPCj92pQkQg9fE8BAABwJHg1CQAAAAC1IDgBAAAAQC0ITjikE088UbfddpvVywAAAAAsx3CIEFDb5L+xY8dq1qxZ9X7cd955R+Hh4Ye5KgAAACB0EJxCQFZWlv/23Llzdd999+mXX37xX4uOjq50/9LS0joFoubNmwdukQAAAEAjRqteLQzDUEFJmSVvhmHUaY1t2rTxvyUmJsrhcPjfLyoqUlJSkt566y2deOKJioqK0muvvaZ9+/bp8ssvV4cOHRQTE6NevXppzpw5lR73j616aWlp+uc//6kJEyYoPj5eHTt21IsvvhjI324AAADAlqg41aKw1KMe931iyXNveOh0xUQE5lt011136fHHH9fMmTMVGRmpoqIiDRgwQHfddZcSEhL04Ycf6uqrr1anTp00ZMiQQz7O448/rr///e+6++67NX/+fN1www06/vjj1a1bt4CsEwAAALAjglMTcdttt+nCCy+sdO3OO+/0377lllu0YMECzZs3r8bgdNZZZ+nGG2+UZIaxJ598Ul999RXBCQAAACGN4FSL6PAwbXjodMueO1AGDhxY6X2Px6NHH31Uc+fO1fbt21VcXKzi4mLFxsbW+Di9e/f23/a1BO7evTtg6wQAAADsyNI9Tt98843OOecctWvXTg6HQ++9916tn/P1119rwIABioqKUqdOnfT888836BodDodiIlyWvNU2La8+/hiIHn/8cT355JP6y1/+oi+++EKrV6/W6aefrpKSkhof549DJRwOh7xeb8DWCQAAANiRpcEpPz9fffr00TPPPFOn+2dkZOiss87SiBEjtGrVKt19992aPHmy3n777QZeaehZtGiRzjvvPF111VXq06ePOnXqpI0bN1q9LAAAAMCWLG3VO/PMM3XmmWfW+f7PP/+8OnbsqKlTp0qSunfvruXLl+v//u//dNFFFzXQKkNTly5d9Pbbb2vJkiVq1qyZnnjiCe3cuVPdu3e3emkAAACA7TSqceRLly7VyJEjK107/fTTtXz5cpWWllb7OcXFxXK73ZXeIN17773q37+/Tj/9dJ144olq06aNzj//fKuXBQAAANhSoxoOsXPnTiUnJ1e6lpycrLKyMu3du1dt27at8jmPPPKIHnzwwWAt0XLjxo3TuHHj/O+npaVVex5U8+bNa91T9tVXX1V6f/PmzVXus3r16vovEgAAAGhkGlXFSVKVgQm+UHCoQQpTpkxRTk6O/23r1q0NvkYAAAAAoaVRVZzatGmjnTt3Vrq2e/duuVwutWjRotrPiYyMVGRkZDCWBwAAACBENaqK09ChQ7Vw4cJK1z799FMNHDiwyphsAAAAAAgUS4NTXl6eVq9e7d8nk5GRodWrVyszM1OS2WY3ZswY//2vv/56bdmyRbfffrt++uknzZgxQ9OnT9edd95pxfIBAAAANBGWtuotX75cJ510kv/922+/XZI0duxYzZo1S1lZWf4QJUnp6en66KOP9Kc//UnPPvus2rVrp6eeeopR5AAAAAAalKXB6cQTT6x24pvPrFmzqlw74YQTtHLlygZcFQAAABqKx2vosU9+1t7cEv39/GMUE9GottyjCeNPKgAAAILCMAw98MF6vfrdFknS3rxivTx2oMLDGtW2ezRR/CkFAABAUDz9xW969bstcjikSJdTX/+6R3fNXyuv99AdSIBdEJwAAADQ4Ob8kKknFv4qSXrgnGP0/NUDFOZ06J1V2/WvBT9bvDqgdgQnSDL3m912223+99PS0jR16tQaP8fhcOi999474ucO1OMAAAB7+mT9Tt3z7jpJ0s0nddHYYWk66ejWeuyi3pKkF77ZpJcXbbJyiUCtCE4h4JxzztGpp55a7ceWLl0qh8NR74Eay5Yt07XXXhuI5fk98MAD6tu3b5XrWVlZOvPMMwP6XAAAwB5+yNivyXNWyWtIowem6I6RXf0fu2hAB005s5sk6eEPf9J7q7ZbtUygVgSnEDBx4kR98cUX2rJlS5WPzZgxQ3379lX//v3r9ZitWrVSTExMoJZYozZt2igyMjIozwUAAILn551uTZq9TMVlXp3aPVn/uKCnHA5Hpftce3wnTRyeLkm6c94aff3rHiuWCtSKqXq1MQyptMCa5w6Pkf7wj0t1zj77bLVu3VqzZs3S/fff779eUFCguXPn6o477tDll1+uRYsWaf/+/ercubPuvvtuXX755Yd8zLS0NN12223+9r2NGzdq4sSJ+uGHH9SpUyf95z//qfI5d911l959911t27ZNbdq00ZVXXqn77rtP4eHhmjVrlh588EFJ8v+DOXPmTI0bN04Oh0Pvvvuuzj//fEnSunXrdOutt2rp0qWKiYnRRRddpCeeeEJxcXGSpHHjxik7O1vDhw/X448/rpKSEl122WWaOnWqwsPD6/RbC1jJMAwt3bRPc37Yqr25xZaupWf7BI0ZmqaU5sH5QYldrNhyQHOXZapL6zhde3xnq5cDhKRtBwo0dsYPcheVaWBqMz1zRT+5qpme53A4dM9Z3bU3r1jvr96hG15boTnXHKs+KUnBXzRQA4JTbUoLpH+2s+a5794hRcTWejeXy6UxY8Zo1qxZuu+++/zBZN68eSopKdGkSZM0Z84c3XXXXUpISNCHH36oq6++Wp06ddKQIUNqfXyv16sLL7xQLVu21HfffSe3211pP5RPfHy8Zs2apXbt2mndunW65pprFB8fr7/85S8aPXq0fvzxRy1YsECfffaZJCkxMbHKYxQUFOiMM87Qscceq2XLlmn37t2aNGmSbr755krnen355Zdq27atvvzyS/32228aPXq0+vbtq2uuuabWrwewSnGZR/9dk6XpizP0U5bb6uVIkpZu2qfpizN0+jFtNGlEuvp3bFblp8Ghoszj1cc/7tT0xRlavTXbf71VfKQu6NfBuoUBIWh/fonGzPhBu9zF6pocp5fHDlRUeNgh7+90OvTvi/tof36JFm3cq/Gzlmn+9UPVqVVcEFcN1IzgFCImTJigf//73/rqq6900kknSTLb9C688EK1b99ed955p/++t9xyixYsWKB58+bVKTh99tln+umnn7R582Z16GC+uPjnP/9ZZV/S3/72N//ttLQ03XHHHZo7d67+8pe/KDo6WnFxcXK5XGrTps0hn+v1119XYWGhXnnlFcXGmqHxmWee0TnnnKN//etfSk5OliQ1a9ZMzzzzjMLCwtStWzeNGjVKn3/+OcEJtrQvr1ivf5+pV7/boj3lFabo8DBdNKC9BqU1tyyoFJd69MGaHVq0ca8+/nGnPv5xp/qkJGni8HSd2bNNyJyrklNYqrnLMjV7yRZtzy6UJEWEOdWzfYJWZmbr7nd+VK/2ierSOt7ilQKhoaCkTBNmLdOmPflqlxil2RMGKykmotbPi3A59fxVA3T5S99p7bYcXT39B71z4zAlJ0QFYdVA7QhOtQmPMSs/Vj13HXXr1k3Dhg3TjBkzdNJJJ+n333/XokWL9Omnn8rj8ejRRx/V3LlztX37dhUXF6u4uNgfTGrz008/qWPHjv7QJElDhw6tcr/58+dr6tSp+u2335SXl6eysjIlJCTU+WvwPVefPn0qre24446T1+vVL7/84g9OxxxzjMLCKn5y1bZtW61bt65ezwU0tI27cjXj2wy9s3K7isu8kqQ2CVEaMyxVVwzuWKcXEg3tkoEp+mVnrmYsztC7q7drzdZsTZ6zSm0TozR2WJouH9RRiTGNswV2y758zfx2s95avlUFJR5JUovYCF11bKquOjZVzWMjNGbG9/r2t3268fWVeu+m4xQTwX+LwJEo9Xh14+srtXprtpJiwvXKxMFqmxhd58+PjXRpxrhBuuT5pcrYm6+xM37QW9cPVUJU4/x3CKGF/yFq43DUqV3ODiZOnKibb75Zzz77rGbOnKnU1FSdcsop+ve//60nn3xSU6dOVa9evRQbG6vbbrtNJSUldXpcw6h6KN0ff0L+3Xff6bLLLtODDz6o008/XYmJiXrzzTf1+OOP1+trMAzjkD99P/j6H/cyORwOeb3eej0X0BAMw9A3G/dq+uIMfXPQBufeHRI1cXi6zurV1naVnKPbxOtfF/fWn884Wq99t0WvfbdFWTlFevTjn/XU5xt1yYAOGn9cutJa2v/fQsMw9EPGfr28OEOf/bRLvn++uibHaeLwdJ3Xt32ldqGpo/vprKcW6dddebrv/fX6v0v6WLRyoPEzDEN3vb1WX/2yR1HhTk0fO+iwKrkt4yL1yoTBuvC5Jfp5Z66umb1csycMrrHVDwgGglMIufTSS3XrrbfqjTfe0OzZs3XNNdfI4XBo0aJFOu+883TVVVdJMvcsbdy4Ud27d6/T4/bo0UOZmZnasWOH2rUz93stXbq00n2+/fZbpaam6p577vFf++OUv4iICHk8nlqfa/bs2crPz/dXnb799ls5nU517dq1xs8FrFRU6tF7q7Zr+uIMbdydJ8n8ucvIHsmaNKKTBqbaf+9Qy7hI3XZqV11/Qmd9sGaHZizO0M87czV76Ra98t0WndItWROHp+vYTta1Fx5KSZlXH67boemLM/Tj9or9Yyce3UoTh6dreJeW1a65VXyknrqsn658+TvNX7FNQ9Kb65KBKcFcOhAyHl3ws95ZuV1hToemXdlfA1KbHfZjpTSP0ezxgzX6haX6PmO/bntztZ69sr/CnPb6twdNC8EphMTFxWn06NG6++67lZOTo3HjxkmSunTporfffltLlixRs2bN9MQTT2jnzp11Dk6nnnqqjj76aI0ZM0aPP/643G53pYDke47MzEy9+eabGjRokD788EO9++67le6TlpamjIwMrV69Wh06dFB8fHyVMeRXXnml7r//fo0dO1YPPPCA9uzZo1tuuUVXX321v00PsJPduUV6bekWvfZ9pvbnm1Xc2IgwXTooReOHpatji8Y3rS4qPEyXDkzRJQM66Nvf9mn64k368pc9+uynXfrsp13q0TZBE4en65w+7RThsrZ6diC/RG/8kKlXlm7WLre5fyzS5dRFAzpownFpdfpp99DOLXT7aV31f5/+qnvf/1G9OyTp6DbsdwLq4+VFm/TC1+YBto9e2Esndzvy/7N7tEvQi2MGauyMH7Rg/U7d9/6Pevj8quPMgWAhOIWYiRMnavr06Ro5cqQ6duwoSbr33nuVkZGh008/XTExMbr22mt1/vnnKycnp06P6XQ69e6772rixIkaPHiw0tLS9NRTT+mMM87w3+e8887Tn/70J918880qLi7WqFGjdO+99+qBBx7w3+eiiy7SO++8o5NOOknZ2dn+ceQHi4mJ0SeffKJbb71VgwYNqjSOHLCTDTvcmr44Q/9ds0MlHrNNtH1StMYfl6ZLB6WERD++w+HQ8KNaavhRLfXb7jzN/DZDb6/cpg1Zbt0xb40eXfCzxg5N1RVDzP1CwfT7njzNWGyup6jU/P1vHR+pMYe5nhtP7KIfNh/QN7/u0Y2vr9AHNw9XbCT/RQaaYRjasq9AKzMPaPXWbDkdDvXrmKR+Kc2U0jyaF8SN1HurtuvhD3+SJN11RreAVm2Hdm6hqZf11U1vrNTr32eqdXyUbj31qIA9PlAfDqO6DSwhzO12KzExUTk5OVUGFxQVFSkjI0Pp6emKimKCSyjhe4tA8HoNffnLbk1fnKElv+/zX+/fMUkTh3fS6cckV3tGSSg5VIXnwv4dNHF43So8h8swjEoVMJ9j2pkVsLN7H1kFbF9esUY9tVg73UW6oF97PXFpH17IH6G84jKt3ZqtlZkHtCozW6u2Zvsrs3/UMi5CfVOaqX9qkvp3bKbeHRIZ1tEIfPPrHk2YtUxlXkPjj0vTfWf3aJC/N69+t0X3vvejJOkfF/TUlUNSA/4caJpqygZ/RHA6CC+uQxffWxyJgpIyvb1im2Z+u1mb9uZLksKcDp3Zs40mDk9Xv46H38ffWJWUefXRuiy9vHhTpT1FJ3RtpUkjDr2n6HAUlY9N9+25ksz9Y6d2N/dcDUkP3J6rZZv367IXv5PHa+jRC3vpssEdA/K4TYFhGNq0N18rtxzQqq3ZWrnlgH7dlSvvH15lRLic6tU+Uf1SkuQ1pJWZB7R+R45KPZXvGOZ0qFubePXraAap/h2bKbVFDGHWRtZszdblL32nghKPzu3TTlNH95WzAfcgPbHwVz31+UY5HdK0KwfojJ6HPt4EqCuCUw0ITk0T31scjp05RZq9dLPe+D5TOYWlkqT4KJcuH9xRY4elqX1S3UfshirfFLvpizO08KApdkcnx2vC8LQqU+zqY29esX/K3948s0oRExHW4FP+nvvqd/1rwc+KdDn13k3HqXvb+h2r0FS4i0q1Zmu2VmVWVJR8f08O1j4puiL8pDZT97bxinRV/jNRVOrR+h1urSp/nJWZB5SVU1TlsZrHRqhfSpL/8XqnJCmOlkpLbNqTp4ufX6r9+SUa3qWlZowb1OB7Hg3D0N3v/qg5P2QqwuXUqxMGa0inFg36nAh9BKcaEJyaJr63qI+127I1fXGGPlybpbLyH5entojR+GFpunhgCi/UDuFQ5yZdeWyqrj42Va3iI2t5BNMvO3M1ffEmvbd6h0rKz79qV36u1GVBOFfK6zU0cfYyffnLHnVqGasPbhne5L/nXq+h3/fk+QPSyswD2rg7T398BRHpcqp3h0T179hM/To2U/+OSWp9mIeXZuUUms+15YBWZh7Qj9vd/v2EPk6H1DU5Xv1Tm6lfSpL6pzZTp5axVKUa2G53kS58bom2HShUr/aJmnPtsUH7O1JWfk7Upxt2KT7KpXnXD1W3NvxwA4eP4FQDglPTxPcWtfF4DS3csEvTF2/Sss0H/NeHpDfXxOHpOqV7MmNw6yinsFRzl2Vq9pIt2p5dKEmKCHPqvL7tNHFEerUvcrxeQ19v3KMZizO0aONe//U+KUmaNDxdZ/RsE9Tzrw7kl2jUU4u0I6dI5/Rpp6cu69ukXoznFJRq1daKkLR6a7Zyi8qq3C+lebQZkspDS/e2CQ32fSou82jDDnelCpfvz9fBEqPD/RWpfh2T1DclSfEhMKzFLtxFpRr9wnf6KcuttBYxmn/DMLWMq9sPRQKlqNSjMdN/0A+b96t1fKTevmGYUpo3vgmmsAeCUw3qEpzS0tIUHU0LTigpLCzU5s2bCU6oIq+4TG8t26pZSzYrc3+BJMnldOicPu00cXi6erZPtHiFjVeZx6sF63dq+uIMrcrM9l8/rksLTRyerhO7tlZxmVfvrNqmGYsz9Psec/+Y0yGdUb5/rH9H686/WrHlgEa/sFRlXkMPn99TVx0bmpvRPV5DG3fnVqru+L4XB4sOD1PvDon+SlK/js3qXEVsKLvcRZXa+9Zuy1FxWeWqlMMhdW0dXylMdW4V16B7cUJVUalH42b+oO827VfLuEi9c8Mwy45cyCks1aXPL9Uvu3LVqWWs5t8wLOjTPREaCE41qOk3x+Px6Ndff1Xr1q3VogU9s6Fk37592r17t7p27aqwME4et4PiMo9eWbLFH1asUFDi0afrdyq32PxJelJMuK4c0lFjhqYp+TDbi1C9lZkHNH1xhhb8uFOe8vbH9Jaxyi4o0YECc19MXKRLlw1K0dhhabb56fFL32zSPz76SREup965YVjIBGmv19Cr323Rpxt2as3WHOUVV60mpbWIqRSSurWJt/3UyJIyr37e6a4YUJF5QFv3V61KxUe51DclyRw2IesCVErzaF0yIEXNGsELfo/X0C1zVuqjdTsVF+nSm9cea/nfh505RbrouSXanl2oPilJmnPNkEYxibGgpEwfrN4hd1Gpzuvbnv9vLEZwqkFtvzlZWVnKzs5W69atFRPD9J7GzjAMFRQUaPfu3UpKSlLbtm2tXhJkvmi75c1V+nBtltVLkSR1ahWricPTdWG/DoqOIFg3pG0HCjR7yWa9+cNWf2BNaR6tccPSdenADrZrqTIMQ9e8skKf/bRLqS1i9N9bhjf6M7qKyzz6y/y1en/1Dv+12Igw9Tlo6ELflCS1CHL7VUPZnVuk1ZnZWpmZrVXlVanCUo/Vy/KLCjfH+U84Ll1dWsdZvZxqGYah+95fr1e/26KIMKdmjR+kYV1aWr0sSdJvu/N0yfNLdKCgVCd0baWXxw4MaltvfVQ3cCg8zKGze9PhYCWCUw1q+80xDEM7d+5UdnZ28BeHBpOUlKQ2bdoQhG3AMAw98MF6zV66ReFhDk0Ynq4olzVhxeEw99CccFQr2naCLK+4TJ/8uFMJ0eE6uVtrW+8fyyko1VlPLdL27EKd1auNnr2if6P9tySnsFTXv7pCSzftk8vp0O0ju+rErq11dJt4W38PAqnM49XPO3O1KvOAf1qjFbyGoS9+3q31OyrG+Z90dCtNHN5Jx3VpYas/Y099vlFPLPxVDof0zOX9Naq3vX4IuSrzgK546XsVlnp0Yb/2+r9L+tjq3/R123I0ffEm/e+ggUMdm8eoZVyEVh7UxsyeWmsQnGpQ198cj8ej0tKqY1XR+ISHh9OeZyPPfvmb/v3JL5Kkpy7vp3P7tLN4RUDtVm/N1iXPL1Gpx9CD5x6jscPSrF5Sve3ILtT4mcv0y65cxUW69NxV/TXiqFZWL6tJMwxD320yx/l//nPFOP9ubeI14bh0ndu33WGP8w+UOT9kaso76yRJD513jMYMTbN0PYfy5S+7NWn2cnm8hq47vpOmnNXd0vX4Bg7NWJyhHzbv918fnNZcE0ek69TycLRmqznF9aN1Vae4XjIwRbFNfKJnMBCcalCf3xwAgTV3Wabuetv8D/j+c3po/HHpFq8IqLsZizP00P82KDzMobdvGKbeHZKsXlKd/bzTrXEzlmmnu0it4yM1c/wgHdOOtiA7ydibr1nfZmjeim3+cf4t4yJ01bGpuurY1KBPrpOkT9bv1A2vrZDXkG45uYvuGHl00NdQH2+v2KY75q2RJN1zVnddc3ynoK/hUAOHzu7dVhOHd1KvDtX/vcvKKdTsJVv0xvdb5C6fYBkf5dIV5ecGtuPcwAZDcKoBwQmwxsINu3Tdq8vlNaQbTuysu87oZvWSgHoxDEM3vLZSC9bvVErzaP3vlhFKjLb/fqclv+3Vda+uUG5xmbq0jtOs8YPUoZk9hm+gqpyCUs1ZlqnZSzb7DwGOcDl1ft92mji8k45uEx+UdfyQsV9XTf9eJWVeXTYoRY9c2MtW7YOH8sLXv+uRj3+WJD05uo8u6NchKM9b3f7NxOiKgUNtEus2AKKgpExvr9imGd9uVsZec7plmNOhM8snjfbr2KzBvoamiuBUA4ITEHzLN+/XlS9/r+Iyry4Z0EGPXdy7UfwHDPxRTmGpzn56kbbuL9TpxyTr+asG2PrP8vurt+vOeWtU6jE0OK25XhozsMEPEEZglHq8+vhHc5z/mq3Z/uvDu7TUxBHpDbo38+edbl36/FK5i8p0avdkPX9Vf9tPVPQxDEP/+PAnvbw4Qy6nQ9PHDdIJXRuuJXXFlgOasThDC9ZXTAzt1DJW44en66L+7Q97yp/Xa+6Bm744Q0s37fNf798xSZNGdNLIHsmN5ntidwSnGhCcYBdlHq/eW71DfTok6qjk4PwE0Qq/7srVJc8vVU5hqU7u1lovXj2Af+ybOk+ZtHOttPUHKTJe6nSClBicnwoHwrptObrouSUq8Xh179k9NHG4/VpODcPQ819v0r8WmD95H9WrrR6/tI/l+2VQf4ZhVBrnX/7aXJ1bxWpCA0wD3XagQBc9t0S73MUalNZMr44fqCinV/KUSN4y81dPieQpLX8rkbylld/3lJZfK6n5eli4FNtKim0txbWquB1+ZOO5vV5Dt7+1Wu+t3qGYiDDNueZY9UlJCsxvkOp2Rl2lUOsplfL3Svm7pfw9Ut4e87an1Pya41qXf+3lt8Mrt+Wt35GjGYs364M121XqMf8AtE+K1vjj0nTpoJRGNenT6zX0+548/yHWD5x7jOX/LhGcakBwgl08/L8NenlxhqLCnXrm8v46tUey1UsKuB3ZhbrouSXKyilSv45JemPSsYz7borKiqUdq6Qt30qbv5W2fi+V5FW+T4ujzADV6UQpbYQUnWTFSuvslaWbdd/76+VyOjTv+qG2ap/xeA09+N/1emXpFknSxOHpuues7raaMobDs3V/eTvYsq3+s7eaxYTrikOdP2cYUsE+yb1dytlu/ureUfFrYXalIOMtK1FuQYEc3jJFOjyKcJTJYXirLqShRSZIsS2rBqrYluUhozxoxLUy71tN1bekzKuJs5dp0ca9ah4bofnXD1WnVkc27j2nsFRzl2Vq9pIt2pedrZaOHLUNy9WZ6WE6Iy1M7Vy5ZjDK310ejspvFx6o3xNFxFUbqHLDkvTVNun930q1qTBGe41EeSMTdMnAFI0flm7ZYcR+ZcXmn6mibP+vBTl7tX1nlvbs3iX3gT0qztuvGE+eEhz5SlS+Si6apd59Blq6bIJTDQhOsINP1+/Uta+u8L/vdEh/P7+nrhySauGqAiu7oEQXP79Uv+3OU5fWcZp33dBGccgjAqCkQNq2TNqyxAxL25ZJZUWV7xOZKHUcIhXsl3aslA5+ceZwSu36mSGq04lSh8FH/BPoQDMMQze/sUofrstS+6RofTh5uJJigvjn2zD+8NN/83ZxcZEe/mCtfvh9pyIcZZo0LEXn9Wx16GpBdDMpuYeU2FFyUgluLHKLSvXWsky9v2SNvNnb1daxX+3DDmhE62INaFaopNI9knub5M6SPMWBfXKHUwqLMN+crorbYb7b4ZIzvOJ2WPihr5cVl1dgdldUZDz1HBEfFlkRog4OVLGtVBTZQg9/tVfL9oTJlZCsGTecruSk2OofxzDMF/y+atBBlaHcfVnavm2LirJ3qpmRo5aOHMU66vn76girCIO+AOgMrwhX+XvN34d6fr+KDZf2KUH7jEQ54lopuV1HtWzdXo641uXByxdAW0sxLSRnLT+8LC2sEn7MX3OqufaHX8uqHjZdmx0Xvqt2vU+u9+cFEsGpBgQnWG3r/gKNemqR3EVlGjcsTYUlHs1dvlWSdNNJnXXnyKNtvWeiLgpLPLry5e+0MjNbbRKi9M6Nw5gIFMqKcsy2u82LzbC0Y5X5U+yDxbSUUodJqceZvyYfU/EfeGG2GbA2fWW+7f218ue6oqXUoVJ6eUWqTW9bvMjPLSrVOU8v1uZ9BTqlW2u9NGZg7VWdvD3S7vXSrvXS7g1SwYE/tDr9sbXpj4GnrOL+gRQRJ7XuYYao1seU/9pDimke2OdB3Xi95gvqShWi8tu+ylFuVt1DRmxrKbG9lNBeSmhX/tZBimkmhUWozOHSPxb8pu+35CoqKkpPXjFIqa2SqglI4bW/8D4ShiEVu/8QXsp/rXJ7j1SSW6+H98gpR0wLOePKK1lO10GhbU/9/165ov5QFaum7c5XLYtuVvu/W76v3xeiqqte+W/vMe9bLw4zPPkClSu6avg5wqDtNRxyK0Y5RqxyFKscI1ZlEQmKjG+hhGYt1bJlslq1TlZYTDMpKsn8IZnFHQYEpxoQnGClkjKvLnl+idZsy1HflCS9dd1QhYc59J/PN2rqZxslSRf2b69HL+ytCJf1LwwPR6nHq+teXaEvft6thCiX5t8wTF1DeA9Xk5S/T8pcaoadLd9KO9dVrhhJUnw7Ke24irDUsmu17TTVytkuZXxdEaTydlX+eHRzKf34iopUc+v2GK3fkaMLpi1RSZlXU87sputO6Gx+oKRA2vOzGY52bZB2/Wjezt/TYGsplUslRpjKHC7FREUpPCKqlp/+u6TcXeY6D/WCMb5dRYhK7mnebtlVcgV/NHbAeD3m9yFne9WW0aAypCJ31WDk3m5Wiur0It4hxSVLCe2UHd5Kq3Pi9N3eSO3wNleW0VxhSe016rgBunBQ+iHPAzIMQ3fMW6N3Vm5XVLhTb1xzrPrbqPW0RqWFlfcMVRu09sqTu0uOogNyqvaXvEZkgvJczbWlOFaZxbHaayRqr5Go5q3baXCvburepbMcvlAUEVf3f9caQmmhv1K3fVumlq77WVu2bFZSeVWsnStXnWIK1czIlrNgn1SHr1+SWVWMSjIDTVSiFJUkb1SSso0Y7SiKUka+S7/muPRbrssfjnIUK7cRK29EnHqnNFe/jknq37GZ+qYkqYUFo/Trg+BUA4ITrPTAB+s1a8lmJcWE68PJI9T+oCrMW8u2asq76+TxGhrepaWeu6q/4hvRhk/J/A/4L/PXat6KbYp0OfX6pCEamMZPrBs9d5aUucTcn7RlibTnp6r3aZZeUU1KHSY1SwvMCwrDMF/Yb/pK2vS1WdX640+Zk1IrQlT6CVJsiyN/3rryevX+V9/qo88+V3dnpsZ0zlfzvN+k/ZuqhklJkkNq3qmishPfxgwhzj+EGt/t6q7/IQit3VmgCbNXaW9+qdonRWv2hEHq0roeP6zwlEr7fjOrYL5K2K4NUk5m9fd3usw9af5AdYz5lphi7YtIyQxFebvKKzPbKu/n8f2am2VW8WzPYf75SGhXXik6qFqU2MH8Na6N5KrcIlrdeUAJUS5dfojzgB75+Ce98PUmhTkdemnMAJ3cLfT220rShm37deNLCxVdsk9npDl1y5AkOY0yf+tcTlgzvbG+ULO+36FdbrPqEuly6sL+HTRxeFr9/k5ZaF9esV7/PlOvLN2ivXnm1xEdHqZL+7fRhP6JSo3Mr2gNLCs6KCAd9GtkvPbll2hVZrZ/iMOabdn+88UO1qlVrPqlNFP/1CT1S2mmo9vEK6yR7ackONWA4ASrfLwuSze8vlKSNGPcwGr/c/rql9268fWVKijxqHvbBM0aP6jqZl8be2zBz5r21e9yOqQXrh6o00Jw4EXIMwwpO7N8f1J5693+TVXv16pbRTWp41CzDSgYPKXS9pUV1ahtP1R9EdymV0WQ6jhMigjQhun8vQcFi/KQsednqbSg+vvHtDADRevyYJHcw/x9izjEHovD8OXP5r8ZhaUe9Sj/N6N1oP7NKMqRdv9UOUztWi8V51R//8gEqXX3ymGqdY/AteF4yqS8nRUhKGd71WCUu1Myqr64q8IRJsW3Nac6Whn2wmP+0EJ30O34NmY4PkyHOg/orF5tNXF4uvqmJOnlRZv08IfmD0L+75I+unhA45lueTiW/r5PY2f8oBKPV1cO6aiHz++pTXvzNfPbDM1fsU1FpeYPO1rFR2rs0FRdMSRVzRvp3tziMo/+uyZL0xdn6Kcss6XP4ZBOPrq1Jo5I19BOLfzbAko9Xv2yM1crMw9o5ZYDWrU1W1v2Vf13LS7Spb4pSerfMUn9yqtJobB3meBUA4ITrLBlX77OfmqxcovLdN0JnTTlzO6HvO+6bTkaP2uZ9uYVq11ilGZPGNwoxpXP/DZDD/53gyTpXxf10uhBHS1ekc3lbDMrOMXug6oHf9xA7apmj0EN151h9X8RaBhmtcG3P2nLEnNTeSUOM4wcXFGKbRmw34ojUpxnrtkXpHavr/zxsAgpZUj5xL6TpLZ9zd+/mpQWSnt+qRySdm+o2jLo44qSp+XRWri3hZYVtlVku1668+oL5IxPbtAX5W/+kKl73vtRHq+hEUe11HNXDVDcIdqxAsYwzICya0PFXq1dG8x9aYdqLUvoULU61eKoypUST6kZeiq1rB1cNdphhqa6THhzhB20j+cPYcRXqYltXfufgxBxqPOAjmmXoPU7zBfUd53RTTec2NmqJQbVR+uydNMbK2UYlX8PJKlH2wRNHJ6us/u0VaQrNCbAGoahpZv2afqiDH3+827/9e5tEzSscwut256jtduy/aHxYF1ax/lDUv+OzdSldVyjqybVBcGpBgQnBFtRqUcXP79EP253a2BqM8259liF13KO0db9BRo78wdt2pOvhCiXXhozUEM6BbH9qJ4+WLNDt765SoYh/fn0o3XTSV2sXpL9FB4ww4nvBf6+3xrmearbzH2oti+H06yY/HHfjdNlbtj1VZRShli+ebfO8nZLGd9Im76Ufv+qagiMTDDHnfsqUmGu8hBQvg9p1wZp/++HfoHeLL2ikpJcvu+neSfJGaafstw6/9lvVVzmbdC/B4Zh6MnPNuqpz819kRf176BHL+pV678rDaqspKLdb/f6it/TnK3V39/pMvdKhUeblaO8XarT/guny9x3ldCuvFJzcDAqfz+udcMOMGjEqjsPaMJx6br37O6NfihRfbz63Rbd+96PksyfbZzSLVkTh6fr2E7NQ/r3YdOePM38drPmr9imwtLKldn4KJf6dWymfilJ6p9qVpMSoxvXdoHDRXCqAcEJwXbvez/q1e+2qHlshD6cPFxtE+s2Xe5AfokmvbJcK7YcUESYU0+M7qOze7dr4NXW3+KNezV+1g8q9RgaOzRVD5x7TEj/x1NnpUXmeUW+IQc7VlUzcru/+UKvuqlph5qmdvABkoHapxEWKXUYZAaltOPM2wFsJ7OMYZhthpu+NL8HGd+Y7Wd1Ed38oIBUXiVp1U2KrPkcmLnLMnXX2+vkdEhzrjk24D/wKPV4NeWddZq/wgyEk0/uoj+d1tW+f+cKs812v4OrU7s3VD8NzBkuJbQ1K1R/3MvjC0ixrW0xUbGx2+0u0pvLtsoV5tD1x3dukmd8vbVsqzbtzdfoQSlKbxkC/97VQ3ZBid5avlVb9xeqV/tE9U9NUqeWcU3yz4FEcKoRwQnB9N81O3TLnFWSpFnjB+nEo1vX6/OLSj267c3VWrB+pyTpb6O6a9KITgFf5+Faty1Hl724VPklHo3q3VZPXdYvJMv4deL1SjvXVlSUMpdWPbuoZdeKSkfqcUdexTGMirHVhxpnXdv1xA5S+wGNe0paXXk9UtbqikETmd9JMqRWR5uVo4PHcce3Oaw2O8MwdMdba/TOqu1qHR+pj24doZYBmiiVV1ymG19fqW9+3SOnQ3r4/F66YkgjbIk1DLMFb/cG889iQnvzz2FMS0IRgKAjONWA4IRgydibr3OeXqy84jLddFJn/fn0bof1OB6vob//b4NmLdksSRp/XJr+NqqH5QFl8958Xfz8Eu3NK9Gwzi00c/ygkOkJr7P9GRVBKeMbqXB/5Y/HtSkPSieY096CNUABdVNWUn6YZ2D3uhSUlOncZ77Vb7vzNOKolpo1fvAR/33d7S7S+FnLtH6HW9HhYXrmin46pTvDVwDgSBGcakBwQjAUlXp0wbQl+inLrSHpzfX6pCFyHcH+A8Mw9PKiDP3jI3P60Zk92+jJ0X0VFW5NUNmdW6SLn1uqzP0F6tE2QXOvO7bRjU4/LPl7K58vlP2Hcc0R8VLa8IqqUqujrR/PDEv8uitX5z3zrQpLPbr9tK6afMpRh/1Yv+3O1dgZy7Q9u1AtYiM0Y9wg9UlJCtxiAaAJq082aBojZYAge/C/G/RTllstYiP01OX9jig0SZLD4dA1x3dScmKU7nxrjT7+caf25n2vl8YMVFJMcEeB5haVatyMZcrcX6COzWM0a8Kg0A1NJQXm+UW+oLRzXeWPO8PN/UC+oNS+/xGND0bo6Jocr7+f31N3zlujqZ/9qoFpzTSsc/0nES7bvF+TZi9XTmGp0lrEaPaEwUpt0bT2YwCAXVBxgmU8XkPfb9qnY9olKjEmdF5svr96u259c7UcDunVCUM0/KjAjm1e+vs+XfvqcuUWlalzq1jNGj9YKc0DdE5NLYrLPBo/c5mW/L5PLeMiNP/6YUoLpU21njJziMOmr8zK0tbvzT0YB0vuedAZQUNrHRaApu3P89Zo3optahkXqY9uHa7W8XU/Y+njdVm6de5qlZR51TclSdPHDlSLAO2XAgCYaNWrAcHJHn7fk6e/zF+rFVsOHN5J9zb12+48nfvMYhWUeDT5lKN0+2ldG+R5ft2Vq7EzflBWTpFaxUdq5rhB6tk+sUGey8frNXTLm6v04dosxUaE6c1rh6pXh4Z9zgZnGNLejRUVpc2Lqk77SuggdT7RPAMo/Xhz1DFQR4UlHp3/7Lf6ZVeuhnZqodcmDanTfqcZizP09w83yDCkU7sn6+nL+yk6oontIQSAICA41YDgZC2P19CMxRn6v09/UXFZxWjmxOhwvTx2oAalNbdwdUfm4BdIwzq30KsT6/YC6XDtzCnSuJk/6OeduYqNCNO0qwbohK6tGuS5DMPQAx+s1+ylWxQe5tDMcYMDXkkLuqw10vwJVc9TikqS0n3n/JxkntHDPiUcgfr8QMXrNfTIxz/ppUUZkqSrju2oB8/tafkwGAAIVQSnGhCcrPPb7lz9ef5arcrMliSNOKql7jqjm+57/0etzMxWhMupqaP76qxeba1d6GE6kpacw+UuKtUNr63Qt7/tk8vp0CMX9tIlA1MC/jzPfvmb/v3JL5Kkpy7vp3P72O88qXr5ZYEZmkrzzTOMOh5b0X7Xtg+HZyLg3lu1XbfNNVt4X5kwWCOOqvpDjqJSj+6Yt0Yfrs2SJN11Rjddf0In+57RBAAhgOBUA4JT8JV5vHppUYae/OxXlZR5FR/p0t/O7q5LB6bI4XCosMSjW99cpU837JLDIf1tVA9NHJ5u9bLrZf6Kbbpz3ho5HdJrk4Yc1ibww1VS5tVf5q/Re6t3SJJuP62rbjm5ixyGVyrYL+XvlvL3SHl7/nB7j1SULXU9QzrutkOen+I70FOS7j+nh8Yf17i+N1V8/6K04C7zMNpOJ0mXzDry85SAOpjyzlrN+WGrWsRG6KNbRyg5oeKHKzkFpbrm1eX6IWO/wsMceuzi3rqgXwcLVwsATQPBqQYEp+D6dVeu/jxvjdZsy5EkndC1lR65sJfaJUVXup/Ha7aCvfrdFknSxOHpuues7o3iFOtfd+Xq3GcWq6jUe8Rjh2tVVlwefHabo7Hzd0t5u2Xk79GGjb9r/+7taunIUYeIPMV5cszwVBdHnS5d+GKVALFwwy5d9+pyeQ3pxhM76y9nHN5ZVLbg9Uif/k36bpr5fv8x0qgnmIKHoDn4mILB6c31RvkxBduzCzVuxg/auDtP8ZEuPX/1AB3XpZG3wgJAI0FwqgHBKTjKPF698M0m/eezjSrxeBUf5dJ9Z/fQxQM6HLLtxDAMvfDNJj368c+SpFG92+rxS/pYdlZRXeQXl+m8Z4/goEvDkEryyoPQnoNC0Z5qAtIeqTin/ouMbm4ONIhtZb4dfLvYLX3xsFRWZO7luewNqXV3SdLyzft15cvfq7jMq0sGdNBjF/duvC1DJfnS29dIv3xovn/K/dLwP7F3CUH3x4OxR/Vqp3Ezf9Du3GK1SYjSzPGD1L0t/zcBQLAQnGpAcGp4P+9068/z1mrddvNF/sndWuufF/RSm8S67fl5f/V23TlvjUo9hganN9dLVw+05bhywzB0x1tr9M6q7UpOiNSHk0eoZV1HBeftkb5/XloxUyrYV78ndoaXB5+W1QaiFXtdeuirvdpRGqf27VP00rhj1Sq+hnXtWCXNvVrK2SqFx0rnT9OvLU/Rxc8tkbuoTKd0a60Xrh5wxGdRWSZ3lzRntPl1hkVKFzwv9bzQ6lWhCfvf2h26+Y1VkqSYiDAVlHjUNTlOs8YPrlKNBwA0LIJTDQhODafU49XzX/2up77YqFKPoYQol+4/5xhd2L99vSsVS37bq+teXaHc4jJ1aR2nWeMHqUOz4JxVVFe+vT9OhzTnmmM1pFOL2j9p/yZpydPS6jfMKo9PeKwUVx6AYltXvu0PSOW3o5vVWilZmXlAk2Yv1/78EvOQ2vGD1KlVDecN5e+V5o+XMr6RJL0SdqEeyL9QfTs21+uTjm28Y5B3bZDeuNQMhTEtpMvmSB2HWL0qQPe+96O/NfnYTs31wtUDlRhtvx8QAUCoIzjVgODUMH7KcuvOeWu0fod5Bs6p3VvrHxf0qrT5ub5+3unWuBnLtNNdpNbxkZo5fpCOaWePc4N+ynLr/Ge/VXGZV38+/WjddFKXmj9hx2rp26nShvfNoQSS1H6AOZShyylSROAPkc3Ym6+xM35Q5v4CNYsJ18tjB2lAarNDf4KnTEUL7lXUMnMP0PKwfupyw1wltUwO+NqC4vcvpLfGmu2ILbpIV84z2xEBGygu8+i+99YrPsqlP59xtCJdjfSHEwDQyBGcakBwCqySMq+mffWbnvniN5V5DSVGh+vBc4/ReX3bBWQ/TFZOocbNWKZfduUqLtKl567qX+0Y32DKKy7TuU8v1qa9+Trx6FaaMXZQ9UMsDEPa9KX07X/Mw1V9upwmDb9NSj2uwffY7M0r1sRZy7RmW44iXU49fXk/jTymTbX3LSzx6MqXv1OHbR/psYgXFaUSKSlVuux1qU2vBl1nwK18RfrfnyRvmfn7PPo1KabxnhEGAAAaBsGpBgSnwFm/I0d3zlurn7LMKtPIHsl6+IKeAT+/KKewVNe9ulzfbdovl9Ohf13UWxcNsGZMr2EYuvXN1fpgzQ61TYzSh5NHqHlsROU7ecqkn943A1PWGvOaI0zqeZF03K1Sm55BXXNBSZlufmOVvvh5t5wO6cFzj9HVQ9Mq3afU49V1r67QFz/vVkKUSx9ckqS0hddI2VskV7R03jNSr4uDuu7D4vVKX/xdWvyE+X7v0dK5T0uuOu49AwAATQrBqQYEpyNXUubVM1/+pmlfmlWmZjHhevC8njqnd9sGm7pWXObRn+et1QdrzLOK/nz60brxxM5Bn/L2+vdbdM+7PyrM6dDca4/VwLSDqhilhdKq16Slz0gHNpvXwmPMsddDb5KSOgZ1rQcr83h17/s/as4PWyVJ15/QWX85/Wg5nQ4ZhqG/zF+reSu2KdLl1OuThphfV8F+6e2JZsubJA29WTr1QSnMZdnXUaPSIum9G6T175jvn/BX6cS/MjkPAAAcEsGpBgSnI/Pj9hzdOW+Nft6ZK0k6s2cbPXRez5qntgWI12voX5/8rBe+3iRJumJIRz107jFBm/b24/YcXfjcEpWUeTXlzG667oTO5gcK9kvLpptT8gr2mteim0tDrpMGXSPF1mFoRBAYhqFnvvhNjy/8VZJ0ft92euziPpr62a+a9tXvCnM69MJVA3Rqj4P2NHk95RWcJ83304+XLp5pDqmwk/y90ptXSFu/N6cOnvu01Pdyq1cFAABsjuBUA4LT4Sku8+jpz3/Tc1//Lo/XUPPYCP39vJ4a1btt0Ncye8lmPfDf9TIM6ZRurfX0Ff0UE9GwVZDcolKd8/Ribd5XoFO6tdZLYwbKmbtdWjpNWjFLKs0375jYURp2s9TvqgYZ+BAI85Zv1ZR31qnMa6hTy1ht2muu/V8X9dLoQYeoiq1/T3rvRvPrTEwx9wy16xu0Nddo70bp9UukAxlSVKI0+nUpfYTVqwIAAI0AwakGBKf6W7stW3fOW6Nfd+VJMg+mfejcY9SirmcWNYAFP+7UrW+uUnGZV31SkjR97MC6n6FUT4Zh6OY3VunDdVlqnxStj69oqYQVz0nr3jKHD0hSck9zQt4x50th9h8p/M2ve3TDayuUX+KRpLpNBtz9k/TmldL+3yVXlHT2VOurOpu/NStNRdnmIIsr50utulq7JgAA0GgQnGpAcKq7olKP/vP5Rr34zSZ5vIZaxplVpjN7Bb/KVJ0VW/Zr4uzlyi4oVWqLGM0eP1hpLQNf5Xll6Wbd9/56DQn7RS92WqzErZ9XfDBtRMVI8Ua2l+bH7Tm6/4P1Ov6oVpp8Spe67RcrzJbeuVba+In5/uDrpNP/YU1YXPuW9P5NkqdE6jDIPKMpztqJiwAAoHEhONWA4FQ3qzIP6M/z1+q33WaV6dw+7fTAucdUnSBnsd/35GnczB+0dX+hmsdGaPrYgerXsYaziupp3dYDevaFZzTJ+YEGOn8tv+qQup9jBqYOAwL2XI2G1yt9/aj09b/M9zsOky6dbR7SGwyGIX39mPTVP833e5wnXfCCFB4dnOcHAAAhg+BUA4JTzYpKPXrys1/10jeb5DWklnGRevj8njqjZ/Vn/9jBntxiTZi1TOu25ygq3KmnL++v03oc4aGtZSUqWDFHuxc8pjRjmyTJCIuQo8/l0rDJUsta2tqagp8/lN65TirJleLbmfueGjpIlpVI/71VWvOG+f5xt0qnPCA5gzMgBAAAhBaCUw0IToe2YssB/WX+Gv2+xxwWcEG/9rrv7B5qZrMqU3Xyi8t00xsr9dUve+R0SA+d11NXHZta/wcqzpVWzJKxdJocuebo8zzFyDVkkqKG3yTF2zdAWmLPr9LcK6W9v0phEdKox83x6w2h8IA092pp8yLzXKxRj0sDxzfMcwEAgCaB4FQDglNVRaUePf7pL3p5cYYMQ2oVH6l/XtDryKs2QVbm8eqed3/U3OXmWUU3nthZfz796Lrt3cnbbY4TX/ayVJQjSdplJGmW9yydNe5u9eqc0pBLb9yK3Ob5ST//z3x/wHjpzH8F9tDZ/RnSG5eaAS0iXrp0ltTl1MA9PgAAaJIITjUgOFW2fPN+/WX+Wv9I6gv7m1WmpBj7V5mqYxiGnvr8Nz35mbkf6cJ+7fXoRb0V4TpEK9e+36UlT0ur35A8xZKkosTOenDfKXq77DjdfU4fjTsuPVjLb7y8XmnR49KX/5BkSB0GS5e+IiUEYJDI1mXSnMvMM7IS2ktXvCW16XnkjwsAAJo8glMNCE4VNuxw6+ynF8lrSMkJkXrkwl46uVvjqjIdylvlZxV5vIaO69JCz101QAlR5ZPfDEPKXCotfdbcp6PyvwLtByp/0M06/eN4bcsp1lm92ujZK/rXrWIF06+fSm9PkopzpLhkMzx1PPbwH2/9e9K710llRVKb3mZoCkQYAwAAUP2yATuqm7DvM/bJa0i9OyTq0z+dEDKhSZIuHZii6WMHKiYiTN/+tk+XPr9UO/e7zRHWL54ozTyzvLXMkI4aKY37SMbEhbp1TQdtyylWaosYPXpRb0JTfXUdKV37pdSqu5S3S5p1ttn+WN+fzxiG9O1/pHljzdDU9Uxp/MeEJgAAYBmCUxOWW2Qe3npMuwQlRtv/0Nb6OvHo1nrruqHqFFuik/e8prCn+kjvXCNlrTYPcB0wTrrxe+nKeVLacXppcYY++2m3IsKcevaK/hUVKtRPi87SpM/MMeHeUunDO6QPbpZKi+r2+Z4y6X9/khbeZ74/+DrpstelyLiGWzMAAEAtXFYvANZxF5ZKkuJDNSDs3aieq57TZ47X5Qw3X7TvUZKK+k5Qymk3S7Et/HddsWW//rXgF0nSvef0UM/2iZYsOWRExkmXzDarRp8/KK16Tdq1QRr9qpTY4dCfV+SW5o2Tfv9ckkM641Hp2OuDtWoAAIBDIjg1Ye4iMzglRIXQHwPDkDK+lpZOkzZ+Isksq5a17qlpRafrmd29pWWRejy9WOf0MT9lf36Jbn5jlTxeQ+f0aaerhnS0bv2hxOGQht8mte0tzZ8g7VgpvXCCeVhu2vCq98/ZJr1+qbR7vRQeI100Xep2VtCXDQAAUJ0QesWM+vK16iWEQpteWbG0bp703XPSrh/LLzqkrmdIQ2+SK224ri3zasObq7Vg/U7dMmeVduYUaeLwdN3+1mpl5RSpU8tYPXJhL/Y1BVrnk6Vrv5LevEratU6afa50+j+lIdeZ4UqSdqyW3hgt5e00h0pcMVdq18/KVQMAAFRCcGrCfBWn+MZcccrbIy2fIS17ScrfY14Lj5H6Xikde4O536ZcVHiYnr2yv/7+vw2atWSz/vHRT/rfuiyt2ZqtSJdTz17ZX3GRjfj3ws6apUkTP5X+O9kMuAvuMitQZ0+VMr4xK1Kl+VLrHubkvCTOzQIAAPbCq8QmzF9xaox7nHZtkL6bZk7JKz9/SfHtzCrGgLFSdLNqPy3M6dD95/RQ+6Ro/eOjn7Rma7Yk6cFzj1H3tk17PH2Di4iRLnzJrCR9eq+0dq60bZl0YLNkeKVOJ5ltfFHsLwMAAPZDcGrCfMMhGk2rntdrDg1Y+qy06cuK6+36S0NvMqe4hdX+tTgcDl1zfCclJ0bpvvd/1Hl92mn0ICocQeFwmN+rNr3MIRD7N5nX+4+VRj1ep+8fAACAFQhOTZi7vOJk+1a90kJpzZvm/qW95uQ7OZxSt7PNF+EpQyr2ytTDuX3a6exebeV0sqcp6NKPl679Wvr8ISllsDRo0mF9DwEAAILF5q+Y0VAMw1Cuf6qeTX/Kn7tT+uElcw9T4X7zWkS81P9qsyWvWdoRPwWhyUJJKdJFL1m9CgAAgDohODVRRaVelXoMSTZs1ctaa+5fWjffPEBVkpI6SkOul/pdLUWxFwkAAADBRXBqonwT9ZwOKTYizOLVyNy/9OsCMzBtXlRxPWWI2Y539CgpjD+uAAAAsAavRJuoXP8o8nBrzy0qzpNWvyF9/1zFoABHmHTM+dKxN0kdBli3NgAAAKAcwamJyin0HX5r4R+BbSuk1y+u2L8UmSgNHCcNvlZK7GDdugAAAIA/IDg1Uf7DbyMt2t+Ut1uae5UZmpqlS8feKPW9QoqMs2Y9AAAAQA0ITk2U//BbKypOnlLprbFS7g6pZVdp0ucMfAAAAICtOa1eAKzhP/zWilHkn/5Nylxijha/7A1CEwAAAGyP4NREuQ8aDhFUa96Uvn/evH3hi1LLo4L7/AAAAMBhIDg1UZa06u1YLf33VvP28X+Rup0VvOcGAAAAjgDBqYkKeqte/j5p7tVSWZF01EjpxCnBeV4AAAAgAAhOTZS7vOIUHxWEipOnTHp7gpSTaU7Qu/BFyckfPQAAADQevHptonwH4CZEB6Hi9MVD0qavpPAYcxhEdLOGf04AAAAggAhOTVRFq14DV5x+fEf69j/m7fOelZJ7NOzzAQAAAA2A4NRE+Vr1GnSP064N0vs3m7eHTZZ6XthwzwUAAAA0IIJTE9XgrXqF2dLcK6XSfCn9BOmU+xvmeQAAAIAgIDg1Ue7CBhwO4fVK71wj7d8kJXaULp4phQVx7DkAAAAQYASnJqjU41VhqUdSA7Xqff2otPFTyRUljX5Vim0R+OcAAAAAgojg1AT5Dr+VGqDi9PNH0tf/Mm+f8x+pXd/APj4AAABgAYJTE+SbqBcTESZXWAD/COzdKL1zrXl78HVSn8sC99gAAACAhQhOTVBuQ0zUK86V3rxSKsmVOg6TTv9H4B4bAAAAsBjBqQly+yfqBahNzzCk926Q9v4ixbeVLpklhQXhYF0AAAAgSAhOTZCvVS8+UBWnxU9IP/1XcoZLl74qxScH5nEBAAAAmyA4NUEVrXoBqDj99pn0+d/N22f9W0oZdOSPCQAAANgMwakJcgfq8Nv9GdL8iZIMqf9YaeD4I18cAAAAYEMEpyaoolXvCCpOJfnS3Kukomyp/UCz2gQAAACEKIJTE+Q+0ql6hiF9MFna9aMU20q69BXJFRnAFQIAAAD2QnBqgo64Ve+7adKP8yWnS7pktpTYPoCrAwAAAOyH4NQEuQvNitNhteplfCN9eq95e+Q/pLTjArgyAAAAwJ4ITk1Qrq/iVN9Wveyt0rzxkuGRel8mDbmuAVYHAAAA2A/BqQny73GqT6teaZH01tVSwV6pTW/pnKmSw9EwCwQAAABshuDUBNV7qp5hSB/eIe1YJUU3l0a/JoVHN+AKAQAAAHshODVB9W7VWz5dWv2a5HBKF8+QmqU24OoAAAAA+yE4NTFer6HcYl+rXh0qTpnfSx//1bx9yv1S55MacHUAAACAPRGcmpi8kjIZhnm71oqTO8vc1+QtlXqcLx13a4OvDwAAALAjglMTk1s+GCIizKlIVw3f/rISad5YKW+X1Kq7dN6zDIMAAABAk0VwamIOHgzhqCkILfirtPV7KTJRuux1KTIuSCsEAAAA7Ifg1MTk1mUU+arXzIEQckgXvSS16BycxQEAAAA2RXBqYnwVp4RDjSLfvlL63+3m7ROnSF1PD9LKAAAAAPsiODUx7iJfq141Fae8PdLcqyVPsXT0WdLxfw7y6gAAAAB7Ijg1MRWten+oOHnKpPnjJfc2qUUX6YLnJSd/PAAAAACJ4NTkVLTq/aHitPA+afMiKSJOuuwNKSrRgtUBAAAA9kRwamIqWvUOqjitnSd996x5+/znpFZHW7AyAAAAwL4ITk2Mv1XPV3HauU764Bbz9vDbpR7nWrQyAAAAwL4ITk2Mr+KUEB0uFWZLb14plRVKnU+RTv6btYsDAAAAbIrg1MS4C82KU3yUS/r5Qyl7i5SYIl30suQMs3h1AAAAgD0RnJqY3KKDhkPk7zEvph4nxTS3cFUAAACAvRGcmhi3fxx5uFSUY15kgh4AAABQI4JTE+MbRx4f5SI4AQAAAHVEcGpCDMM46ABcKk4AAABAXRGcmpDiMq9KPF5JUgIVJwAAAKDOCE5NiK9Nz+GQYiMITgAAAEBdEZyaEN9giPhIl5xOB8EJAAAAqCOCUxNS6fBbieAEAAAA1BHBqQmpmKhHcAIAAADqg+DUhPgn6kW5pLJiqazQ/ADBCQAAAKgRwakJqdSqV+Su+EBkgkUrAgAAABoHglMT4qs4VTr8NiJeCnNZuCoAAADA/iwPTtOmTVN6erqioqI0YMAALVq0qMb7v/766+rTp49iYmLUtm1bjR8/Xvv27QvSahs33x6nhCgOvwUAAADqw9LgNHfuXN1222265557tGrVKo0YMUJnnnmmMjMzq73/4sWLNWbMGE2cOFHr16/XvHnztGzZMk2aNCnIK2+c/K16US6pKNu8GJ1k2XoAAACAxsLS4PTEE09o4sSJmjRpkrp3766pU6cqJSVFzz33XLX3/+6775SWlqbJkycrPT1dw4cP13XXXafly5cHeeWNk384RDQVJwAAAKA+LAtOJSUlWrFihUaOHFnp+siRI7VkyZJqP2fYsGHatm2bPvroIxmGoV27dmn+/PkaNWrUIZ+nuLhYbre70ltTRaseAAAAcHgsC0579+6Vx+NRcnJypevJycnauXNntZ8zbNgwvf766xo9erQiIiLUpk0bJSUl6emnnz7k8zzyyCNKTEz0v6WkpAT062hM3NUNhyA4AQAAALWyfDiEw+Go9L5hGFWu+WzYsEGTJ0/WfffdpxUrVmjBggXKyMjQ9ddff8jHnzJlinJycvxvW7duDej6G5PcSuPICU4AAABAXVk2h7ply5YKCwurUl3avXt3lSqUzyOPPKLjjjtOf/7znyVJvXv3VmxsrEaMGKGHH35Ybdu2rfI5kZGRioyMDPwX0Ai5C30H4BKcAAAAgPqwrOIUERGhAQMGaOHChZWuL1y4UMOGDav2cwoKCuR0Vl5yWFiYJLNShZr5purRqgcAAADUj6WterfffrtefvllzZgxQz/99JP+9Kc/KTMz0996N2XKFI0ZM8Z//3POOUfvvPOOnnvuOW3atEnffvutJk+erMGDB6tdu3ZWfRmNQpnHq4ISjyRa9QAAAID6sqxVT5JGjx6tffv26aGHHlJWVpZ69uypjz76SKmpqZKkrKysSmc6jRs3Trm5uXrmmWd0xx13KCkpSSeffLL+9a9/WfUlNBq+UeQSFScAAACgvhxGE+txc7vdSkxMVE5OjhISEqxeTtBs2ZevE/79laLDw/TT38+Qnhks7f1FGvtfKf14q5cHAAAABF19soHlU/UQHBWH35YXGak4AQAAAHVGcGoiKh1+KxGcAAAAgHogODURlSbqlRVLZYXmBwhOAAAAQK0ITk2E29+qFy4VuSs+ENl09nkBAAAAh4vg1ERUatXztelFJkjOMAtXBQAAADQOBKcmwldxYhQ5AAAAUH8EpyYit3yPk9mql21eJDgBAAAAdUJwaiLcheV7nA5u1SM4AQAAAHVCcGoicg+eqkdwAgAAAOqF4NREuCu16hGcAAAAgPogODURFa16VJwAAACA+iI4NRG5xb5WPSpOAAAAQH0RnJoIX8UpMZqKEwAAAFBfBKcmwDCMg4ZDUHECAAAA6ovg1ATkl3jkNczbjCMHAAAA6o/g1AS4C81qU3iYQ1HhToITAAAAUE8EpybAfVCbnsPhIDgBAAAA9URwagJyiw4aRS4RnAAAAIB6Ijg1Ab5WvYTocKmsWCorND9AcAIAAADqhODUBFS06rmkInfFByITLFoRAAAA0LgQnJqAila9gybqRSZIzjALVwUAAAA0HgSnJsDfqscocgAAAOCwEJyaAHd5xcls1cs2LxKcAAAAgDojODUBuUUHDYeg4gQAAADUG8GpCXAXHjSOnOAEAAAA1BvBqQk4+ABcWvUAAACA+iM4NQG+PU606gEAAACHh+DUBOT6p+rRqgcAAAAcDoJTE1AxVY+KEwAAAHA4CE5NgNs/VY+KEwAAAHA4CE4hrqjUo5IyryT2OAEAAACHi+AU4nLL2/QcDikugooTAAAAcDgITiHO16YXF+mS0+kgOAEAAACHgeAU4tz+iXrh5gWCEwAAAFBvBKcQl+ufqOeSSouksiLzAwQnAAAAoM4ITiGuYqJeuFTsrvhAZIJFKwIAAAAaH4JTiHMXmhWnSoffRiZIzjALVwUAAAA0LgSnEJdbdNAeJ/Y3AQAAAIeF4BTiKrXqFWWbFwlOAAAAQL0QnEKcr1UvPooznAAAAIDDRXAKcbTqAQAAAEeO4BTi3OXjyBOiD644JVm3IAAAAKARIjiFON8BuPFUnAAAAIDDRnAKcb4DcGnVAwAAAA4fwSnEVUzVYzgEAAAAcLgITiGOVj0AAADgyBGcQliZx6v8Eo8kKYFx5AAAAMBhIziFsLziMv9tKk4AAADA4SM4hTDfYIiocKciXE6CEwAAAHCYCE4hLKfwoMNvJYITAAAAcJgITiGsYqJeuFRaJJUVmR8gOAEAAAD1QnAKYb5Wvfgol1TsLr/qkCITrFsUAAAA0AgRnEKY++BWPV+bXmSC5OTbDgAAANQHr6BDmLu84pQQzUQ9AAAA4EgQnEJYbpHv8FuXVJRtXiQ4AQAAAPVGcAph7sLyihNnOAEAAABHhOAUwiqm6rkITgAAAMARIDiFsIpWPSpOAAAAwJEgOIWwilY9Kk4AAADAkSA4hTB/qx4VJwAAAOCIEJxCWK5/HDkVJwAAAOBIEJxCGBUnAAAAIDAITiHKMAx/xYnhEAAAAMCRITiFqIISjzxeQxKtegAAAMCRIjiFKF+bnsvpUHR4GMEJAAAAOAIEpxDlG0UeH+WSw+EgOAEAAABHgOAUonyH3yZEh0ulRVJZkfkBghMAAABQbwSnEFVpol6xu/yqQ4pMsG5RAAAAQCNFcApRFRP1DhoMEZkgOfmWAwAAAPXFq+gQ5S7kDCcAAAAgUAhOIcpdXnEyR5FnmxcJTgAAAMBhITiFKN8eJw6/BQAAAI4cwSlE+caR06oHAAAAHDmCU4jyT9WLdhGcAAAAgCNEcApRFVP1qDgBAAAAR4rgFKIqpupRcQIAAACOFMEpRFW06lFxAgAAAI4UwSlEVToAtzDbvEhwAgAAAA4LwSlEcQAuAAAAEDgEpxBUXOZRcZlXEsEJAAAACASCUwjytelJUhzDIQAAAIAjRnAKQb42vfhIl8KcDoITAAAAcIQITiHIffBgiNIiyVNsfoDgBAAAABwWglMIyq1uFLkcUmSCdYsCAAAAGjGCUwhyF5oVp0qDISITJCffbgAAAOBw8Eo6BPkOv41nMAQAAAAQEASnEFRtqx7BCQAAADhsBKcQVNGq55KKss2LBCcAAADgsBGcQlCuv1WPihMAAAAQCASnEOQbR54QzR4nAAAAIBAITiHIdwBuAhUnAAAAICAITiEo138ALsEJAAAACASCUwhy+6fq0aoHAAAABALBKQRV26oXnWTdggAAAIBGjuAUgipa9ag4AQAAAIFAcAoxHq+h3GLfVD32OAEAAACBQHAKMXnl1SaJihMAAAAQKASnEOMbDBHpcirSFUZwAgAAAAKA4BRiKibqhUulRZKn2PwAwQkAAAA4bASnEOMuLN/fdHCbnhxSRLx1iwIAAAAaOYJTiMktrzhVPvw2QXLyrQYAAAAOF6+mQ4y7iIl6AAAAQKARnEKM7/BbJuoBAAAAgUNwCjG+w28TosKlomzzYlSSZesBAAAAQgHBKcRUTNWj4gQAAAAECsEpxPha9RKi2OMEAAAABArBKcRUtOpRcQIAAAACheAUYiodgEtwAgAAAAKC4BRifBUnpuoBAAAAgUNwCjH+ihN7nAAAAICAITiFGP9wCFr1AAAAgIAhOIUQwzBo1QMAAAAaAMEphBSWelTmNSTRqgcAAAAEEsEphLgLzWpTmNOhmIgwghMAAAAQIASnEJJbPhgiPsolR1mx5Ck2P0BwAgAAAI4IwSmEVDtRTw4pIt66RQEAAAAhgOAUQnytegnRBw+GSJCcfJsBAACAI8Er6hDiqzjFRzIYAgAAAAgkglMIcRdVV3EiOAEAAABHiuAUQvyH30aFS0XZ5sWoJMvWAwAAAIQKglMIqTj8llY9AAAAIJAITiHEP1WPVj0AAAAgoAhOIaRyqx7BCQAAAAgUglMIqWjVo+IEAAAABBLBKYRUtOpRcQIAAAACieAUQnytelScAAAAgMAiOIUQX6see5wAAACAwCI4hRBfq14irXoAAABAQBGcQkRJmVdFpV5JtOoBAAAAgUZwChG55dUmSYqLCJOKss13CE4AAADAEbM8OE2bNk3p6emKiorSgAEDtGjRohrvX1xcrHvuuUepqamKjIxU586dNWPGjCCt1r7c5fub4iJdchklkqfE/ADBCQAAADhiLiuffO7cubrttts0bdo0HXfccXrhhRd05plnasOGDerYsWO1n3PppZdq165dmj59urp06aLdu3errKwsyCu3H1/FqVKbnhxSRLx1iwIAAABChKXB6YknntDEiRM1adIkSdLUqVP1ySef6LnnntMjjzxS5f4LFizQ119/rU2bNql58+aSpLS0tGAu2bbchdVN1EuQnJYXFQEAAIBGz7JX1SUlJVqxYoVGjhxZ6frIkSO1ZMmSaj/ngw8+0MCBA/XYY4+pffv26tq1q+68804VFhYe8nmKi4vldrsrvYWiisNvGQwBAAAABJplFae9e/fK4/EoOTm50vXk5GTt3Lmz2s/ZtGmTFi9erKioKL377rvau3evbrzxRu3fv/+Q+5weeeQRPfjggwFfv91UtOqFMxgCAAAACDDL+7gcDkel9w3DqHLNx+v1yuFw6PXXX9fgwYN11lln6YknntCsWbMOWXWaMmWKcnJy/G9bt24N+NdgBxWtegdXnJKsWxAAAAAQQiyrOLVs2VJhYWFVqku7d++uUoXyadu2rdq3b6/ExIpKSvfu3WUYhrZt26ajjjqqyudERkYqMjIysIu3oYpWPSpOAAAAQKBZVnGKiIjQgAEDtHDhwkrXFy5cqGHDhlX7Occdd5x27NihvLw8/7Vff/1VTqdTHTp0aND12l1u+ThyDr8FAAAAAs/SVr3bb79dL7/8smbMmKGffvpJf/rTn5SZmanrr79ektlmN2bMGP/9r7jiCrVo0ULjx4/Xhg0b9M033+jPf/6zJkyYoOjoaKu+DFtwF5ZXnCpN1SM4AQAAAIFg6Tjy0aNHa9++fXrooYeUlZWlnj176qOPPlJqaqokKSsrS5mZmf77x8XFaeHChbrllls0cOBAtWjRQpdeeqkefvhhq74E26jUqucmOAEAAACBZGlwkqQbb7xRN954Y7UfmzVrVpVr3bp1q9LeB8lNqx4AAADQYCyfqofAoFUPAAAAaDgEpxDhGw5hTtUjOAEAAACBRHAKEW7/Abi06gEAAACBRnAKAV6vobxi3wG44RyACwAAAARYvYNTWlqaHnrooUrT7mCt3OIyGYZ5Oz4yjIoTAAAAEGD1Dk533HGH3n//fXXq1EmnnXaa3nzzTRUXFzfE2lBHueVtehEup6IcpZKnxPwAwQkAAAAIiHoHp1tuuUUrVqzQihUr1KNHD02ePFlt27bVzTffrJUrVzbEGlELd2E1bXoOpxQRZ+GqAAAAgNBx2Huc+vTpo//85z/avn277r//fr388ssaNGiQ+vTpoxkzZsjw9Y6hwfkqTgkHD4aITJCcbGEDAAAAAuGwD8AtLS3Vu+++q5kzZ2rhwoU69thjNXHiRO3YsUP33HOPPvvsM73xxhuBXCsOwX/4LaPIAQAAgAZR7+C0cuVKzZw5U3PmzFFYWJiuvvpqPfnkk+rWrZv/PiNHjtTxxx8f0IXi0CoOv2UUOQAAANAQ6h2cBg0apNNOO03PPfeczj//fIWHh1e5T48ePXTZZZcFZIGoXUWrHhUnAAAAoCHUOzht2rRJqampNd4nNjZWM2fOPOxFoX58rXoJ0S6pKNu8SHACAAAAAqbe0wN2796t77//vsr177//XsuXLw/IolA/Fa16HH4LAAAANIR6B6ebbrpJW7durXJ9+/btuummmwKyKNRPrm84BHucAAAAgAZR7+C0YcMG9e/fv8r1fv36acOGDQFZFOrH7dvjxFQ9AAAAoEHUOzhFRkZq165dVa5nZWXJ5Trs6eY4Am6GQwAAAAANqt7B6bTTTtOUKVOUk5Pjv5adna27775bp512WkAXh7qhVQ8AAABoWPUuET3++OM6/vjjlZqaqn79+kmSVq9ereTkZL366qsBXyBq5x8OQaseAAAA0CDqHZzat2+vtWvX6vXXX9eaNWsUHR2t8ePH6/LLL6/2TCc0PP84clr1AAAAgAZxWJuSYmNjde211wZ6LTgMhmH4D8ClVQ8AAABoGIc9zWHDhg3KzMxUSUlJpevnnnvuES8KdVdU6lWpx5AkJRCcAAAAgAZR7+C0adMmXXDBBVq3bp0cDocMw3zR7nA4JEkejyewK0SNfBP1nA4p1lkqecqDLMEJAAAACJh6T9W79dZblZ6erl27dikmJkbr16/XN998o4EDB+qrr75qgCWiJhVteuFyFLvNiw6nFBFn4aoAAACA0FLvitPSpUv1xRdfqFWrVnI6nXI6nRo+fLgeeeQRTZ48WatWrWqIdeIQcgrLB0NEH9SmF5kgOeudiQEAAAAcQr1fXXs8HsXFmdWMli1baseOHZKk1NRU/fLLL4FdHWrF4bcAAABAw6t3xalnz55au3atOnXqpCFDhuixxx5TRESEXnzxRXXq1Kkh1ogacPgtAAAA0PDqHZz+9re/KT8/X5L08MMP6+yzz9aIESPUokULzZ07N+ALRM38h99ScQIAAAAaTL2D0+mnn+6/3alTJ23YsEH79+9Xs2bN/JP1EDy+ilNCdLhUlG1eJDgBAAAAAVWvPU5lZWVyuVz68ccfK11v3rw5ocki7moPv02ybkEAAABACKpXcHK5XEpNTeWsJhuhVQ8AAABoePWeqve3v/1NU6ZM0f79+xtiPagnhkMAAAAADa/ee5yeeuop/fbbb2rXrp1SU1MVGxtb6eMrV64M2OJQO/848uhwaSfBCQAAAGgI9Q5O559/fgMsA4eLVj0AAACg4dU7ON1///0NsQ4cJv9UPVr1AAAAgAZT7z1OsJdKrXoEJwAAAKBB1Lvi5HQ6axw9zsS94HIX+ipOBCcAAACgodQ7OL377ruV3i8tLdWqVas0e/ZsPfjggwFbGGpX6vGqsNQMqvGRYQQnAAAAoIHUOzidd955Va5dfPHFOuaYYzR37lxNnDgxIAtD7Xz7myQp3lUmeUrMdwhOAAAAQEAFbI/TkCFD9NlnnwXq4VAHvol6sRFhcpXmmhcdTikizsJVAQAAAKEnIMGpsLBQTz/9tDp06BCIh0MdVRx+e9D+psgEycnMDwAAACCQ6t2q16xZs0rDIQzDUG5urmJiYvTaa68FdHGoWcVEPUaRAwAAAA2p3sHpySefrBScnE6nWrVqpSFDhqhZs2YBXRxqVunw28Js8yLBCQAAAAi4egencePGNcAycDgqWvWoOAEAAAANqd6bYWbOnKl58+ZVuT5v3jzNnj07IItC3VQ+/DbbvEhwAgAAAAKu3sHp0UcfVcuWLatcb926tf75z38GZFGom0qtev6KU5J1CwIAAABCVL2D05YtW5Senl7lempqqjIzMwOyKNSNm1Y9AAAAICjqHZxat26ttWvXVrm+Zs0atWjRIiCLQt1UbtUjOAEAAAANpd7B6bLLLtPkyZP15ZdfyuPxyOPx6IsvvtCtt96qyy67rCHWiEPwDYeo3KpHcAIAAAACrd5T9R5++GFt2bJFp5xyilwu89O9Xq/GjBnDHqcg8+1xolUPAAAAaFj1Dk4RERGaO3euHn74Ya1evVrR0dHq1auXUlNTG2J9qIFvjxOtegAAAEDDqndw8jnqqKN01FFHBXItqKdc3x4nKk4AAABAg6r3HqeLL75Yjz76aJXr//73v3XJJZcEZFGom4pWPSpOAAAAQEOqd3D6+uuvNWrUqCrXzzjjDH3zzTcBWRRq5/Uayi32DYcIIzgBAAAADajewSkvL08RERFVroeHh8vtdgdkUahdfkmZDMO8neAqk7xm9YngBAAAAARevYNTz549NXfu3CrX33zzTfXo0SMgi0LtfIMhIsKciizLNS86nFJkvIWrAgAAAEJTvYdD3Hvvvbrooov0+++/6+STT5Ykff7553rjjTc0f/78gC8Q1fPtb0qIdslRVF7pi0qUHA4LVwUAAACEpnoHp3PPPVfvvfee/vnPf2r+/PmKjo5Wnz599MUXXyghIaEh1ohq+A6/ZTAEAAAA0PAOaxz5qFGj/AMisrOz9frrr+u2227TmjVr5PF4ArpAVM9fcWIUOQAAANDg6r3HyeeLL77QVVddpXbt2umZZ57RWWedpeXLlwdybaiB23eGE4ffAgAAAA2uXhWnbdu2adasWZoxY4by8/N16aWXqrS0VG+//TaDIYKsolXPJRVlmxcJTgAAAECDqHPF6ayzzlKPHj20YcMGPf3009qxY4eefvrphlwbalDRqkfFCQAAAGhoda44ffrpp5o8ebJuuOEGHXXUUQ25JtRB9a16SdYtCAAAAAhhda44LVq0SLm5uRo4cKCGDBmiZ555Rnv27GnItaEG/la9SIZDAAAAAA2tzsFp6NCheumll5SVlaXrrrtOb775ptq3by+v16uFCxcqNze3IdeJP2A4BAAAABA89Z6qFxMTowkTJmjx4sVat26d7rjjDj366KNq3bq1zj333IZYI6rhLjQrTgnRVJwAAACAhnbY48gl6eijj9Zjjz2mbdu2ac6cOYFaE+ogt7ziFB9JxQkAAABoaEcUnHzCwsJ0/vnn64MPPgjEw6EO3EW+ihPBCQAAAGhoAQlOCL5c/x4nWvUAAACAhkZwaoQMw/DvcWKqHgAAANDwCE6NUHGZVyUeryQpwVUqec3qE8EJAAAAaBgEp0bIN4rc6ZBivfnmRYdTioizcFUAAABA6CI4NUK+Nr24SJecxW7zYlSi5HBYuCoAAAAgdBGcGiEOvwUAAACCi+DUCOX6RpFHEZwAAACAYCA4NULuwvLDb6OYqAcAAAAEA8GpEarcqpdtXiQ4AQAAAA2G4NQI+Vr1qDgBAAAAwUFwaoR8rXqV9zglWbcgAAAAIMQRnBohpuoBAAAAwUVwaoQqpurRqgcAAAAEA8GpEaq+VY/gBAAAADQUglMj5PZVnKKpOAEAAADBQHBqhHKLfOc4UXECAAAAgoHg1Ai5C317nAhOAAAAQDAQnBoh/1S9qDCCEwAAABAEBKdGpszjVUGJR5IU7yqTvGaIIjgBAAAADYfg1Mj4RpFLUrzyzRsOpxQRZ9GKAAAAgNBHcGpkfMEpJiJM4SW55sWoRMnhsHBVAAAAQGgjODUybv9EPUaRAwAAAMFCcGpkOPwWAAAACD6CUyNTcfgtwQkAAAAIFoJTI1O5VS/bvEhwAgAAABoUwamRoVUPAAAACD6CUyOT62/VO7jilGTZegAAAICmgODUyFS06lFxAgAAAIKF4NTIuAvLK04EJwAAACBoCE6NTG55xcls1SM4AQAAAMFAcGpkaNUDAAAAgo/g1MhUtOpRcQIAAACCheDUyOQWU3ECAAAAgo3g1Mj4Kk6JUWEEJwAAACBICE6NiGEYFcMhXKWS1wxRBCcAAACgYRGcGpH8Eo+8hnk7XgXmDYdTioizblEAAABAE0BwakTchWa1KTzMoaiyXPNiVKLkcFi4KgAAACD0EZwakdyiisNvHcVu8yJtegAAAECDIzg1IhVnOB08ijzJugUBAAAATQTBqRHxteolRDOKHAAAAAgmglMjcnCrHsEJAAAACB6CUyNSuVUv27xIcAIAAAAaHMGpEfG36lFxAgAAAIKK4NSI+Fv1ohkOAQAAAAQTwakRqWjVo+IEAAAABBPBqRFxF/qGQ7gITgAAAEAQEZwaEV/FiXHkAAAAQHARnBoRd/keJ1r1AAAAgOAiODUiuf6perTqAQAAAMFEcGpE3EXscQIAAACsQHBqRPx7nFylktcMUQQnAAAAoOERnBqJolKPSsq8kqQER4F50REmRcRauCoAAACgaSA4NRK+w28dDinWk2dejEo0LwAAAABoUASnRsLXphcX6ZKzxG1epE0PAAAACAqCUyPh9k/UYxQ5AAAAEGwEp0Yi13+GExP1AAAAgGAjODUS/ol60VScAAAAgGAjODUSuf4znMKlomzzIsEJAAAACAqCUyNRsceJVj0AAAAg2AhOjUT1rXpJ1i0IAAAAaEIITo1ERaseFScAAAAg2AhOjYSvVS+eceQAAABA0BGcGgm3r+IUTcUJAAAACDaCUyORW8QBuAAAAIBVLA9O06ZNU3p6uqKiojRgwAAtWrSoTp/37bffyuVyqW/fvg27QJtwF/oOwCU4AQAAAMFmaXCaO3eubrvtNt1zzz1atWqVRowYoTPPPFOZmZk1fl5OTo7GjBmjU045JUgrtZ5/ql5UGMEJAAAACDJLg9MTTzyhiRMnatKkSerevbumTp2qlJQUPffcczV+3nXXXacrrrhCQ4cODdJKreebqpfoKpO85m2CEwAAABAclgWnkpISrVixQiNHjqx0feTIkVqyZMkhP2/mzJn6/fffdf/999fpeYqLi+V2uyu9NTZlHq/yisuHQyjfvOgIkyJiLVwVAAAA0HRYFpz27t0rj8ej5OTkSteTk5O1c+fOaj9n48aN+utf/6rXX39dLperTs/zyCOPKDEx0f+WkpJyxGsPNl9okqQ4X3CKSpQcDotWBAAAADQtlg+HcPzhxb9hGFWuSZLH49EVV1yhBx98UF27dq3z40+ZMkU5OTn+t61btx7xmoPN16YXHR6m8NJc8yJtegAAAEDQ1K1s0wBatmypsLCwKtWl3bt3V6lCSVJubq6WL1+uVatW6eabb5Ykeb1eGYYhl8ulTz/9VCeffHKVz4uMjFRkZGTDfBFBkuM//JYznAAAAAArWFZxioiI0IABA7Rw4cJK1xcuXKhhw4ZVuX9CQoLWrVun1atX+9+uv/56HX300Vq9erWGDBkSrKUHnX+iXjSjyAEAAAArWFZxkqTbb79dV199tQYOHKihQ4fqxRdfVGZmpq6//npJZpvd9u3b9corr8jpdKpnz56VPr9169aKioqqcj3U+Fr1Eqg4AQAAAJawNDiNHj1a+/bt00MPPaSsrCz17NlTH330kVJTUyVJWVlZtZ7p1BS4/a164VJRtnmR4AQAAAAEjcMwDMPqRQST2+1WYmKicnJylJCQYPVy6mT64gz9/X8bdE6fdnq6xdvSkqeloTdLp//D6qUBAAAAjVZ9soHlU/VQu9yi6oZDJFm3IAAAAKCJITg1Au5C3x4nhkMAAAAAViA4NQK5/ql6DIcAAAAArEBwagTcRQcNhyjMNi8SnAAAAICgITg1AhWtelScAAAAACsQnBqB3GIOwAUAAACsRHBqBPwVp8gwghMAAABgAYJTI+Db45ToKpUMj3mR4AQAAAAEDcHJ5gzDUG6RWXFKdBSYFx1hUkSshasCAAAAmhaCk80VlHjk8RqSpDgj37wYlSg5HBauCgAAAGhaCE4252vTczkdivLkmhdp0wMAAACCiuBkc742vYTocDmK3OZFghMAAAAQVAQnm3MX+g6/5QwnAAAAwCoEJ5vzteolRHGGEwAAAGAVgpPNVbTqUXECAAAArEJwsjl/q15kuFSUbV6MTrJsPQAAAEBTRHCyOTcVJwAAAMByBCebq36PU5J1CwIAAACaIIKTzbkLzYpTPMMhAAAAAMsQnGzOX3GiVQ8AAACwDMHJ5vxT9ag4AQAAAJYhONkcB+ACAAAA1iM42VyufzgEwQkAAACwCsHJ5nzjyBNdJZLhMS8SnAAAAICgIjjZnK9VL1EF5gWnSwqPsXBFAAAAQNNDcLKx4jKPisu8kqR4R3lwikqUHA4LVwUAAAA0PQQnG/NN1JOkGG+eeYM2PQAAACDoCE425p+oF+lSWLHbvEhwAgAAAIKO4GRj/jOcojnDCQAAALASwcnG3EWc4QQAAADYAcHJxtyF5RWnKCpOAAAAgJUITjbmP/w22iUVZZsXCU4AAABA0BGcbKyiVY+KEwAAAGAlgpONVbTqHbzHKcm6BQEAAABNFMHJxipa9ag4AQAAAFYiONmYu3wcOVP1AAAAAGsRnGzMdwAuU/UAAAAAaxGcbIwDcAEAAAB7IDjZmH+qXmQYwQkAAACwEMHJxnytekmuUsnwmBcJTgAAAEDQEZxszNeql+gsMC84XVJ4jIUrAgAAAJomgpNNebyGcovLp+oZ+ebFqETJ4bBwVQAAAEDTRHCyqbzy0CRJsQcHJwAAAABBR3CyKd/+pqhwpyJKc82LBCcAAADAEgQnm/JP1OMMJwAAAMByBCeb8p/hFOUiOAEAAAAWIzjZlK9Vj4oTAAAAYD2Ck025fRWn6HCpKNu8SHACAAAALEFwsqnc8j1OtOoBAAAA1iM42ZS7sPwMp0qteknWLQgAAABowghONuWbqpcQTcUJAAAAsBrByaYqWvUYDgEAAABYjeBkU75WPfY4AQAAANYjONlURaseFScAAADAagQnm/IfgBtJxQkAAACwGsHJpnwVp0RXsWR4zIsEJwAAAMASBCebcheawSnJWWhecLqk8BgLVwQAAAA0XQQnGzIMo6JVT/nmxahEyeGwcFUAAABA00VwsqHCUo/KvIYkKdY4KDgBAAAAsATByYZ8o8jDnA5FleWaFwlOAAAAgGUITjZUcfitS45it3mR4AQAAABYhuBkQ76JevFRnOEEAAAA2AHByYbcvsEQ0ZzhBAAAANgBwcmGfKPIE6g4AQAAALZAcLIhX8UpPsolFWWbFwlOAAAAgGUITjZUMRzi4IpTknULAgAAAJo4gpMN+caRJ0QTnAAAAAA7IDjZUMVUPYZDAAAAAHZAcLKhXN9UPYZDAAAAALZAcLIh31Q9Kk4AAACAPRCcbMjtHw5BcAIAAADsgOBkQ75WvSRXiWR4zYsEJwAAAMAyBCcb8rXqJTkKzAvOcCk82sIVAQAAAE0bwcmGfK16ib7gFJUoORwWrggAAABo2ghONlNS5lVRqdmeF2fkmxdp0wMAAAAsRXCymdzyapMkRXvzzBsEJwAAAMBSBCebcZcPhoiLdCmsxG1eJDgBAAAAliI42Uwuo8gBAAAA2yE42Yy70Kw4xUeFE5wAAAAAmyA42Yz/8NtoKk4AAACAXRCcbKaiVS9cKso2LxKcAAAAAEsRnGymolWPihMAAABgFwQnm/FXnKIP3uOUZN2CAAAAABCc7MY3jjyB4RAAAACAbRCcbMZdaFacaNUDAAAA7IPgZDP+ilM0FScAAADALghONuMfRx5JxQkAAACwC4KTzfha9ZJcxZLhNS8SnAAAAABLEZxsJre8VS/JWWhecIZL4dEWrggAAAAAwclm/K16jnzzQlSi5HBYuCIAAAAABCcb8XoN5RWXH4BrFJgXadMDAAAALEdwspG8kjIZhnk7xptn3iA4AQAAAJYjONmIbzBEhMupiLJc8yLBCQAAALAcwclG3IXlZzhFcYYTAAAAYCcEJxvJ9Q2GiOYMJwAAAMBOCE424i4fRR5PxQkAAACwFYKTjfj2OCVEuaSibPMiwQkAAACwHMHJRipa9ag4AQAAAHZCcLIRX6ueWXHyBack6xYEAAAAQBLByVYqWvWoOAEAAAB2QnCykVxfxYlWPQAAAMBWCE424i7f4xQfxThyAAAAwE4ITjbirzhFEpwAAAAAOyE42Yiv4tQsvFgyvOZFghMAAABgOYKTjfiGQyQ5C80LznApPNrCFQEAAACQCE624m/VU4F5ISpRcjgsXBEAAAAAieBkG4ZhVAyHUL55kTY9AAAAwBYITjZRVOpVqceQJMUaBCcAAADATghONpFbXm1yOqSoMrd5keAEAAAA2ALBySZ8bXoJ0eFyFBGcAAAAADshONlETqE5GILDbwEAAAD7ITjZhK9VLyEqnOAEAAAA2AzBySbcvlHkBCcAAADAdghONuE7/NZs1cs2LxKcAAAAAFsgONmE//Db6IMrTknWLQgAAACAH8HJJvyH3zIcAgAAALAdgpNN+Fr12OMEAAAA2A/BySaqbdWLTrJuQQAAAAD8CE424W/Vi3RKxRyACwAAANgJwckmfK16zVylkuE1LxKcAAAAAFsgONmEr1WvWViBeSEsQnJFWbgiAAAAAD4EJ5vwteolqjw4RSVKDoeFKwIAAADgQ3CyCf9wCOWbF2jTAwAAAGyD4GQDpR6vCko8kqRYg+AEAAAA2A3ByQZ81SZJivbkmTcITgAAAIBtEJxsILd8f1NsRJjCShhFDgAAANgNwckG3IXVHH5LcAIAAABsg+BkA/7Db6NcBCcAAADAhghONuBr1UuIouIEAAAA2BHByQYqt+plmxcJTgAAAIBtEJxsoPpWvSTrFgQAAACgEsuD07Rp05Senq6oqCgNGDBAixYtOuR933nnHZ122mlq1aqVEhISNHToUH3yySdBXG3DcPsOv6VVDwAAALAlS4PT3Llzddttt+mee+7RqlWrNGLECJ155pnKzMys9v7ffPONTjvtNH300UdasWKFTjrpJJ1zzjlatWpVkFceWO7C8j1O0QyHAAAAAOzIZeWTP/HEE5o4caImTZokSZo6dao++eQTPffcc3rkkUeq3H/q1KmV3v/nP/+p999/X//973/Vr1+/ap+juLhYxcXF/vfdbnfgvoAAqWjVo+IEAAAA2JFlFaeSkhKtWLFCI0eOrHR95MiRWrJkSZ0ew+v1Kjc3V82bNz/kfR555BElJib631JSUo5o3Q0h19eqFxkmFXMALgAAAGA3lgWnvXv3yuPxKDk5udL15ORk7dy5s06P8fjjjys/P1+XXnrpIe8zZcoU5eTk+N+2bt16ROtuCL5WvebhJZLhNS8SnAAAAADbsLRVT5IcDkel9w3DqHKtOnPmzNEDDzyg999/X61btz7k/SIjIxUZGXnE62xIvuEQSc5C80JYhOSKsnBFAAAAAA5mWXBq2bKlwsLCqlSXdu/eXaUK9Udz587VxIkTNW/ePJ166qkNucyg8B2Am+jINy9EJUp1CI8AAAAAgsOyVr2IiAgNGDBACxcurHR94cKFGjZs2CE/b86cORo3bpzeeOMNjRo1qqGXGRS+Vr14FZgXaNMDAAAAbMXSVr3bb79dV199tQYOHKihQ4fqxRdfVGZmpq6//npJ5v6k7du365VXXpFkhqYxY8boP//5j4499lh/tSo6OlqJiY0zbHi9hnKLzVa9OCPPvEhwAgAAAGzF0uA0evRo7du3Tw899JCysrLUs2dPffTRR0pNTZUkZWVlVTrT6YUXXlBZWZluuukm3XTTTf7rY8eO1axZs4K9/IDILymTYZi3Y7wHteoBAAAAsA3Lh0PceOONuvHGG6v92B/D0FdffdXwCwoy32CIiDCnIkpzzYsEJwAAAMBWLNvjBJNvMERCtIvDbwEAAACbIjhZzF1YfvhtVDjBCQAAALApgpPF/BP1oqg4AQAAAHZFcLJYbrGvVS9cKso2LxKcAAAAAFshOFms+la9JOsWBAAAAKAKgpPFaNUDAAAA7I/gZDHf4bdmqx7BCQAAALAjgpPFfBWnBCpOAAAAgG0RnCzmLj/HKT4yTCp2mxcJTgAAAICtEJwslltktuo1Dy+RDK95keAEAAAA2ArByWK+Vr1mzkLzQliE5IqycEUAAAAA/ojgZDF3ecUp0VFgXohKlBwOC1cEAAAA4I8IThbLLd/jlODINy/QpgcAAADYDsHJQoZh+A/AjTMOqjgBAAAAsBWCk4WKy7wq8ZgDIWK8eeZFghMAAABgOwQnC/lGkTsdUmRZrnmR4AQAAADYDsHJQr42vfiocDk5wwkAAACwLYKThfyH30a5pKIc8yLBCQAAALAdgpOFfIffJkSFS0XZ5kWCEwAAAGA7BCcL+Q6/peIEAAAA2BvByUL+ilN0+EHBKcm6BQEAAACoFsHJQgnRLvVJSVLnVnG06gEAAAA25rJ6AU3Z2b3b6eze7cx3ptKqBwAAANgVFSe7YI8TAAAAYFsEJzvweqUiznECAAAA7IrgZAcluZIM8zbBCQAAALAdgpMd+Nr0wiKl8Ghr1wIAAACgCoKTHbC/CQAAALA1gpMdEJwAAAAAWyM42QHBCQAAALA1gpMdEJwAAAAAWyM42QHBCQAAALA1gpMdEJwAAAAAWyM42QHBCQAAALA1gpMdEJwAAAAAWyM42QHBCQAAALA1gpMdEJwAAAAAWyM42UFRtvlrVJKVqwAAAABwCAQnO6DiBAAAANgawckOCE4AAACArRGcrOb1SkVu8zbBCQAAALAlgpPVSnIlGeZtghMAAABgSwQnq/na9MIipfAoa9cCAAAAoFoEJ6uxvwkAAACwPYKT1QhOAAAAgO0RnKxGcAIAAABsj+BkNYITAAAAYHsEJ6sRnAAAAADbIzhZjeAEAAAA2B7ByWoEJwAAAMD2CE5WIzgBAAAAtkdwshrBCQAAALA9gpPVCE4AAACA7RGcrFaUbf4alWTlKgAAAADUgOBkNSpOAAAAgO0RnKxGcAIAAABsj+BkJa9XKnKbtwlOAAAAgG0RnKxUkivJMG8TnAAAAADbIjhZydemFxYphUdZuxYAAAAAh0RwshL7mwAAAIBGgeBkJYITAAAA0CgQnKxEcAIAAAAaBYKTlQhOAAAAQKNAcLISwQkAAABoFFxWL6BJ63G+1LqHFJ1k9UoAAAAA1IDgZKWEtuYbAAAAAFujVQ8AAAAAakFwAgAAAIBaEJwAAAAAoBYEJwAAAACoBcEJAAAAAGpBcAIAAACAWhCcAAAAAKAWBCcAAAAAqAXBCQAAAABqQXACAAAAgFoQnAAAAACgFgQnAAAAAKgFwQkAAAD/3979h1ZV/3Ecf53lvN6NqzjX7o9061LT8kfCnOjMrIzGbjCYLrIyueuPbLSNxhAGlexGohRk/5iDpKRoUQxyDRrJ8scqQxrRcsgSI2ODNaZG7TppUvt8/xAv3O/Ek99v7bN77/MBF+4952x7Xfj4htc99xwBuKA4AQAAAIALihMAAAAAuKA4AQAAAIALihMAAAAAuKA4AQAAAIALihMAAAAAuKA4AQAAAIALihMAAAAAuKA4AQAAAIALihMAAAAAuKA4AQAAAIALihMAAAAAuKA4AQAAAIALihMAAAAAuKA4AQAAAICLWbYDTDdjjCRpbGzMchIAAAAANl3rBNc6wo1kXHGKx+OSpEWLFllOAgAAAGAmiMfjmjdv3g2PcczfqVdpZHJyUsPDw/L5fHIcx3YcjY2NadGiRRoaGtLcuXNtx0GGYf3BJtYfbGL9wSbW38xhjFE8HlcoFFJW1o2vYsq4M05ZWVlauHCh7RhTzJ07l384sIb1B5tYf7CJ9QebWH8zg9uZpmu4OQQAAAAAuKA4AQAAAIALipNlHo9HLS0t8ng8tqMgA7H+YBPrDzax/mAT6y81ZdzNIQAAAADgZnHGCQAAAABcUJwAAAAAwAXFCQAAAABcUJwAAAAAwAXFyaL9+/crHA5rzpw5WrVqlb788kvbkZABYrGYHMdJegQCAduxkMa++OILVVZWKhQKyXEcdXR0JO03xigWiykUCsnr9eqBBx7Q6dOn7YRF2nFbfzU1NVNm4tq1a+2ERVrZs2ePVq9eLZ/Pp4KCAlVVVenMmTNJxzD/UgvFyZKPPvpIjY2NevHFF/Xdd9/pvvvuUyQS0eDgoO1oyADLli3TL7/8knj09/fbjoQ0Nj4+rpUrV2rfvn3X3f/aa69p79692rdvn3p7exUIBPTwww8rHo9Pc1KkI7f1J0kVFRVJM7Grq2saEyJd9fT0qK6uTidPnlR3d7f+/PNPlZeXa3x8PHEM8y+1cDtyS9asWaOSkhK1trYmtt19992qqqrSnj17LCZDuovFYuro6FBfX5/tKMhAjuPo0KFDqqqqknT109ZQKKTGxkY1NzdLkiYmJuT3+/Xqq6/q2WeftZgW6ea/15909YzTb7/9NuVMFPBPO3/+vAoKCtTT06MNGzYw/1IQZ5wsuHLlir799luVl5cnbS8vL9fXX39tKRUyydmzZxUKhRQOh/X444/rp59+sh0JGercuXMaGRlJmocej0f3338/8xDT5vjx4yooKNDixYv1zDPPaHR01HYkpKHff/9dkpSXlyeJ+ZeKKE4WXLhwQX/99Zf8fn/Sdr/fr5GREUupkCnWrFmj9957T4cPH9aBAwc0MjKidevW6eLFi7ajIQNdm3nMQ9gSiUTU1tamo0eP6vXXX1dvb682btyoiYkJ29GQRowxampq0vr167V8+XJJzL9UNMt2gEzmOE7Sa2PMlG3APy0SiSSer1ixQmVlZbrjjjv07rvvqqmpyWIyZDLmIWzZsmVL4vny5ctVWlqqoqIiffrpp9q8ebPFZEgn9fX1OnXqlL766qsp+5h/qYMzThbk5+frlltumfJpwujo6JRPHYB/W25urlasWKGzZ8/ajoIMdO2OjsxDzBTBYFBFRUXMRPxjGhoa1NnZqWPHjmnhwoWJ7cy/1ENxsmD27NlatWqVuru7k7Z3d3dr3bp1llIhU01MTGhgYEDBYNB2FGSgcDisQCCQNA+vXLminp4e5iGsuHjxooaGhpiJ+L8ZY1RfX6+PP/5YR48eVTgcTtrP/Es9fFXPkqamJm3btk2lpaUqKyvTW2+9pcHBQdXW1tqOhjS3Y8cOVVZWqrCwUKOjo9q1a5fGxsYUjUZtR0OaunTpkn788cfE63Pnzqmvr095eXkqLCxUY2Ojdu/ereLiYhUXF2v37t3KycnRk08+aTE10sWN1l9eXp5isZiqq6sVDAb1888/64UXXlB+fr42bdpkMTXSQV1dnT744AN98skn8vl8iTNL8+bNk9frleM4zL9UY2DNm2++aYqKiszs2bNNSUmJ6enpsR0JGWDLli0mGAya7OxsEwqFzObNm83p06dtx0IaO3bsmJE05RGNRo0xxkxOTpqWlhYTCASMx+MxGzZsMP39/XZDI23caP1dvnzZlJeXm1tvvdVkZ2ebwsJCE41GzeDgoO3YSAPXW3eSzMGDBxPHMP9SC/+PEwAAAAC44BonAAAAAHBBcQIAAAAAFxQnAAAAAHBBcQIAAAAAFxQnAAAAAHBBcQIAAAAAFxQnAAAAAHBBcQIAAAAAFxQnAABuguM46ujosB0DADDNKE4AgJRRU1Mjx3GmPCoqKmxHAwCkuVm2AwAAcDMqKip08ODBpG0ej8dSGgBApuCMEwAgpXg8HgUCgaTH/PnzJV39Gl1ra6sikYi8Xq/C4bDa29uTfr6/v18bN26U1+vVggULtH37dl26dCnpmHfeeUfLli2Tx+NRMBhUfX190v4LFy5o06ZNysnJUXFxsTo7O//dNw0AsI7iBABIKzt37lR1dbW+//57PfXUU3riiSc0MDAgSbp8+bIqKio0f/589fb2qr29XZ9//nlSMWptbVVdXZ22b9+u/v5+dXZ26s4770z6Gy+//LIee+wxnTp1So888oi2bt2qX3/9dVrfJwBgejnGGGM7BAAAf0dNTY3ef/99zZkzJ2l7c3Ozdu7cKcdxVFtbq9bW1sS+tWvXqqSkRPv379eBAwfU3NysoaEh5ebmSpK6urpUWVmp4eFh+f1+3XbbbXr66ae1a9eu62ZwHEcvvfSSXnnlFUnS+Pi4fD6furq6uNYKANIY1zgBAFLKgw8+mFSMJCkvLy/xvKysLGlfWVmZ+vr6JEkDAwNauXJlojRJ0r333qvJyUmdOXNGjuNoeHhYDz300A0z3HPPPYnnubm58vl8Gh0d/V/fEgAgBVCcAAApJTc3d8pX59w4jiNJMsYknl/vGK/X+7d+X3Z29pSfnZycvKlMAIDUwjVOAIC0cvLkySmv77rrLknS0qVL1dfXp/Hx8cT+EydOKCsrS4sXL5bP59Ptt9+uI0eOTGtmAMDMxxknAEBKmZiY0MjISNK2WbNmKT8/X5LU3t6u0tJSrV+/Xm1tbfrmm2/09ttvS5K2bt2qlpYWRaNRxWIxnT9/Xg0NDdq2bZv8fr8kKRaLqba2VgUFBYpEIorH4zpx4oQaGhqm940CAGYUihMAIKV89tlnCgaDSduWLFmiH374QdLVO959+OGHeu655xQIBNTW1qalS5dKknJycnT48GE9//zzWr16tXJyclRdXa29e/cmflc0GtUff/yhN954Qzt27FB+fr4effTR6XuDAIAZibvqAQDShuM4OnTokKqqqmxHAQCkGa5xAgAAAAAXFCcAAAAAcME1TgCAtMG3zwEA/xbOOAEAAACAC4oTAAAAALigOAEAAACAC4oTAAAAALigOAEAAACAC4oTAAAAALigOAEAAACAC4oTAAAAALj4D7lP8vPsZbsvAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x900 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy on test set: 0.9480001\n"
     ]
    }
   ],
   "source": [
    "# TODO : show your result\n",
    "print('Best hidden_size: %d\\nBest lr: %e\\nBest lr_decay: %e\\nBest reg: %e\\ntrain accuracy: %f\\nval accuracy: %f' %\n",
    "     (best_hidden_size, best_lr, best_lr_decay, best_reg, results[(best_lr, best_reg, best_lr_decay )][0], results[(best_lr, best_reg,  best_lr_decay )][1]))\n",
    "\n",
    "params_test = Network.init(input_size = input_size, hidden_size = best_hidden_size, output_size = num_classes, random_seed = random_seed)\n",
    "stats_test, best_params_test = Network.train(params_test, X_train, y_train, X_val, y_val, random_seed = random_seed,\n",
    "                learning_rate=best_lr, momentum=0.9, wd_decay=best_reg, lr_decay = best_lr_decay,\n",
    "                num_iters=num_iters, batch_size=batch_size,\n",
    "                do_early_stopping=True,\n",
    "                print_every=50, verbose=True)\n",
    "test_utils.plot_loss_history(stats_test['loss_history'], stats_test['val_loss_history'])\n",
    "test_utils.plot_acc_history(stats_test['acc_history'], stats_test['val_acc_history'])\n",
    "print('Final accuracy on test set:', Network.get_acc(best_params_test, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "60b2ce29840163f6873a9432829aeb3249d3202bcf52bf02732bf29c17ccb792"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
